{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import *\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as normal_datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<TruncBackward object at 0x7f52f06e20b8>\n"
     ]
    }
   ],
   "source": [
    "#Random dataset\n",
    "dist1 = torch.randint(0,16,(100,)).float().requires_grad_(True).cuda()\n",
    "\n",
    "#Do Stuff to Data to create graph\n",
    "conv = torch.nn.Conv1d(1,1,1).cuda()(dist1[(None,)*2])\n",
    "\n",
    "#Calculate histogram\n",
    "if conv.min().item() < 0:\n",
    "    conv = conv - conv.min()\n",
    "bins = 10\n",
    "conv_binned = torch.trunc(conv * bins/conv.max().item())\n",
    "ones = torch.ones_like(conv_binned, requires_grad=True)\n",
    "zeros = torch.zeros_like(conv_binned, requires_grad=True)\n",
    "hist = torch.tensor([torch.where(conv_binned == bin, ones, zeros).sum() \n",
    "for bin in range(bins)], requires_grad=True)\n",
    "\n",
    "print(hist.grad_fn)\n",
    "print(conv_binned.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_positive_expectation(p_samples, average=True):\n",
    "    #Measure = JSD (Simplified from DIM Code for clarity)\n",
    "    log_2 = math.log(2.)\n",
    "    Ep = log_2 - F.softplus(-p_samples)  # Note JSD will be shifted\n",
    "    if average:\n",
    "        return Ep.mean()\n",
    "    else:\n",
    "        return Ep\n",
    "\n",
    "def get_negative_expectation(q_samples, average=True):\n",
    "    #Measure = JSD (Simplified from DIM Code for clarity)\n",
    "    log_2 = math.log(2.)\n",
    "    Eq = F.softplus(-q_samples) + q_samples - log_2  # Note JSD will be shifted\n",
    "    if average:\n",
    "        return Eq.mean()\n",
    "    else:\n",
    "        return Eq\n",
    "\n",
    "def loss_calc(lmap, gmap):\n",
    "    #The fenchel_dual_loss from the DIM code\n",
    "    #Reshape tensors dims to (N, Channels, chunks)\n",
    "    lmap = lmap.reshape(2,128,-1)\n",
    "    gmap = gmap.squeeze()\n",
    "    \n",
    "    N, units, n_locals = lmap.size()\n",
    "    n_multis = gmap.size(2)\n",
    "\n",
    "    # First we make the input tensors the right shape.\n",
    "    l = lmap.view(N, units, n_locals)\n",
    "    l = lmap.permute(0, 2, 1)\n",
    "    l = lmap.reshape(-1, units)\n",
    "\n",
    "    m = gmap.view(N, units, n_multis)\n",
    "    m = gmap.permute(0, 2, 1)\n",
    "    m = gmap.reshape(-1, units)\n",
    "    \n",
    "    u = torch.mm(m, l.t())\n",
    "    u = u.reshape(N, n_multis, N, n_locals).permute(0, 2, 3, 1)\n",
    "    \n",
    "    mask = torch.eye(N).to(l.device)\n",
    "    n_mask = 1 - mask\n",
    "    \n",
    "    E_pos = get_positive_expectation(u, average=False).mean(2).mean(2)\n",
    "    E_neg = get_negative_expectation(u, average=False).mean(2).mean(2)\n",
    "    \n",
    "    E_pos = (E_pos * mask).sum() / mask.sum()\n",
    "    E_neg = (E_neg * n_mask).sum() / n_mask.sum()\n",
    "    loss = E_neg - E_pos\n",
    "    \n",
    "    return loss\n",
    "\n",
    "class Mixed_Dim(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mixed_Dim, self).__init__()\n",
    "        #Local Feature Map: Local feature map of size [N, 128, 8, 8, 8]\n",
    "        self.lmapnet = Sequential(Conv3d(1, 8, 2, 2), ReLU(), BatchNorm3d(8),\n",
    "                                  Conv3d(8, 16, 2, 2), ReLU(), BatchNorm3d(16),\n",
    "                                  Conv3d(16, 32, 2, 2), ReLU(), BatchNorm3d(32),\n",
    "                                  Conv3d(32, 64, 2, 2), ReLU(), BatchNorm3d(64),\n",
    "                                  Conv3d(64, 128, 2, 2), ReLU(), BatchNorm3d(128) \n",
    "                                 )\n",
    "        \n",
    "        #Global Feature Map: Global feature map of size [N, 128, 1, 1, 1]\n",
    "        self.gmapnet = Sequential(Conv3d(128, 128, 2, 2), ReLU(), BatchNorm3d(128),\n",
    "                                  Conv3d(128, 128, 2, 2), ReLU(), BatchNorm3d(128),\n",
    "                                  Conv3d(128, 128, 2, 2), ReLU()\n",
    "                                 )\n",
    "        #Per paper, global map is activated:\n",
    "        self.gfc1 = Sequential(Linear(1, 512), ReLU(), BatchNorm3d(128),\n",
    "                               Linear(512, 512))\n",
    "        self.gfc2 = Sequential(Linear(1, 512), ReLU(), BatchNorm3d(128))\n",
    "        \n",
    "        #Per paper, local map is activated:\n",
    "        self.lfc1 = Sequential(Conv3d(128, 128, 1, 1), ReLU(), BatchNorm3d(128),\n",
    "                               Conv3d(128, 128, 1, 1))\n",
    "        self.lfc2 = Sequential(Conv3d(128, 128, 1, 1), ReLU(), BatchNorm3d(128))\n",
    "        self.Laynorm = LayerNorm([128, 8, 8, 8])\n",
    "        \n",
    "    def forward(self, moving, atlas):\n",
    "        #Local feature maps of moving and atlas\n",
    "        lmap_moving = self.lmapnet(moving)\n",
    "        lmap_atlas = self.lmapnet(atlas)\n",
    "        \n",
    "        #Global feature map of atlas\n",
    "        gmap_atlas = self.gmapnet(lmap_atlas)\n",
    "        \n",
    "        #Encode Global feature map\n",
    "        gout1, gout2 = self.gfc1(gmap_atlas), self.gfc2(gmap_atlas)\n",
    "        gmap_atlas_enc = self.gfc1(gmap_atlas) + self.gfc2(gmap_atlas)\n",
    "        \n",
    "        #Encode Local feature maps\n",
    "        lmap_atlas_enc = self.Laynorm(self.lfc1(lmap_atlas) + self.lfc2(lmap_atlas))\n",
    "        lmap_moving_enc = self.Laynorm(self.lfc1(lmap_moving) + self.lfc2(lmap_moving))\n",
    "        \n",
    "        return lmap_atlas_enc, lmap_moving_enc, gmap_atlas_enc\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randint(0, 256, (2, 1, 256, 256, 256)).float().requires_grad_(True)\n",
    "B = torch.randint(0, 256, (2, 1, 256, 256, 256)).float().requires_grad_(True)\n",
    "model_dim = Mixed_Dim().cuda().train()\n",
    "optim = torch.optim.Adam(model_dim.parameters(), lr=0.01)\n",
    "A = A.cuda()\n",
    "B = B.cuda()\n",
    "for epoch in range(100):\n",
    "    loc_atlas, loc_moving, glob_atlas = model_dim(B, A)\n",
    "    loss1 = loss_calc(loc_moving, glob_atlas)\n",
    "    loss2 = loss_calc(loc_atlas, glob_atlas)\n",
    "    loss = (loss2 + loss1)\n",
    "    model_dim.zero_grad()\n",
    "    print(loss.item())\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# 将数据处理成Variable, 如果有GPU, 可以转成cuda形式\n",
    "def get_variable(x):\n",
    "    x = Variable(x)\n",
    "    return x.cuda() if torch.cuda.is_available() else x\n",
    "\n",
    "# 从torchvision.datasets中加载一些常用数据集\n",
    "train_dataset = normal_datasets.MNIST(\n",
    "                            root='./mnist/',                 # 数据集保存路径\n",
    "                            train=True,                      # 是否作为训练集\n",
    "                            transform=transforms.ToTensor(), # 数据如何处理, 可以自己自定义\n",
    "                            download=True)                   # 路径下没有的话, 可以下载\n",
    "\n",
    "# 见数据加载器和batch\n",
    "test_dataset = normal_datasets.MNIST(root='./mnist/',\n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torchvision' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-295-cdbe3f368f1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                           shuffle=False)\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtestset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/home/dltdc/data/pytorch_datasets'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtestloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torchvision' is not defined"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "testset = normal_datasets.CIFAR10(root='/home/dltdc/data/pytorch_datasets', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 28, 28])"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels = next(iter(test_loader))\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.2432,  2.0860, -0.1020,  ...,  0.6909,  0.8534,  0.1113],\n",
      "          [ 0.0522,  0.0127,  0.0801,  ..., -0.5781,  0.1754, -0.5378],\n",
      "          [ 1.2599, -0.6276,  0.8985,  ..., -0.4199, -1.9612,  0.9508],\n",
      "          ...,\n",
      "          [ 0.4318,  0.9677, -1.3690,  ..., -0.3889,  0.4352,  0.0706],\n",
      "          [-0.4099,  1.3556, -0.6281,  ..., -2.4886,  0.1191,  1.4687],\n",
      "          [ 1.2941,  0.3983,  0.9566,  ..., -1.3547, -1.4695,  1.0963]],\n",
      "\n",
      "         [[ 2.1112,  1.7537,  0.5890,  ...,  1.8097,  0.1650,  0.0179],\n",
      "          [-1.0349, -3.0297, -0.5800,  ...,  0.7339, -1.3606, -0.1293],\n",
      "          [ 0.1269, -0.2297,  0.2872,  ..., -0.4281, -0.4633, -0.6673],\n",
      "          ...,\n",
      "          [-1.0434, -2.5238,  0.5666,  ..., -0.7338,  0.3869, -0.9548],\n",
      "          [ 0.4015, -2.9308, -0.9158,  ...,  1.6569, -0.0793, -0.9980],\n",
      "          [ 0.1843, -1.6088,  1.1173,  ...,  0.9143,  0.6597, -0.3660]],\n",
      "\n",
      "         [[-0.7815,  1.0543, -0.5680,  ..., -1.0372,  1.2295, -0.8847],\n",
      "          [-1.4189,  0.7522,  0.7087,  ...,  0.6603,  0.9178,  0.3819],\n",
      "          [-0.6339,  1.6142,  0.1206,  ..., -0.5730, -0.3835,  0.7600],\n",
      "          ...,\n",
      "          [-0.1770,  1.1312,  0.1470,  ..., -1.6359, -0.7273,  0.3422],\n",
      "          [ 2.4024,  3.3665,  1.0205,  ..., -0.8364, -2.3873,  1.1752],\n",
      "          [-0.2329, -1.9764,  1.1458,  ..., -2.3963, -1.4113, -0.4500]],\n",
      "\n",
      "         [[-1.7291, -0.5162, -0.0503,  ...,  0.1929,  0.2163,  1.8192],\n",
      "          [ 0.0606, -0.4980,  1.7458,  ..., -0.0680, -0.0097,  1.1537],\n",
      "          [-0.8354, -0.3444,  0.1591,  ...,  0.6589,  0.1686, -1.2577],\n",
      "          ...,\n",
      "          [-0.0418,  1.3978,  0.1248,  ...,  0.1407,  0.3214,  1.3309],\n",
      "          [-2.1603, -0.3179,  1.2912,  ...,  0.5382,  0.2158, -0.4141],\n",
      "          [ 0.2766, -0.9207,  0.3059,  ...,  0.8650, -0.8912, -0.3313]]]],\n",
      "       device='cuda:0')\n",
      "tensor([  12.,   99.,  618., 2314., 4542., 4860., 2863.,  898.,  163.,   15.],\n",
      "       device='cuda:0')\n",
      "tensor([  12.,   99.,  618., 2314., 4542., 4860., 2863.,  898.,  163.,   15.],\n",
      "       device='cuda:0')\n",
      "tensor([20.,  2.,  3.,  3.,  0.,  2.,  2.,  3.,  3.,  2.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dltdc/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:101: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "/opt/conda/conda-bld/pytorch_1570910687650/work/torch/csrc/autograd/python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n"
     ]
    }
   ],
   "source": [
    "def hist_with_neg(inp, nb_bins=10):\n",
    "    \"\"\"\n",
    "    https://discuss.pytorch.org/t/differentiable-torch-histc/25865\n",
    "    \"\"\"\n",
    "    x = inp\n",
    "    x = x.contiguous().cuda()\n",
    "    with torch.no_grad():\n",
    "        hist = []\n",
    "    #     if x.min().item() < 0:\n",
    "    #         x = x - x.min()\n",
    "    #     conv_binned = torch.trunc(x * (nb_bins-1)/x.max().item())\n",
    "    #     ones = torch.ones_like(conv_binned)\n",
    "    #     zeros = torch.zeros_like(conv_binned)\n",
    "    #     hist = torch.tensor([torch.where(conv_binned == bin_, 1, 0).sum() \n",
    "    #                     for bin_ in range(nb_bins)])\n",
    "\n",
    "        for b in range(inp.shape[0]):\n",
    "            hist_pre_ch = []\n",
    "            for c in range(inp.shape[1]):\n",
    "                #histc\n",
    "\n",
    "                x = inp[b,c,:,:]\n",
    "                if x.min().item() < 0:\n",
    "                    x = x - x.min()\n",
    "                conv_binned = torch.trunc(x * (nb_bins-1)/x.max().item()).cuda()\n",
    "                ones = torch.ones_like(conv_binned, device=device)\n",
    "                zeros = torch.zeros_like(conv_binned, device=device)\n",
    "                hist_pre_ch.append(torch.tensor([torch.where(conv_binned == bin_, ones, zeros).sum() \n",
    "                                for bin_ in range(nb_bins)], device=device))\n",
    "\n",
    "            hist.append(torch.stack(hist_pre_ch, 0).cuda())\n",
    "        hist = torch.stack(hist, 0).cuda()\n",
    "    return hist\n",
    "\n",
    "\n",
    "\n",
    "class Hist(torch.autograd.Function):\n",
    "    def __init__(self, nb_bins=10, in_channel=3):\n",
    "        super(Hist, self).__init__()\n",
    "        self.nb_bins = nb_bins\n",
    "        self.in_channel = in_channel\n",
    "    \n",
    "#     @staticmethod\n",
    "    def forward(self, input):\n",
    "        return hist_with_neg(input)\n",
    "#         ctx.save_for_backward(input, nb_bins)\n",
    "        \n",
    "    \n",
    "#     @staticmethod\n",
    "#     def backward(ctx, grad_output):\n",
    "#         x, nb_bins = ctx.saved_tensors\n",
    "#         x_grad = w_grad = None\n",
    "#         nb_bin_grad = torch.zeros(nb_bins.shape, device=ctx._input_device, dtype=torch.int32)\n",
    "        \n",
    "#         if ctx.needs_input_grad[0]:\n",
    "#           x_grad = torch.nn.grad.conv2d_input(x.shape, w, grad_output)\n",
    "#         if ctx.needs_input_grad[1]:\n",
    "#           nb_bins_grad = torch.nn.grad.conv2d_weight(x, w.shape, grad_output)\n",
    "#         return x_grad, w_grad\n",
    "        \n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "#         self.conv2 = nn.Sequential(\n",
    "#             nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "#             nn.BatchNorm2d(32),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2))\n",
    "#         self.fc = nn.Linear(7 * 7 * 32, 10)\n",
    "        self.hist = Hist()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "#         out = self.conv1(x)\n",
    "#         out = self.conv2(out)\n",
    "#         out = out.view(out.size(0), -1)  # reshape\n",
    "#         out = self.fc(out)\n",
    "        out2 = self.hist(x)\n",
    "        out = torch.histc(x, bins=10, min=x.min().item(), max=x.max().item())\n",
    "#         out = self.fc(out)\n",
    "#         print(out)\n",
    "        return out,out2\n",
    "    \n",
    "\n",
    "    \n",
    "def initialize_weights(model):\n",
    "    if type(model) in [nn.Linear]:\n",
    "        nn.init.xavier_normal(model.weight.data)\n",
    "    elif type(model) in [nn.LSTM, nn.RNN, nn.GRU]:\n",
    "        nn.init.xavier_normal(model.weight_hh_l0)\n",
    "        nn.init.xavier_normal(model.weight_ih_l0)\n",
    "    elif isinstance(model, nn.Conv2d):\n",
    "        nn.init.xavier_normal(model.weight.data)\n",
    "#         nn.init.xavier_normal(model.bias.data)\n",
    "\n",
    "\n",
    "cnn = CNN()\n",
    "cnn.apply(initialize_weights)\n",
    "if torch.cuda.is_available():\n",
    "    cnn = cnn.cuda()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)\n",
    "\n",
    "input = torch.randn(1, 4, 64, 64).cuda()\n",
    "target = torch.randn(1, 4).type(torch.LongTensor).cuda()\n",
    "print(input)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "output,out2 = cnn(input)\n",
    "print(output)\n",
    "# print(out2)\n",
    "print(torch.histc(input, bins=10))\n",
    "# print(torch.histc(out2, bins=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 1.],\n",
      "          [1., 1.]],\n",
      "\n",
      "         [[1., 1.],\n",
      "          [1., 1.]],\n",
      "\n",
      "         [[1., 1.],\n",
      "          [1., 1.]]]], dtype=torch.float64)\n",
      "tensor([[[[1., 1., 0., 0., 0., 1., 1., 1.],\n",
      "          [0., 1., 1., 0., 0., 0., 1., 0.],\n",
      "          [0., 0., 0., 0., 1., 0., 1., 1.],\n",
      "          [1., 0., 1., 0., 0., 1., 1., 1.],\n",
      "          [0., 1., 1., 1., 1., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 0., 0., 0., 1.],\n",
      "          [0., 1., 0., 0., 0., 1., 1., 1.],\n",
      "          [1., 0., 1., 1., 0., 1., 0., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "          [0., 1., 0., 0., 1., 1., 1., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 1., 1.],\n",
      "          [1., 0., 1., 1., 1., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 0., 1., 0., 0.],\n",
      "          [0., 1., 0., 1., 0., 0., 0., 1.],\n",
      "          [1., 1., 0., 0., 1., 0., 1., 0.],\n",
      "          [1., 0., 0., 0., 1., 1., 0., 0.]],\n",
      "\n",
      "         [[0., 1., 0., 0., 0., 0., 1., 1.],\n",
      "          [1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "          [0., 1., 1., 0., 0., 1., 1., 0.],\n",
      "          [1., 1., 0., 1., 1., 0., 0., 0.],\n",
      "          [0., 1., 1., 0., 1., 1., 1., 0.],\n",
      "          [0., 0., 0., 1., 0., 1., 1., 0.],\n",
      "          [0., 1., 1., 1., 1., 0., 1., 0.],\n",
      "          [1., 0., 1., 1., 1., 1., 1., 1.]]]], dtype=torch.float64)\n",
      "tensor([[1., 1., 0., 0., 0., 1., 1., 1.],\n",
      "        [0., 1., 1., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 0., 0., 1., 1., 1.],\n",
      "        [0., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 1., 1., 1.],\n",
      "        [1., 0., 1., 1., 0., 1., 0., 1.]], dtype=torch.float64)\n",
      "tensor([[[1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1.],\n",
      "         [1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "         [0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.],\n",
      "         [1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.],\n",
      "         [1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "         [1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0.],\n",
      "         [0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.],\n",
      "         [1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.],\n",
      "         [1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0.],\n",
      "         [1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "         [0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1.]]],\n",
      "       dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given output_size=(5, 5), kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2), expected size of input's dimension 2 to match the calculated number of sliding blocks 2 * 2 = 4, but got input.size(2)=16.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-7b95b5a7c435>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mwindows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(input, output_size, kernel_size, dilation, padding, stride)\u001b[0m\n\u001b[1;32m   3092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3093\u001b[0m         return torch._C._nn.col2im(input, _pair(output_size), _pair(kernel_size),\n\u001b[0;32m-> 3094\u001b[0;31m                                    _pair(dilation), _pair(padding), _pair(stride))\n\u001b[0m\u001b[1;32m   3095\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3096\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input Error: Only 3D input Tensors are supported (got {}D)\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given output_size=(5, 5), kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2), expected size of input's dimension 2 to match the calculated number of sliding blocks 2 * 2 = 4, but got input.size(2)=16."
     ]
    }
   ],
   "source": [
    "filters = torch.ones(1,3,2,2).double()\n",
    "inputs = torch.LongTensor(1,3,8,8).random_(0, 2).double()\n",
    "print(filters)\n",
    "print(inputs)\n",
    "print(inputs[0,0,])\n",
    "F.conv2d(inputs, filters, stride=1, padding=0).shape\n",
    "windows = F.unfold(inputs, kernel_size=2, stride=2)\n",
    "print(windows)\n",
    "out = F.fold(windows, (5,5), kernel_size=2, stride=2)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 3.,  6.,  4.,  8.,  7.,  3.,  5.,  3.,  8.,  4.,  7.,  0.,  6.,  3.,\n",
      "            4.,  1.],\n",
      "          [ 6.,  7.,  5.,  7.,  0.,  4.,  7.,  4.,  7.,  5.,  6.,  6.,  4.,  7.,\n",
      "            2.,  3.],\n",
      "          [ 6.,  9.,  3.,  6.,  6.,  2.,  7.,  5.,  8.,  5.,  5.,  5.,  2.,  5.,\n",
      "            5.,  4.],\n",
      "          [ 4.,  1.,  5.,  2.,  9.,  0.,  4.,  7.,  8.,  7.,  0.,  6.,  5.,  5.,\n",
      "            5.,  3.],\n",
      "          [ 5.,  5.,  7.,  6.,  7.,  6.,  6.,  6.,  4.,  5., 10.,  2.,  4.,  3.,\n",
      "            4.,  5.],\n",
      "          [ 6.,  9.,  3.,  7.,  3.,  5.,  2.,  4.,  4.,  5.,  0.,  3.,  6.,  5.,\n",
      "            8.,  4.],\n",
      "          [ 4.,  4.,  3.,  3.,  5.,  3.,  7.,  3.,  6.,  2.,  7.,  1.,  3.,  5.,\n",
      "            5.,  4.],\n",
      "          [ 4.,  5.,  6.,  5.,  1.,  3.,  3.,  5.,  4.,  1.,  8.,  1.,  6.,  4.,\n",
      "            8.,  2.],\n",
      "          [ 3.,  2.,  4.,  3.,  2.,  5.,  3.,  5.,  4.,  6.,  5.,  5.,  6.,  4.,\n",
      "            8.,  0.],\n",
      "          [ 6.,  7.,  6.,  6.,  6.,  4.,  3.,  5.,  5.,  5.,  4.,  3.,  6.,  6.,\n",
      "            1.,  0.],\n",
      "          [ 4.,  5.,  6.,  7.,  3.,  4.,  5.,  3.,  4.,  4.,  4.,  6.,  8.,  2.,\n",
      "            3.,  3.],\n",
      "          [ 4.,  5.,  1.,  5.,  7.,  3.,  3.,  4.,  5.,  0.,  4.,  3.,  4.,  7.,\n",
      "            3.,  5.],\n",
      "          [ 5.,  4.,  4.,  5.,  3.,  4.,  4.,  6.,  0.,  1.,  5.,  8.,  7.,  8.,\n",
      "            5.,  7.],\n",
      "          [ 6.,  2.,  4.,  6.,  7.,  5.,  7.,  4.,  2.,  7.,  6.,  4.,  5.,  2.,\n",
      "            6.,  4.],\n",
      "          [ 2.,  2.,  3.,  8.,  2.,  4.,  2.,  5.,  4.,  3.,  3.,  9.,  7.,  5.,\n",
      "            6.,  3.],\n",
      "          [ 6.,  4.,  4.,  2.,  4.,  5.,  6.,  5.,  5.,  4.,  4.,  3.,  3.,  3.,\n",
      "            2.,  8.]]]], device='cuda:0')\n",
      "tensor([ 9.,  9., 20., 39., 50., 50., 36., 25., 13.,  4.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.randn(1, 1, 16, 16).cuda()\n",
    "if x.min().item() < 0:\n",
    "    x = x - x.min()\n",
    "conv_binned = torch.trunc(x * 10/x.max().item())\n",
    "print(conv_binned)\n",
    "ones = torch.ones_like(conv_binned, requires_grad=True)\n",
    "zeros = torch.zeros_like(conv_binned, requires_grad=True)\n",
    "hist = torch.tensor([torch.where(conv_binned == bin_, ones, zeros).sum() \n",
    "                for bin_ in range(10)], requires_grad=True)\n",
    "print(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2167, grad_fn=<MeanBackward0>)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "out_channels = 6\n",
    "in_channels = 1\n",
    "kh, kw = 3, 3\n",
    "weight = torch.randn(out_channels, in_channels, kh, kw, requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    weight[:, :, 0, 0] = 0.\n",
    "    weight[:, :, -1, 0] = 0.\n",
    "    weight[:, :, 0, -1] = 0.\n",
    "    weight[:, :, -1, -1] = 0.\n",
    "\n",
    "x = torch.randn(1, in_channels, 5, 5)\n",
    "output = F.conv2d(x, weight)\n",
    "print(output.mean())\n",
    "print(weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmac_hist(x, L=16, eps=1e-6):\n",
    "    ovr = 0.4 # desired overlap of neighboring regions\n",
    "    steps = torch.Tensor([2, 3, 4, 5, 6, 7]).cuda() # possible regions for the long dimension\n",
    "\n",
    "    W = x.size(3)\n",
    "    H = x.size(2)\n",
    "\n",
    "    w = min(W, H)\n",
    "    w2 = math.floor(w/2.0 - 1)\n",
    "\n",
    "    b = (max(H, W)-w)/(steps-1)\n",
    "    (tmp, idx) = torch.min(torch.abs(((w**2 - w*b)/w**2)-ovr), 0) # steps(idx) regions for long dimension\n",
    "\n",
    "    # region overplus per dimension\n",
    "    Wd = 0;\n",
    "    Hd = 0;\n",
    "    if H < W:  \n",
    "        Wd = idx.item() + 1\n",
    "    elif H > W:\n",
    "        Hd = idx.item() + 1\n",
    "\n",
    "    v = [hist_with_neg(x)]\n",
    "#     v = v / (torch.norm(v, p=2, dim=1, keepdim=True) + eps).expand_as(v)\n",
    "    \n",
    "    L_min = 16 #1\n",
    "    for l in range(L_min, L+1):\n",
    "        wl = math.floor(2*w/(l+1))\n",
    "        wl2 = math.floor(wl/2 - 1)\n",
    "\n",
    "        if l+Wd == 1:\n",
    "            b = 0\n",
    "        else:\n",
    "            b = (W-wl)/(l+Wd-1)\n",
    "        cenW = torch.floor(wl2 + torch.Tensor(range(l-1+Wd+1))*b) - wl2 # center coordinates\n",
    "        if l+Hd == 1:\n",
    "            b = 0\n",
    "        else:\n",
    "            b = (H-wl)/(l+Hd-1)\n",
    "        cenH = torch.floor(wl2 + torch.Tensor(range(l-1+Hd+1))*b) - wl2 # center coordinates\n",
    "        \n",
    "        vt_array = []\n",
    "        for i_ in cenH.tolist():\n",
    "            for j_ in cenW.tolist():\n",
    "                if wl == 0:\n",
    "                    continue\n",
    "                R = x[:,:,(int(i_)+torch.Tensor(range(wl)).long()).tolist(),:]\n",
    "                R = R[:,:,:,(int(j_)+torch.Tensor(range(wl)).long()).tolist()]\n",
    "                vt = hist_with_neg(R)\n",
    "                vt = vt / (torch.norm(vt, p=2, dim=-1, keepdim=True) + eps).expand_as(vt)\n",
    "\n",
    "#                 v += vt\n",
    "                vt_array+=[vt]\n",
    "\n",
    "        print(vt_array[0].shape)\n",
    "        vt_array = torch.stack(vt_array, -1)\n",
    "        print(len(cenH.tolist()),len(cenW.tolist()))\n",
    "        print(vt_array[0,...].shape)\n",
    "        arr_along_batch = []\n",
    "        for batch_id in range(x.shape[0]):\n",
    "            arr_along_batch.append(F.fold(vt_array[batch_id,...], (len(cenH.tolist()),len(cenW.tolist())), (1,1)))\n",
    "        arr_along_batch = torch.stack(arr_along_batch, 0).cuda()\n",
    "        print(arr_along_batch.shape)\n",
    "#         v += vt_array\n",
    "\n",
    "    return arr_along_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 10])\n",
      "16 16\n",
      "torch.Size([4, 10, 256])\n",
      "torch.Size([1, 4, 10, 16, 16])\n",
      "tensor([[[[[0.2212, 0.1701, 0.2321,  ..., 0.1729, 0.0977, 0.1517],\n",
      "           [0.1074, 0.1712, 0.3259,  ..., 0.2141, 0.0489, 0.1044],\n",
      "           [0.2233, 0.2276, 0.0526,  ..., 0.0475, 0.1099, 0.1041],\n",
      "           ...,\n",
      "           [0.1068, 0.2240, 0.2166,  ..., 0.1579, 0.1644, 0.0493],\n",
      "           [0.1735, 0.1712, 0.1138,  ..., 0.2776, 0.2192, 0.1123],\n",
      "           [0.1588, 0.1071, 0.2276,  ..., 0.2632, 0.2179, 0.2173]],\n",
      "\n",
      "          [[0.0553, 0.1701, 0.3482,  ..., 0.1153, 0.0489, 0.1011],\n",
      "           [0.2147, 0.2854, 0.1629,  ..., 0.0535, 0.1466, 0.1044],\n",
      "           [0.0558, 0.1707, 0.1579,  ..., 0.1900, 0.1649, 0.1041],\n",
      "           ...,\n",
      "           [0.1601, 0.0000, 0.0542,  ..., 0.2632, 0.1644, 0.0987],\n",
      "           [0.2892, 0.0571, 0.1138,  ..., 0.3702, 0.2192, 0.3370],\n",
      "           [0.1059, 0.2141, 0.2276,  ..., 0.2632, 0.1089, 0.3259]],\n",
      "\n",
      "          [[0.3871, 0.5103, 0.3482,  ..., 0.3458, 0.3420, 0.4046],\n",
      "           [0.1610, 0.3995, 0.5431,  ..., 0.3212, 0.2443, 0.2610],\n",
      "           [0.3349, 0.3413, 0.2105,  ..., 0.1425, 0.3848, 0.4165],\n",
      "           ...,\n",
      "           [0.3203, 0.2799, 0.4332,  ..., 0.3158, 0.5480, 0.5426],\n",
      "           [0.3470, 0.2854, 0.2844,  ..., 0.3239, 0.5480, 0.4493],\n",
      "           [0.5293, 0.4818, 0.3982,  ..., 0.4211, 0.3268, 0.1629]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.3871, 0.2268, 0.2321,  ..., 0.2882, 0.1466, 0.2023],\n",
      "           [0.2147, 0.3424, 0.1086,  ..., 0.2141, 0.1466, 0.2610],\n",
      "           [0.2791, 0.2276, 0.2632,  ..., 0.0475, 0.1099, 0.3123],\n",
      "           ...,\n",
      "           [0.2135, 0.2799, 0.2708,  ..., 0.0526, 0.0548, 0.0987],\n",
      "           [0.2892, 0.2283, 0.3413,  ..., 0.0000, 0.1096, 0.2247],\n",
      "           [0.1588, 0.1606, 0.2844,  ..., 0.1053, 0.1634, 0.3802]],\n",
      "\n",
      "          [[0.0553, 0.1134, 0.0580,  ..., 0.2306, 0.0000, 0.0000],\n",
      "           [0.0537, 0.1141, 0.0543,  ..., 0.2141, 0.0000, 0.0000],\n",
      "           [0.1674, 0.2276, 0.0526,  ..., 0.2851, 0.1649, 0.0000],\n",
      "           ...,\n",
      "           [0.1601, 0.2799, 0.0000,  ..., 0.1053, 0.1644, 0.0493],\n",
      "           [0.0578, 0.3995, 0.3413,  ..., 0.0000, 0.0548, 0.0562],\n",
      "           [0.0000, 0.0000, 0.0569,  ..., 0.0000, 0.0545, 0.0543]],\n",
      "\n",
      "          [[0.0553, 0.0567, 0.0580,  ..., 0.0576, 0.0489, 0.0506],\n",
      "           [0.0537, 0.0571, 0.0543,  ..., 0.0535, 0.0489, 0.0522],\n",
      "           [0.0000, 0.0569, 0.0526,  ..., 0.0000, 0.0550, 0.0521],\n",
      "           ...,\n",
      "           [0.0000, 0.0560, 0.0542,  ..., 0.0526, 0.0548, 0.0493],\n",
      "           [0.0578, 0.0571, 0.0000,  ..., 0.0463, 0.0548, 0.0562],\n",
      "           [0.0529, 0.0535, 0.0569,  ..., 0.0526, 0.0545, 0.0543]]],\n",
      "\n",
      "\n",
      "         [[[0.3391, 0.2403, 0.2306,  ..., 0.1138, 0.0952, 0.0468],\n",
      "           [0.3212, 0.2669, 0.0508,  ..., 0.1116, 0.1011, 0.0931],\n",
      "           [0.2596, 0.1610, 0.1006,  ..., 0.2135, 0.2105, 0.2589],\n",
      "           ...,\n",
      "           [0.1747, 0.1696, 0.0484,  ..., 0.1701, 0.1068, 0.0497],\n",
      "           [0.2361, 0.2233, 0.2941,  ..., 0.0567, 0.1664, 0.2123],\n",
      "           [0.2306, 0.1083, 0.1649,  ..., 0.1025, 0.4237, 0.3370]],\n",
      "\n",
      "          [[0.2826, 0.3004, 0.3458,  ..., 0.1707, 0.2381, 0.0936],\n",
      "           [0.2676, 0.2669, 0.0000,  ..., 0.1116, 0.1517, 0.3260],\n",
      "           [0.3634, 0.0537, 0.0000,  ..., 0.1068, 0.1053, 0.2071],\n",
      "           ...,\n",
      "           [0.3493, 0.3957, 0.1452,  ..., 0.2835, 0.2669, 0.3478],\n",
      "           [0.1771, 0.2233, 0.2353,  ..., 0.3402, 0.3328, 0.1592],\n",
      "           [0.1729, 0.1625, 0.0550,  ..., 0.2562, 0.1816, 0.3370]],\n",
      "\n",
      "          [[0.1696, 0.4206, 0.2882,  ..., 0.3982, 0.5238, 0.3742],\n",
      "           [0.3212, 0.5871, 0.2033,  ..., 0.5023, 0.4551, 0.5123],\n",
      "           [0.5192, 0.2684, 0.0503,  ..., 0.4804, 0.4737, 0.5178],\n",
      "           ...,\n",
      "           [0.2911, 0.3391, 0.4355,  ..., 0.3969, 0.4804, 0.3975],\n",
      "           [0.3542, 0.1116, 0.3529,  ..., 0.2268, 0.4438, 0.4246],\n",
      "           [0.2306, 0.2166, 0.2199,  ..., 0.1537, 0.2421, 0.3932]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.1696, 0.3004, 0.1729,  ..., 0.3413, 0.0476, 0.0000],\n",
      "           [0.1606, 0.2135, 0.2033,  ..., 0.3907, 0.0506, 0.0000],\n",
      "           [0.0519, 0.3758, 0.6038,  ..., 0.3203, 0.2632, 0.1553],\n",
      "           ...,\n",
      "           [0.1747, 0.2261, 0.0000,  ..., 0.3969, 0.1068, 0.0497],\n",
      "           [0.3542, 0.2791, 0.3529,  ..., 0.3402, 0.0555, 0.0531],\n",
      "           [0.4611, 0.3249, 0.4397,  ..., 0.3586, 0.4237, 0.1685]],\n",
      "\n",
      "          [[0.2261, 0.1803, 0.2306,  ..., 0.1707, 0.0000, 0.0000],\n",
      "           [0.0000, 0.0534, 0.1525,  ..., 0.1674, 0.0000, 0.0000],\n",
      "           [0.0000, 0.1074, 0.2516,  ..., 0.0000, 0.0526, 0.0000],\n",
      "           ...,\n",
      "           [0.1747, 0.0565, 0.0484,  ..., 0.2268, 0.0534, 0.0000],\n",
      "           [0.1771, 0.1116, 0.0588,  ..., 0.2835, 0.0555, 0.0000],\n",
      "           [0.2882, 0.0542, 0.1099,  ..., 0.1025, 0.3026, 0.0000]],\n",
      "\n",
      "          [[0.0565, 0.0601, 0.0576,  ..., 0.0000, 0.0476, 0.0468],\n",
      "           [0.0535, 0.0534, 0.0508,  ..., 0.0000, 0.0506, 0.0466],\n",
      "           [0.0519, 0.0537, 0.0503,  ..., 0.0534, 0.0526, 0.0518],\n",
      "           ...,\n",
      "           [0.0582, 0.0565, 0.0484,  ..., 0.0567, 0.0534, 0.0497],\n",
      "           [0.0590, 0.0558, 0.0588,  ..., 0.0567, 0.0555, 0.0531],\n",
      "           [0.0000, 0.0542, 0.0550,  ..., 0.0512, 0.0605, 0.0562]]],\n",
      "\n",
      "\n",
      "         [[[0.0991, 0.1649, 0.1592,  ..., 0.2290, 0.3969, 0.0959],\n",
      "           [0.0503, 0.2791, 0.1145,  ..., 0.1549, 0.0492, 0.1044],\n",
      "           [0.0512, 0.1988, 0.4540,  ..., 0.1855, 0.1805, 0.2791],\n",
      "           ...,\n",
      "           [0.1723, 0.1674, 0.2386,  ..., 0.2724, 0.4295, 0.4479],\n",
      "           [0.1038, 0.0883, 0.1525,  ..., 0.0543, 0.1747, 0.0526],\n",
      "           [0.1074, 0.0523, 0.1068,  ..., 0.1109, 0.2329, 0.2254]],\n",
      "\n",
      "          [[0.1487, 0.2199, 0.1592,  ..., 0.4008, 0.3969, 0.0000],\n",
      "           [0.1006, 0.1674, 0.1145,  ..., 0.2582, 0.1476, 0.2088],\n",
      "           [0.1537, 0.2485, 0.3027,  ..., 0.0464, 0.0903, 0.1674],\n",
      "           ...,\n",
      "           [0.1723, 0.0000, 0.2386,  ..., 0.0545, 0.2147, 0.2799],\n",
      "           [0.1038, 0.0000, 0.3558,  ..., 0.0543, 0.3493, 0.3684],\n",
      "           [0.1610, 0.3664, 0.2135,  ..., 0.2219, 0.2329, 0.2817]],\n",
      "\n",
      "          [[0.3470, 0.3848, 0.1061,  ..., 0.4581, 0.5670, 0.1918],\n",
      "           [0.1509, 0.1674, 0.3436,  ..., 0.6197, 0.3444, 0.1566],\n",
      "           [0.1537, 0.2485, 0.2522,  ..., 0.0464, 0.0903, 0.1116],\n",
      "           ...,\n",
      "           [0.1723, 0.2233, 0.1790,  ..., 0.2724, 0.4831, 0.5599],\n",
      "           [0.0519, 0.3091, 0.2033,  ..., 0.3802, 0.4076, 0.3684],\n",
      "           [0.2684, 0.2617, 0.3736,  ..., 0.3328, 0.2911, 0.2817]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.0496, 0.2748, 0.2654,  ..., 0.2863, 0.2268, 0.2877],\n",
      "           [0.3019, 0.2791, 0.2290,  ..., 0.1549, 0.0984, 0.3132],\n",
      "           [0.2049, 0.0000, 0.0000,  ..., 0.0927, 0.0451, 0.2791],\n",
      "           ...,\n",
      "           [0.4596, 0.4465, 0.2983,  ..., 0.2179, 0.1610, 0.1680],\n",
      "           [0.4673, 0.1325, 0.0508,  ..., 0.3259, 0.2911, 0.1053],\n",
      "           [0.1074, 0.2094, 0.1601,  ..., 0.1664, 0.2329, 0.3381]],\n",
      "\n",
      "          [[0.0000, 0.0000, 0.1592,  ..., 0.1718, 0.0567, 0.0000],\n",
      "           [0.0503, 0.2791, 0.2863,  ..., 0.0516, 0.0000, 0.0000],\n",
      "           [0.0512, 0.0000, 0.0000,  ..., 0.0464, 0.0000, 0.2791],\n",
      "           ...,\n",
      "           [0.2872, 0.2233, 0.2386,  ..., 0.1089, 0.0000, 0.0560],\n",
      "           [0.1558, 0.0000, 0.0000,  ..., 0.1629, 0.1164, 0.1053],\n",
      "           [0.1074, 0.0000, 0.0000,  ..., 0.0555, 0.1164, 0.0000]],\n",
      "\n",
      "          [[0.0496, 0.0550, 0.0531,  ..., 0.0573, 0.0567, 0.0479],\n",
      "           [0.0503, 0.0558, 0.0573,  ..., 0.0516, 0.0492, 0.0522],\n",
      "           [0.0512, 0.0497, 0.0504,  ..., 0.0000, 0.0451, 0.0558],\n",
      "           ...,\n",
      "           [0.0574, 0.0558, 0.0597,  ..., 0.0545, 0.0537, 0.0560],\n",
      "           [0.0519, 0.0442, 0.0508,  ..., 0.0543, 0.0582, 0.0000],\n",
      "           [0.0537, 0.0523, 0.0534,  ..., 0.0555, 0.0582, 0.0563]]],\n",
      "\n",
      "\n",
      "         [[[0.2808, 0.2254, 0.2676,  ..., 0.0504, 0.1558, 0.0504],\n",
      "           [0.2941, 0.2077, 0.1491,  ..., 0.0510, 0.1041, 0.1019],\n",
      "           [0.2141, 0.2808, 0.0529,  ..., 0.1056, 0.0482, 0.0474],\n",
      "           ...,\n",
      "           [0.1127, 0.0472, 0.0433,  ..., 0.1041, 0.0522, 0.0507],\n",
      "           [0.1583, 0.0531, 0.0466,  ..., 0.1149, 0.0542, 0.1639],\n",
      "           [0.2808, 0.2179, 0.0936,  ..., 0.0550, 0.2748, 0.2018]],\n",
      "\n",
      "          [[0.2247, 0.1690, 0.3212,  ..., 0.1513, 0.3115, 0.1009],\n",
      "           [0.1176, 0.0000, 0.0497,  ..., 0.0510, 0.3123, 0.2548],\n",
      "           [0.1606, 0.1685, 0.1588,  ..., 0.3694, 0.0000, 0.0000],\n",
      "           ...,\n",
      "           [0.2254, 0.0472, 0.0000,  ..., 0.1562, 0.4176, 0.3549],\n",
      "           [0.3167, 0.2654, 0.0000,  ..., 0.1723, 0.0542, 0.3278],\n",
      "           [0.1685, 0.4358, 0.0000,  ..., 0.4947, 0.1649, 0.4035]],\n",
      "\n",
      "          [[0.3370, 0.2817, 0.3747,  ..., 0.0000, 0.3115, 0.3027],\n",
      "           [0.3529, 0.3115, 0.1988,  ..., 0.0510, 0.4165, 0.5096],\n",
      "           [0.3747, 0.3370, 0.1588,  ..., 0.3694, 0.1927, 0.2844],\n",
      "           ...,\n",
      "           [0.2254, 0.2360, 0.2166,  ..., 0.4165, 0.5742, 0.3042],\n",
      "           [0.3694, 0.1061, 0.1397,  ..., 0.1723, 0.2708, 0.2732],\n",
      "           [0.4493, 0.2179, 0.0468,  ..., 0.2748, 0.4947, 0.2522]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.2808, 0.2817, 0.0535,  ..., 0.4035, 0.1038, 0.1513],\n",
      "           [0.3529, 0.2077, 0.1491,  ..., 0.4077, 0.1041, 0.0510],\n",
      "           [0.2141, 0.2247, 0.2117,  ..., 0.0000, 0.0963, 0.0948],\n",
      "           ...,\n",
      "           [0.2817, 0.2832, 0.3032,  ..., 0.2603, 0.0522, 0.1521],\n",
      "           [0.1056, 0.3715, 0.2329,  ..., 0.4596, 0.2708, 0.2732],\n",
      "           [0.1123, 0.2179, 0.3742,  ..., 0.2748, 0.2199, 0.0504]],\n",
      "\n",
      "          [[0.0562, 0.1127, 0.1606,  ..., 0.2522, 0.0000, 0.0504],\n",
      "           [0.1765, 0.2596, 0.1988,  ..., 0.2039, 0.0521, 0.2039],\n",
      "           [0.0000, 0.1123, 0.2117,  ..., 0.0528, 0.0482, 0.0948],\n",
      "           ...,\n",
      "           [0.1690, 0.0000, 0.0000,  ..., 0.1041, 0.0522, 0.1521],\n",
      "           [0.0000, 0.2123, 0.1863,  ..., 0.3447, 0.3249, 0.0000],\n",
      "           [0.1123, 0.0545, 0.1871,  ..., 0.0550, 0.3848, 0.0504]],\n",
      "\n",
      "          [[0.0000, 0.0000, 0.0535,  ..., 0.0504, 0.0519, 0.0504],\n",
      "           [0.0588, 0.0000, 0.0497,  ..., 0.0510, 0.0521, 0.0510],\n",
      "           [0.0535, 0.0562, 0.0529,  ..., 0.0528, 0.0482, 0.0474],\n",
      "           ...,\n",
      "           [0.0563, 0.0472, 0.0433,  ..., 0.0521, 0.0522, 0.0507],\n",
      "           [0.0528, 0.0531, 0.0466,  ..., 0.0574, 0.0542, 0.0546],\n",
      "           [0.0562, 0.0545, 0.0468,  ..., 0.0550, 0.0550, 0.0504]]]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# print(input)\n",
    "hist_pooling_res = rmac_hist(input)\n",
    "print(hist_pooling_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 10, 16, 16])\n",
      "torch.Size([1, 4, 10, 16, 16])\n",
      "torch.Size([1, 4, 64, 64])\n",
      "tensor([[[[[0.2212, 0.1701, 0.2321,  ..., 0.1729, 0.0977, 0.1517],\n",
      "           [0.1074, 0.1712, 0.3259,  ..., 0.2141, 0.0489, 0.1044],\n",
      "           [0.2233, 0.2276, 0.0526,  ..., 0.0475, 0.1099, 0.1041],\n",
      "           ...,\n",
      "           [0.1068, 0.2240, 0.2166,  ..., 0.1579, 0.1644, 0.0493],\n",
      "           [0.1735, 0.1712, 0.1138,  ..., 0.2776, 0.2192, 0.1123],\n",
      "           [0.1588, 0.1071, 0.2276,  ..., 0.2632, 0.2179, 0.2173]],\n",
      "\n",
      "          [[0.0553, 0.1701, 0.3482,  ..., 0.1153, 0.0489, 0.1011],\n",
      "           [0.2147, 0.2854, 0.1629,  ..., 0.0535, 0.1466, 0.1044],\n",
      "           [0.0558, 0.1707, 0.1579,  ..., 0.1900, 0.1649, 0.1041],\n",
      "           ...,\n",
      "           [0.1601, 0.0000, 0.0542,  ..., 0.2632, 0.1644, 0.0987],\n",
      "           [0.2892, 0.0571, 0.1138,  ..., 0.3702, 0.2192, 0.3370],\n",
      "           [0.1059, 0.2141, 0.2276,  ..., 0.2632, 0.1089, 0.3259]],\n",
      "\n",
      "          [[0.3871, 0.5103, 0.3482,  ..., 0.3458, 0.3420, 0.4046],\n",
      "           [0.1610, 0.3995, 0.5431,  ..., 0.3212, 0.2443, 0.2610],\n",
      "           [0.3349, 0.3413, 0.2105,  ..., 0.1425, 0.3848, 0.4165],\n",
      "           ...,\n",
      "           [0.3203, 0.2799, 0.4332,  ..., 0.3158, 0.5480, 0.5426],\n",
      "           [0.3470, 0.2854, 0.2844,  ..., 0.3239, 0.5480, 0.4493],\n",
      "           [0.5293, 0.4818, 0.3982,  ..., 0.4211, 0.3268, 0.1629]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.3871, 0.2268, 0.2321,  ..., 0.2882, 0.1466, 0.2023],\n",
      "           [0.2147, 0.3424, 0.1086,  ..., 0.2141, 0.1466, 0.2610],\n",
      "           [0.2791, 0.2276, 0.2632,  ..., 0.0475, 0.1099, 0.3123],\n",
      "           ...,\n",
      "           [0.2135, 0.2799, 0.2708,  ..., 0.0526, 0.0548, 0.0987],\n",
      "           [0.2892, 0.2283, 0.3413,  ..., 0.0000, 0.1096, 0.2247],\n",
      "           [0.1588, 0.1606, 0.2844,  ..., 0.1053, 0.1634, 0.3802]],\n",
      "\n",
      "          [[0.0553, 0.1134, 0.0580,  ..., 0.2306, 0.0000, 0.0000],\n",
      "           [0.0537, 0.1141, 0.0543,  ..., 0.2141, 0.0000, 0.0000],\n",
      "           [0.1674, 0.2276, 0.0526,  ..., 0.2851, 0.1649, 0.0000],\n",
      "           ...,\n",
      "           [0.1601, 0.2799, 0.0000,  ..., 0.1053, 0.1644, 0.0493],\n",
      "           [0.0578, 0.3995, 0.3413,  ..., 0.0000, 0.0548, 0.0562],\n",
      "           [0.0000, 0.0000, 0.0569,  ..., 0.0000, 0.0545, 0.0543]],\n",
      "\n",
      "          [[0.0553, 0.0567, 0.0580,  ..., 0.0576, 0.0489, 0.0506],\n",
      "           [0.0537, 0.0571, 0.0543,  ..., 0.0535, 0.0489, 0.0522],\n",
      "           [0.0000, 0.0569, 0.0526,  ..., 0.0000, 0.0550, 0.0521],\n",
      "           ...,\n",
      "           [0.0000, 0.0560, 0.0542,  ..., 0.0526, 0.0548, 0.0493],\n",
      "           [0.0578, 0.0571, 0.0000,  ..., 0.0463, 0.0548, 0.0562],\n",
      "           [0.0529, 0.0535, 0.0569,  ..., 0.0526, 0.0545, 0.0543]]],\n",
      "\n",
      "\n",
      "         [[[0.3391, 0.2403, 0.2306,  ..., 0.1138, 0.0952, 0.0468],\n",
      "           [0.3212, 0.2669, 0.0508,  ..., 0.1116, 0.1011, 0.0931],\n",
      "           [0.2596, 0.1610, 0.1006,  ..., 0.2135, 0.2105, 0.2589],\n",
      "           ...,\n",
      "           [0.1747, 0.1696, 0.0484,  ..., 0.1701, 0.1068, 0.0497],\n",
      "           [0.2361, 0.2233, 0.2941,  ..., 0.0567, 0.1664, 0.2123],\n",
      "           [0.2306, 0.1083, 0.1649,  ..., 0.1025, 0.4237, 0.3370]],\n",
      "\n",
      "          [[0.2826, 0.3004, 0.3458,  ..., 0.1707, 0.2381, 0.0936],\n",
      "           [0.2676, 0.2669, 0.0000,  ..., 0.1116, 0.1517, 0.3260],\n",
      "           [0.3634, 0.0537, 0.0000,  ..., 0.1068, 0.1053, 0.2071],\n",
      "           ...,\n",
      "           [0.3493, 0.3957, 0.1452,  ..., 0.2835, 0.2669, 0.3478],\n",
      "           [0.1771, 0.2233, 0.2353,  ..., 0.3402, 0.3328, 0.1592],\n",
      "           [0.1729, 0.1625, 0.0550,  ..., 0.2562, 0.1816, 0.3370]],\n",
      "\n",
      "          [[0.1696, 0.4206, 0.2882,  ..., 0.3982, 0.5238, 0.3742],\n",
      "           [0.3212, 0.5871, 0.2033,  ..., 0.5023, 0.4551, 0.5123],\n",
      "           [0.5192, 0.2684, 0.0503,  ..., 0.4804, 0.4737, 0.5178],\n",
      "           ...,\n",
      "           [0.2911, 0.3391, 0.4355,  ..., 0.3969, 0.4804, 0.3975],\n",
      "           [0.3542, 0.1116, 0.3529,  ..., 0.2268, 0.4438, 0.4246],\n",
      "           [0.2306, 0.2166, 0.2199,  ..., 0.1537, 0.2421, 0.3932]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.1696, 0.3004, 0.1729,  ..., 0.3413, 0.0476, 0.0000],\n",
      "           [0.1606, 0.2135, 0.2033,  ..., 0.3907, 0.0506, 0.0000],\n",
      "           [0.0519, 0.3758, 0.6038,  ..., 0.3203, 0.2632, 0.1553],\n",
      "           ...,\n",
      "           [0.1747, 0.2261, 0.0000,  ..., 0.3969, 0.1068, 0.0497],\n",
      "           [0.3542, 0.2791, 0.3529,  ..., 0.3402, 0.0555, 0.0531],\n",
      "           [0.4611, 0.3249, 0.4397,  ..., 0.3586, 0.4237, 0.1685]],\n",
      "\n",
      "          [[0.2261, 0.1803, 0.2306,  ..., 0.1707, 0.0000, 0.0000],\n",
      "           [0.0000, 0.0534, 0.1525,  ..., 0.1674, 0.0000, 0.0000],\n",
      "           [0.0000, 0.1074, 0.2516,  ..., 0.0000, 0.0526, 0.0000],\n",
      "           ...,\n",
      "           [0.1747, 0.0565, 0.0484,  ..., 0.2268, 0.0534, 0.0000],\n",
      "           [0.1771, 0.1116, 0.0588,  ..., 0.2835, 0.0555, 0.0000],\n",
      "           [0.2882, 0.0542, 0.1099,  ..., 0.1025, 0.3026, 0.0000]],\n",
      "\n",
      "          [[0.0565, 0.0601, 0.0576,  ..., 0.0000, 0.0476, 0.0468],\n",
      "           [0.0535, 0.0534, 0.0508,  ..., 0.0000, 0.0506, 0.0466],\n",
      "           [0.0519, 0.0537, 0.0503,  ..., 0.0534, 0.0526, 0.0518],\n",
      "           ...,\n",
      "           [0.0582, 0.0565, 0.0484,  ..., 0.0567, 0.0534, 0.0497],\n",
      "           [0.0590, 0.0558, 0.0588,  ..., 0.0567, 0.0555, 0.0531],\n",
      "           [0.0000, 0.0542, 0.0550,  ..., 0.0512, 0.0605, 0.0562]]],\n",
      "\n",
      "\n",
      "         [[[0.0991, 0.1649, 0.1592,  ..., 0.2290, 0.3969, 0.0959],\n",
      "           [0.0503, 0.2791, 0.1145,  ..., 0.1549, 0.0492, 0.1044],\n",
      "           [0.0512, 0.1988, 0.4540,  ..., 0.1855, 0.1805, 0.2791],\n",
      "           ...,\n",
      "           [0.1723, 0.1674, 0.2386,  ..., 0.2724, 0.4295, 0.4479],\n",
      "           [0.1038, 0.0883, 0.1525,  ..., 0.0543, 0.1747, 0.0526],\n",
      "           [0.1074, 0.0523, 0.1068,  ..., 0.1109, 0.2329, 0.2254]],\n",
      "\n",
      "          [[0.1487, 0.2199, 0.1592,  ..., 0.4008, 0.3969, 0.0000],\n",
      "           [0.1006, 0.1674, 0.1145,  ..., 0.2582, 0.1476, 0.2088],\n",
      "           [0.1537, 0.2485, 0.3027,  ..., 0.0464, 0.0903, 0.1674],\n",
      "           ...,\n",
      "           [0.1723, 0.0000, 0.2386,  ..., 0.0545, 0.2147, 0.2799],\n",
      "           [0.1038, 0.0000, 0.3558,  ..., 0.0543, 0.3493, 0.3684],\n",
      "           [0.1610, 0.3664, 0.2135,  ..., 0.2219, 0.2329, 0.2817]],\n",
      "\n",
      "          [[0.3470, 0.3848, 0.1061,  ..., 0.4581, 0.5670, 0.1918],\n",
      "           [0.1509, 0.1674, 0.3436,  ..., 0.6197, 0.3444, 0.1566],\n",
      "           [0.1537, 0.2485, 0.2522,  ..., 0.0464, 0.0903, 0.1116],\n",
      "           ...,\n",
      "           [0.1723, 0.2233, 0.1790,  ..., 0.2724, 0.4831, 0.5599],\n",
      "           [0.0519, 0.3091, 0.2033,  ..., 0.3802, 0.4076, 0.3684],\n",
      "           [0.2684, 0.2617, 0.3736,  ..., 0.3328, 0.2911, 0.2817]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.0496, 0.2748, 0.2654,  ..., 0.2863, 0.2268, 0.2877],\n",
      "           [0.3019, 0.2791, 0.2290,  ..., 0.1549, 0.0984, 0.3132],\n",
      "           [0.2049, 0.0000, 0.0000,  ..., 0.0927, 0.0451, 0.2791],\n",
      "           ...,\n",
      "           [0.4596, 0.4465, 0.2983,  ..., 0.2179, 0.1610, 0.1680],\n",
      "           [0.4673, 0.1325, 0.0508,  ..., 0.3259, 0.2911, 0.1053],\n",
      "           [0.1074, 0.2094, 0.1601,  ..., 0.1664, 0.2329, 0.3381]],\n",
      "\n",
      "          [[0.0000, 0.0000, 0.1592,  ..., 0.1718, 0.0567, 0.0000],\n",
      "           [0.0503, 0.2791, 0.2863,  ..., 0.0516, 0.0000, 0.0000],\n",
      "           [0.0512, 0.0000, 0.0000,  ..., 0.0464, 0.0000, 0.2791],\n",
      "           ...,\n",
      "           [0.2872, 0.2233, 0.2386,  ..., 0.1089, 0.0000, 0.0560],\n",
      "           [0.1558, 0.0000, 0.0000,  ..., 0.1629, 0.1164, 0.1053],\n",
      "           [0.1074, 0.0000, 0.0000,  ..., 0.0555, 0.1164, 0.0000]],\n",
      "\n",
      "          [[0.0496, 0.0550, 0.0531,  ..., 0.0573, 0.0567, 0.0479],\n",
      "           [0.0503, 0.0558, 0.0573,  ..., 0.0516, 0.0492, 0.0522],\n",
      "           [0.0512, 0.0497, 0.0504,  ..., 0.0000, 0.0451, 0.0558],\n",
      "           ...,\n",
      "           [0.0574, 0.0558, 0.0597,  ..., 0.0545, 0.0537, 0.0560],\n",
      "           [0.0519, 0.0442, 0.0508,  ..., 0.0543, 0.0582, 0.0000],\n",
      "           [0.0537, 0.0523, 0.0534,  ..., 0.0555, 0.0582, 0.0563]]],\n",
      "\n",
      "\n",
      "         [[[0.2808, 0.2254, 0.2676,  ..., 0.0504, 0.1558, 0.0504],\n",
      "           [0.2941, 0.2077, 0.1491,  ..., 0.0510, 0.1041, 0.1019],\n",
      "           [0.2141, 0.2808, 0.0529,  ..., 0.1056, 0.0482, 0.0474],\n",
      "           ...,\n",
      "           [0.1127, 0.0472, 0.0433,  ..., 0.1041, 0.0522, 0.0507],\n",
      "           [0.1583, 0.0531, 0.0466,  ..., 0.1149, 0.0542, 0.1639],\n",
      "           [0.2808, 0.2179, 0.0936,  ..., 0.0550, 0.2748, 0.2018]],\n",
      "\n",
      "          [[0.2247, 0.1690, 0.3212,  ..., 0.1513, 0.3115, 0.1009],\n",
      "           [0.1176, 0.0000, 0.0497,  ..., 0.0510, 0.3123, 0.2548],\n",
      "           [0.1606, 0.1685, 0.1588,  ..., 0.3694, 0.0000, 0.0000],\n",
      "           ...,\n",
      "           [0.2254, 0.0472, 0.0000,  ..., 0.1562, 0.4176, 0.3549],\n",
      "           [0.3167, 0.2654, 0.0000,  ..., 0.1723, 0.0542, 0.3278],\n",
      "           [0.1685, 0.4358, 0.0000,  ..., 0.4947, 0.1649, 0.4035]],\n",
      "\n",
      "          [[0.3370, 0.2817, 0.3747,  ..., 0.0000, 0.3115, 0.3027],\n",
      "           [0.3529, 0.3115, 0.1988,  ..., 0.0510, 0.4165, 0.5096],\n",
      "           [0.3747, 0.3370, 0.1588,  ..., 0.3694, 0.1927, 0.2844],\n",
      "           ...,\n",
      "           [0.2254, 0.2360, 0.2166,  ..., 0.4165, 0.5742, 0.3042],\n",
      "           [0.3694, 0.1061, 0.1397,  ..., 0.1723, 0.2708, 0.2732],\n",
      "           [0.4493, 0.2179, 0.0468,  ..., 0.2748, 0.4947, 0.2522]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.2808, 0.2817, 0.0535,  ..., 0.4035, 0.1038, 0.1513],\n",
      "           [0.3529, 0.2077, 0.1491,  ..., 0.4077, 0.1041, 0.0510],\n",
      "           [0.2141, 0.2247, 0.2117,  ..., 0.0000, 0.0963, 0.0948],\n",
      "           ...,\n",
      "           [0.2817, 0.2832, 0.3032,  ..., 0.2603, 0.0522, 0.1521],\n",
      "           [0.1056, 0.3715, 0.2329,  ..., 0.4596, 0.2708, 0.2732],\n",
      "           [0.1123, 0.2179, 0.3742,  ..., 0.2748, 0.2199, 0.0504]],\n",
      "\n",
      "          [[0.0562, 0.1127, 0.1606,  ..., 0.2522, 0.0000, 0.0504],\n",
      "           [0.1765, 0.2596, 0.1988,  ..., 0.2039, 0.0521, 0.2039],\n",
      "           [0.0000, 0.1123, 0.2117,  ..., 0.0528, 0.0482, 0.0948],\n",
      "           ...,\n",
      "           [0.1690, 0.0000, 0.0000,  ..., 0.1041, 0.0522, 0.1521],\n",
      "           [0.0000, 0.2123, 0.1863,  ..., 0.3447, 0.3249, 0.0000],\n",
      "           [0.1123, 0.0545, 0.1871,  ..., 0.0550, 0.3848, 0.0504]],\n",
      "\n",
      "          [[0.0000, 0.0000, 0.0535,  ..., 0.0504, 0.0519, 0.0504],\n",
      "           [0.0588, 0.0000, 0.0497,  ..., 0.0510, 0.0521, 0.0510],\n",
      "           [0.0535, 0.0562, 0.0529,  ..., 0.0528, 0.0482, 0.0474],\n",
      "           ...,\n",
      "           [0.0563, 0.0472, 0.0433,  ..., 0.0521, 0.0522, 0.0507],\n",
      "           [0.0528, 0.0531, 0.0466,  ..., 0.0574, 0.0542, 0.0546],\n",
      "           [0.0562, 0.0545, 0.0468,  ..., 0.0550, 0.0550, 0.0504]]]]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 7.1264e-01, -1.4829e+00,  7.5374e-01,  ..., -3.9685e-01,\n",
       "           -1.4078e+00,  1.3761e+00],\n",
       "          [ 7.5623e-01, -1.0336e+00, -9.4676e-01,  ..., -7.1415e-01,\n",
       "           -9.4391e-01, -6.2532e-01],\n",
       "          [-1.5443e+00, -1.8265e+00, -1.1229e-01,  ..., -1.0241e+00,\n",
       "           -4.0456e-01,  4.8919e-01],\n",
       "          ...,\n",
       "          [ 9.7460e-01,  1.8843e+00, -1.0724e-01,  ...,  1.1198e+00,\n",
       "            1.2194e-01,  1.2331e+00],\n",
       "          [-1.9268e-01,  4.1561e-01,  4.9510e-01,  ..., -2.0681e+00,\n",
       "           -1.9003e+00,  1.3527e+00],\n",
       "          [ 7.2349e-01, -1.9242e-02,  1.3629e+00,  ..., -2.2780e-02,\n",
       "           -1.3694e+00, -4.0969e-01]],\n",
       "\n",
       "         [[-4.2746e-01,  8.1367e-01, -9.8953e-01,  ..., -1.5456e-03,\n",
       "            8.3861e-01, -2.3021e+00],\n",
       "          [ 1.1974e+00, -1.2911e-02,  1.3376e-01,  ..., -1.1046e+00,\n",
       "            1.8243e+00,  1.1191e+00],\n",
       "          [ 1.1517e+00,  1.6706e+00,  2.2423e-01,  ...,  8.6491e-01,\n",
       "           -1.6997e-01,  1.2227e+00],\n",
       "          ...,\n",
       "          [ 7.7837e-02,  7.7206e-01,  1.2233e+00,  ...,  7.6639e-01,\n",
       "           -1.4176e+00,  9.5543e-01],\n",
       "          [ 7.4047e-01,  4.0757e-01,  8.3048e-01,  ...,  8.4930e-01,\n",
       "           -5.5404e-01,  1.5584e+00],\n",
       "          [-2.2956e-01,  1.2923e-01, -5.7797e-01,  ...,  6.1133e-01,\n",
       "            1.4985e+00,  5.9028e-01]],\n",
       "\n",
       "         [[ 2.7377e+00,  2.2727e-01, -3.3639e-02,  ..., -1.2035e-01,\n",
       "           -6.3444e-01, -2.7543e-01],\n",
       "          [ 1.4812e+00, -2.8966e-01,  4.2606e-01,  ...,  5.6247e-01,\n",
       "           -2.9716e+00, -7.9896e-01],\n",
       "          [ 1.8651e-01, -2.2219e+00,  8.6411e-01,  ..., -2.7015e+00,\n",
       "           -1.5332e+00, -6.3354e-01],\n",
       "          ...,\n",
       "          [ 3.9270e-01,  6.7907e-01, -6.9641e-02,  ...,  6.3937e-01,\n",
       "            2.5911e-01, -1.5424e-01],\n",
       "          [ 1.0636e+00, -7.7276e-01, -8.6450e-01,  ...,  1.0776e+00,\n",
       "           -6.6766e-02,  5.7780e-01],\n",
       "          [-4.1169e-01, -3.5559e-01,  6.5106e-01,  ..., -1.5039e+00,\n",
       "           -4.2637e-01,  6.5196e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-5.9356e-02,  1.2766e-01,  5.1806e-02,  ...,  2.4987e-02,\n",
       "            1.1283e-01,  5.6092e-02],\n",
       "          [ 1.9516e-01,  1.3221e-01, -1.9710e-01,  ...,  1.4717e-01,\n",
       "           -2.0236e-01,  5.6092e-02],\n",
       "          [-5.3010e-02, -6.7082e-03, -1.1441e-03,  ..., -1.0647e-01,\n",
       "           -7.2903e-02,  5.6092e-02],\n",
       "          ...,\n",
       "          [ 1.8419e-01,  1.2416e-01, -2.2047e-01,  ...,  9.8927e-02,\n",
       "           -1.4577e-01,  5.6092e-02],\n",
       "          [-4.8137e-02, -7.1503e-02, -2.1919e-03,  ...,  2.2418e-02,\n",
       "           -3.0567e-02,  5.6092e-02],\n",
       "          [ 5.6092e-02,  5.6092e-02,  5.6092e-02,  ...,  5.6092e-02,\n",
       "            5.6092e-02,  5.6092e-02]],\n",
       "\n",
       "         [[-1.8537e-01,  1.1592e-01, -1.5249e-01,  ...,  2.1812e-01,\n",
       "            8.7877e-03,  2.9478e-03],\n",
       "          [ 7.6627e-02, -3.6765e-01,  1.1378e-01,  ..., -3.4293e-01,\n",
       "            1.4631e-01,  2.9478e-03],\n",
       "          [-7.9536e-02, -1.3862e-01, -1.6674e-01,  ..., -5.6100e-02,\n",
       "           -2.9817e-01,  2.9478e-03],\n",
       "          ...,\n",
       "          [ 1.1408e-01, -3.8175e-01,  1.0367e-01,  ..., -3.2225e-01,\n",
       "            1.0469e-01,  2.9478e-03],\n",
       "          [-5.0700e-02, -1.3606e-01, -2.1608e-01,  ..., -1.9097e-02,\n",
       "           -2.2932e-01,  2.9478e-03],\n",
       "          [ 2.9478e-03,  2.9478e-03,  2.9478e-03,  ...,  2.9478e-03,\n",
       "            2.9478e-03,  2.9478e-03]],\n",
       "\n",
       "         [[-7.1629e-02, -8.0969e-02,  7.0002e-03,  ..., -2.3007e-02,\n",
       "           -3.2820e-02, -6.6129e-02],\n",
       "          [-3.0843e-01, -9.8531e-02,  5.5131e-02,  ..., -4.6129e-04,\n",
       "            1.0484e-01, -6.6129e-02],\n",
       "          [-1.2626e-01, -2.4731e-01,  3.0190e-02,  ..., -2.0674e-01,\n",
       "            3.4484e-02, -6.6129e-02],\n",
       "          ...,\n",
       "          [-3.0067e-01, -9.0801e-02,  5.9096e-02,  ..., -1.4830e-01,\n",
       "            3.1138e-03, -6.6129e-02],\n",
       "          [-9.8098e-02, -2.1617e-01,  4.3040e-02,  ..., -1.8305e-01,\n",
       "            5.4400e-02, -6.6129e-02],\n",
       "          [-6.6129e-02, -6.6129e-02, -6.6129e-02,  ..., -6.6129e-02,\n",
       "           -6.6129e-02, -6.6129e-02]]]], device='cuda:0',\n",
       "       grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "m=nn.AdaptiveMaxPool3d((None,1,1))\n",
    "conv_du = nn.Sequential(\n",
    "                nn.Conv3d(4, 4, 1, bias=True),\n",
    "#                 nn.ReLU(inplace=True),\n",
    "#                 nn.Conv3d(2, 4, 1, bias=True),\n",
    "                nn.Sigmoid()\n",
    "        ).cuda()\n",
    "redu_hist3d = conv_du(hist_pooling_res)\n",
    "print(redu_hist3d.shape)\n",
    "print(hist_pooling_res.shape)\n",
    "\n",
    "attn_res = hist_pooling_res*m(redu_hist3d)\n",
    "\n",
    "attn_res_reduced = attn_res.sum(1)\n",
    "upsample = nn.ConvTranspose2d(10, 10, 3, stride=np.rint(input.shape[-1]//attn_res_reduced.shape[-1]).astype(int)).cuda()\n",
    "print(input.size())\n",
    "upsampled_hist = upsample(attn_res_reduced, output_size=input.size())\n",
    "# upsampled_hist\n",
    "print(hist_pooling_res)\n",
    "torch.cat((input,nn.SELU(inplace=True)(upsampled_hist).cuda()), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-5.3928, -2.7985, -1.2736],\n",
      "          [ 0.5560, -0.2581,  1.7089],\n",
      "          [ 1.4241,  2.0407,  4.7615]],\n",
      "\n",
      "         [[ 1.4115,  0.0413,  5.8817],\n",
      "          [-1.0436, -0.8633,  4.0784],\n",
      "          [-2.0931, -3.9664, -0.8548]],\n",
      "\n",
      "         [[ 0.7466,  0.6219,  1.7725],\n",
      "          [-0.6392, -0.4786,  0.6590],\n",
      "          [-0.8573, -1.3902, -0.8062]],\n",
      "\n",
      "         [[-1.6163,  3.4959,  1.1526],\n",
      "          [-0.4965,  4.5342, -1.9171],\n",
      "          [ 1.5929, -2.4109, -4.6280]],\n",
      "\n",
      "         [[-3.0557, -3.7205,  1.6888],\n",
      "          [-3.0168, -0.6681,  3.7868],\n",
      "          [ 2.8815,  2.3262,  3.0939]],\n",
      "\n",
      "         [[ 0.1994, -2.7422, -3.0247],\n",
      "          [ 0.2476,  2.5019, -0.0529],\n",
      "          [ 3.9685,  4.0618,  0.5308]]]], grad_fn=<MkldnnConvolutionBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1570910687650/work/torch/csrc/autograd/python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-219-0da0f44ef35e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(output)\n",
    "a = cnn.forward(input)\n",
    "print(a.shape)\n",
    "loss = loss_func(output.sum(), target)\n",
    "loss.backward(retain_graph=True)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 50 + 25 * torch.randn(1,1000)\n",
    "\n",
    "hist = torch.histc(data, bins=10, min=0, max=100)\n",
    "\n",
    "print(data)\n",
    "print(hist)\n",
    "\n",
    "class SoftHistogram(nn.Module):\n",
    "    def __init__(self, bins, min, max, sigma):\n",
    "        super(SoftHistogram, self).__init__()\n",
    "        self.bins = bins\n",
    "        self.min = min\n",
    "        self.max = max\n",
    "        self.sigma = sigma\n",
    "        self.delta = float(max - min) / float(bins)\n",
    "        self.centers = float(min) + self.delta * (torch.arange(bins).float() + 0.5)\n",
    "        self.centers = nn.Parameter(self.centers, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.unsqueeze(x, 1) - torch.unsqueeze(self.centers, 1)\n",
    "        x = torch.exp(-0.5*(x/self.sigma)**2) / (self.sigma * np.sqrt(np.pi*2)) * self.delta\n",
    "        x = x.sum(dim=-1)\n",
    "        x = x/x.sum(dim=-1).unsqueeze(1) # normalization\n",
    "        return x\n",
    "\n",
    "softhist = SoftHistogram(bins=10, min=-2, max=2, sigma=3*25)\n",
    "\n",
    "data.requires_grad = True\n",
    "hist = softhist(data)\n",
    "print(50 + 25 * hist[0])\n",
    "\n",
    "hist.sum().backward()\n",
    "print(data.grad.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn.apply(initialize_weights)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = get_variable(images)\n",
    "        labels = get_variable(labels)\n",
    "#         print(images.shape)\n",
    "#         print(labels.shape)\n",
    "        outputs = cnn(images)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f'\n",
    "                  % (epoch + 1, num_epochs, i + 1, len(train_dataset) // batch_size, loss.item()))\n",
    "\n",
    "# 测试模型\n",
    "cnn.eval()  # 改成测试形态, 应用场景如: dropout\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = get_variable(images)\n",
    "    labels = get_variable(labels)\n",
    "\n",
    "    outputs = cnn(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels.data).sum()\n",
    "\n",
    "print(' test acc %d %%' % (100 * correct / total))\n",
    "\n",
    "# Save the Trained Model\n",
    "torch.save(cnn.state_dict(), 'cnn.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d_Hist(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=1, bias=True)\n",
      ")\n",
      "tensor(1.4422, grad_fn=<MseLossBackward>)\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Mismatch in shape: grad_output[0] has a shape of torch.Size([4, 1]) and output[0] has a shape of torch.Size([]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-c319546a5652>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     27\u001b[0m                                    \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" and output[\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                                    \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"] has a shape of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                                    + str(out.shape) + \".\")\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Mismatch in shape: grad_output[0] has a shape of torch.Size([4, 1]) and output[0] has a shape of torch.Size([])."
     ]
    }
   ],
   "source": [
    "batchsize = 4\n",
    "nb_in_channel = 3\n",
    "n_classes = 1\n",
    "net = Conv2d_Hist(in_channels=nb_in_channel, n_classes=n_classes)\n",
    "print(net)\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "input = torch.randn(batchsize, nb_in_channel, 32, 32)\n",
    "net.zero_grad()\n",
    "\n",
    "output = net(input)\n",
    "\n",
    "target = torch.randn(batchsize, n_classes)  # a dummy target, for example\n",
    "# target = target.view(1,-1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)\n",
    "\n",
    "\n",
    "print(output.shape)\n",
    "print(target.shape)\n",
    "\n",
    "loss.backward(torch.randn(output.shape))\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
