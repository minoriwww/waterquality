{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dltdc/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/dltdc/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/dltdc/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/dltdc/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/dltdc/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/dltdc/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/dltdc/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/dltdc/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/dltdc/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/dltdc/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/dltdc/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/dltdc/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import initializers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Conv2DTranspose, Dense, Dropout, Activation, Flatten\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import keras_applications\n",
    "\n",
    "import sklearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import joblib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.client.session.Session object at 0x7f962d862080>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction=0.5\n",
    "\n",
    "# gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "# sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "KTF.set_session(sess)\n",
    "\n",
    "tf.keras.backend.set_session(sess)\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "# IMPORTANT: models have to be loaded AFTER SETTING THE SESSION for keras! \n",
    "# Otherwise, their weights will be unavailable in the threads after the session there has been set\n",
    "set_session(sess)\n",
    "\n",
    "img_height = 512\n",
    "img_width = 512\n",
    "nb_channels = 3 #1 3\n",
    "\n",
    "BATCH_SIZE = 4 #8 16 1\n",
    "RANDOM_STATE = random_state = 42\n",
    "time_str = str(time.time())\n",
    "\n",
    "BIN_SIZE = 20#20\n",
    "MAX_LIM = 200#200\n",
    "MIN_LIM = 20\n",
    "nb_bins = (MAX_LIM - MIN_LIM)/BIN_SIZE\n",
    "\n",
    "IS_SCALE = False\n",
    "nb_epochs = 400\n",
    "print(tf.Session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('WaterQuality/X_train_ratio=0.2.npy')\n",
    "X_test = np.load('WaterQuality/X_test_ratio=0.2.npy')\n",
    "y_train = np.load('WaterQuality/y_train_ratio=0.2.npy')\n",
    "y_test = np.load('WaterQuality/y_test_ratio=0.2.npy')\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=True, random_state=random_state)\n",
    "scaler = preprocessing.MaxAbsScaler()#StandardScaler\n",
    "y_train = scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "y_train = y_train.flatten()\n",
    "\n",
    "joblib.dump(scaler, 'MaxAbsScaler.pkl')\n",
    "\n",
    "# scaler_val = preprocessing.MaxAbsScaler()\n",
    "y_val = scaler.transform(y_val.reshape(-1, 1))\n",
    "y_val = y_val.flatten()\n",
    "\n",
    "\n",
    "y_test = scaler.transform(y_test.reshape(-1, 1))\n",
    "y_test = y_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_g = np.zeros(X_train.shape[:-1])\n",
    "X_val_g = np.zeros(X_val.shape[:-1])\n",
    "X_test_g = np.zeros(X_test.shape[:-1])\n",
    "for i in range(X_train.shape[0]):\n",
    "    X_train_g[i] = cv2.cvtColor(X_train[i], cv2.COLOR_RGB2GRAY)\n",
    "for i in range(X_val.shape[0]):\n",
    "    X_val_g[i] = cv2.cvtColor(X_val[i], cv2.COLOR_RGB2GRAY)\n",
    "for i in range(X_test.shape[0]):\n",
    "    X_test_g[i] = cv2.cvtColor(X_test[i], cv2.COLOR_RGB2GRAY)\n",
    "X_train_g = np.stack([X_train_g]*3, axis=-1)\n",
    "X_val_g = np.stack([X_val_g]*3, axis=-1)\n",
    "X_test_g = np.stack([X_test_g]*3, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_models(train_shape=None, input_shape=None, model_name=\"resnet50\", **kwargs):\n",
    "    \"\"\"\n",
    "    model_name = \"inception_v3\", \"mobilenet\", or \"densenet\" \"resnet50\" \"resnet101\"\n",
    "    \"\"\"\n",
    "\n",
    "    def inner( input_shape=input_shape, model_name=model_name,is_classfication=False, **kwargs):\n",
    "#         nonlocal model\n",
    "#         nonlocal input_shape\n",
    "        if input_shape is None and train_shape is not None:\n",
    "            input_shape = train_shape[1:]\n",
    "        else:\n",
    "            input_shape = (img_width, img_height, nb_channels)\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        if model_name == 'inception_v3':\n",
    "            Kerasmodel = keras.applications.inception_v3.InceptionV3(include_top=False\n",
    "                , weights='imagenet'\n",
    "                # , input_tensor=inputs\n",
    "                , input_shape=input_shape\n",
    "                , pooling=None)\n",
    "        if model_name == 'mobilenet':\n",
    "            Kerasmodel = keras.applications.mobilenet.MobileNet(include_top=False\n",
    "                , weights='imagenet'\n",
    "                # , input_tensor=inputs\n",
    "                , input_shape=input_shape\n",
    "                , pooling=None)\n",
    "        if model_name == 'densenet':\n",
    "            Kerasmodel = keras.applications.densenet.DenseNet121(include_top=False\n",
    "                , weights='imagenet'\n",
    "                # , input_tensor=inputs\n",
    "                , input_shape=input_shape\n",
    "                , pooling=None)\n",
    "        if model_name == 'resnet50':\n",
    "            Kerasmodel = keras.applications.resnet50.ResNet50(include_top=False\n",
    "                , weights='imagenet'\n",
    "                # , input_tensor=inputs\n",
    "                , input_shape=input_shape\n",
    "                , pooling=None)\n",
    "        if model_name == 'resnet101':\n",
    "            Kerasmodel = keras.applications.resnet101.ResNet101(include_top=False\n",
    "                , weights='imagenet'\n",
    "                # , input_tensor=inputs\n",
    "                , input_shape=input_shape\n",
    "                , pooling=None)\n",
    "        model.add(Flatten())\n",
    "        # model.add(Dropout(0.5))\n",
    "#         model.add(Dense(256, kernel_initializer='he_normal', use_bias=True, kernel_regularizer=l2(0.2)))\n",
    "#         model.add(Dropout(0.5))\n",
    "\n",
    "        if is_classfication == False:\n",
    "            print('categorical false')\n",
    "            model.add(Dense(1,kernel_initializer='he_normal'))\n",
    "            model.add(Activation('linear')) #softmax\n",
    "            model = Model( inputs=Kerasmodel.input , outputs=model(Kerasmodel.output) )\n",
    "            # model = Model( inputs=inputs, outputs=result )\n",
    "            model.compile(loss='mean_squared_error', #mse binary_crossentropy Dice-coefficient loss function vs cross-entropy\n",
    "                optimizer=optimizers.Adam(lr=5e-3),\n",
    "                metrics=['mse'])\n",
    "        elif is_classfication == True:\n",
    "            print('categorical true')\n",
    "            model.add(Dense(int(kwargs['nb_classes']), kernel_initializer='he_normal'))\n",
    "            model.add(Activation('sigmoid')) #softmax\n",
    "            model = Model( inputs=Kerasmodel.input , outputs=model(Kerasmodel.output) )\n",
    "\n",
    "            model.compile(loss='binary_crossentropy',\n",
    "                optimizer=optimizers.Adam(lr=5e-3),\n",
    "                metrics=['accuracy'])\n",
    "        else:\n",
    "            raise Exception('No model returned')\n",
    "        print(model.summary())\n",
    "        return model\n",
    "    return inner\n",
    "\n",
    "\n",
    "def run_model(train, label, vali_train, vali_label, test, test_label, model_name=\"resnet50\",  model_fn = VGG16, is_classfication=False, fold=1, nb_classes=1):\n",
    "    model = model_fn( is_classfication=is_classfication, nb_classes=nb_classes)\n",
    "    early_stop = EarlyStopping(monitor='val_mean_squared_error', patience=30)\n",
    "    # model.load_weights('./modelWights/regression_model'+model_fn.__name__+'.h5')\n",
    "    model_checkpoint = ModelCheckpoint('./modelWights/weights_'+model_name+'_fold_'+str(fold)+time_str+'_gray_scale.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "    history = model.fit(train, label, batch_size=BATCH_SIZE, epochs=nb_epochs, validation_data=(vali_train, vali_label), callbacks=[model_checkpoint])\n",
    "    # print(history.history)\n",
    "    import json\n",
    "    with open('./modelWights/history.json', 'w') as f:\n",
    "        json.dump(history.history, f)\n",
    "    y_pred = model.predict(test, batch_size=1, verbose=1)\n",
    "\n",
    "    y_test = test_label\n",
    "    \n",
    "    return y_pred, y_test\n",
    "\n",
    "def post_training_data(y_test, y_pred, is_scaler=True ):\n",
    "    if not is_scaler:\n",
    "        y_test = y_test\n",
    "        y_pred = y_pred\n",
    "    else:\n",
    "        scaler_val = joblib.load('MaxAbsScaler.pkl')\n",
    "        y_test, y_pred = transform_y(y_test, y_pred, scaler_val)\n",
    "\n",
    "    np.save('y_pred.npy', y_pred)\n",
    "    np.save('y_test_transformed.npy', y_test)\n",
    "    print(y_pred.shape)###\n",
    "    print(y_test.shape)###\n",
    "    y_pred = y_pred.flatten()\n",
    "    print(y_pred)###\n",
    "    print(y_test)###\n",
    "    # y_test = y_test.flatten()\n",
    "\n",
    "    # if is_categorical:\n",
    "    #     y_pred = y_pred.reshape(-1,2)\n",
    "    #     y_test = y_test[:,0]\n",
    "    #     y_pred = y_pred[:,0]\n",
    "\n",
    "    ########### margin cut off ###########\n",
    "    y_pred[np.where(y_pred>500)  ] = 500\n",
    "    y_pred[np.where(y_pred<0) ] = 0\n",
    "    return y_pred, y_test\n",
    "\n",
    "def transform_y(y_test, y_pred, scaler_test):\n",
    "    y_test = y_test.reshape(-1, 1)\n",
    "    y_pred = y_pred.reshape(-1, 1)\n",
    "    y_test = scaler_test.inverse_transform(y_test)\n",
    "    y_pred = scaler_test.inverse_transform(y_pred)\n",
    "    y_test = y_test.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "    return y_test, y_pred\n",
    "def save_result(name,y_pred,y_true):\n",
    "    with open(name,\"a\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        y_diff = [(y_pred[i] - y_true[i]) for i in range(len(y_pred))]\n",
    "        y = [y_pred, y_true, y_diff]\n",
    "        mse = 0.0\n",
    "        for num in range(len(y_diff)):\n",
    "            mse += pow(y_diff[num],2) / len(y_diff)\n",
    "        print('mse = ', mse)\n",
    "        writer.writerows(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = pretrained_models(model_name=\"resnet50\") #dnn_model VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical false\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 512, 512, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 518, 518, 3)  0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 256, 256, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 256, 256, 64) 256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 256, 256, 64) 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 258, 258, 64) 0           activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 128, 128, 64) 0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 128, 128, 64) 4160        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 128, 128, 64) 256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 128, 128, 64) 0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 128, 128, 64) 36928       activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 128, 128, 64) 256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 128, 128, 64) 0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 128, 128, 256 16640       activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 128, 128, 256 16640       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 128, 128, 256 1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 128, 128, 256 1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 128, 128, 256 0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 128, 128, 256 0           add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 128, 128, 64) 16448       activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 128, 128, 64) 256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 128, 128, 64) 0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 128, 128, 64) 36928       activation_105[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 128, 128, 64) 256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 128, 128, 64) 0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 128, 128, 256 16640       activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 128, 128, 256 1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 128, 128, 256 0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, 128, 128, 256 0           add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 128, 128, 64) 16448       activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 128, 128, 64) 256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 128, 128, 64) 0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 128, 128, 64) 36928       activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 128, 128, 64) 256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 128, 128, 64) 0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 128, 128, 256 16640       activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 128, 128, 256 1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 128, 128, 256 0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 128, 128, 256 0           add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 64, 64, 128)  32896       activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, 64, 64, 128)  0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, 64, 64, 128)  0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_112[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 64, 64, 512)  131584      activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 64, 64, 512)  2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 64, 64, 512)  0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, 64, 64, 512)  0           add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 64, 64, 128)  65664       activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, 64, 64, 128)  0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 64, 64, 128)  0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, 64, 64, 512)  0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 64, 64, 512)  0           add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 64, 64, 128)  65664       activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 64, 64, 128)  0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 64, 64, 128)  0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (None, 64, 64, 512)  0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 64, 64, 512)  0           add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 64, 64, 128)  65664       activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 64, 64, 128)  0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 64, 64, 128)  0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (None, 64, 64, 512)  0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 64, 64, 512)  0           add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 32, 32, 256)  131328      activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 32, 32, 256)  0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 32, 32, 256)  0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_124[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 32, 32, 1024) 525312      activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 32, 32, 1024) 4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (None, 32, 32, 1024) 0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 32, 32, 1024) 0           add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, 32, 32, 256)  0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, 32, 32, 256)  0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_41 (Add)                    (None, 32, 32, 1024) 0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, 32, 32, 1024) 0           add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, 32, 32, 256)  0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, 32, 32, 256)  0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_42 (Add)                    (None, 32, 32, 1024) 0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, 32, 32, 1024) 0           add_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 32, 32, 256)  0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, 32, 32, 256)  0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_133[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_43 (Add)                    (None, 32, 32, 1024) 0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 32, 32, 1024) 0           add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 32, 32, 256)  0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 32, 32, 256)  0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_44 (Add)                    (None, 32, 32, 1024) 0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 32, 32, 1024) 0           add_44[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, 32, 32, 256)  0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 32, 32, 256)  0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_45 (Add)                    (None, 32, 32, 1024) 0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, 32, 32, 1024) 0           add_45[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 16, 16, 512)  524800      activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 16, 16, 512)  2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, 16, 16, 512)  0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 16, 16, 512)  2359808     activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 16, 16, 512)  2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 16, 16, 512)  0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 16, 16, 2048) 1050624     activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 16, 16, 2048) 2099200     activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, 16, 16, 2048) 8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 16, 16, 2048) 8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_46 (Add)                    (None, 16, 16, 2048) 0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 16, 16, 2048) 0           add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 16, 16, 512)  1049088     activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 16, 16, 512)  2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 16, 16, 512)  0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 16, 16, 512)  2359808     activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 16, 16, 512)  2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, 16, 16, 512)  0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 16, 16, 2048) 1050624     activation_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, 16, 16, 2048) 8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_47 (Add)                    (None, 16, 16, 2048) 0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, 16, 16, 2048) 0           add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 16, 16, 512)  1049088     activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 16, 16, 512)  2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, 16, 16, 512)  0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 16, 16, 512)  2359808     activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 16, 16, 512)  2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, 16, 16, 512)  0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, 16, 16, 2048) 1050624     activation_148[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, 16, 16, 2048) 8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_48 (Add)                    (None, 16, 16, 2048) 0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, 16, 16, 2048) 0           add_48[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_4 (Sequential)       (None, 1)            524289      activation_149[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 24,112,001\n",
      "Trainable params: 24,058,881\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1053 samples, validate on 264 samples\n",
      "Epoch 1/400\n",
      "1053/1053 [==============================] - 115s 109ms/step - loss: 803.4075 - mean_squared_error: 803.4075 - val_loss: 92.9796 - val_mean_squared_error: 92.9796\n",
      "Epoch 2/400\n",
      "1053/1053 [==============================] - 88s 84ms/step - loss: 25.6277 - mean_squared_error: 25.6277 - val_loss: 41.8336 - val_mean_squared_error: 41.8336\n",
      "Epoch 3/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 4.2023 - mean_squared_error: 4.2023 - val_loss: 8.8229 - val_mean_squared_error: 8.8229\n",
      "Epoch 4/400\n",
      "1053/1053 [==============================] - 83s 78ms/step - loss: 0.9280 - mean_squared_error: 0.9280 - val_loss: 10.2148 - val_mean_squared_error: 10.2148\n",
      "Epoch 5/400\n",
      "1053/1053 [==============================] - 82s 78ms/step - loss: 1.5310 - mean_squared_error: 1.5310 - val_loss: 11.9949 - val_mean_squared_error: 11.9949\n",
      "Epoch 6/400\n",
      "1053/1053 [==============================] - 81s 77ms/step - loss: 9.8627 - mean_squared_error: 9.8627 - val_loss: 7.0885 - val_mean_squared_error: 7.0885\n",
      "Epoch 7/400\n",
      "1053/1053 [==============================] - 85s 80ms/step - loss: 0.5648 - mean_squared_error: 0.5648 - val_loss: 1.4428 - val_mean_squared_error: 1.4428\n",
      "Epoch 8/400\n",
      "1053/1053 [==============================] - 83s 79ms/step - loss: 0.4405 - mean_squared_error: 0.4405 - val_loss: 3.1514 - val_mean_squared_error: 3.1514\n",
      "Epoch 9/400\n",
      "1053/1053 [==============================] - 86s 81ms/step - loss: 0.8034 - mean_squared_error: 0.8034 - val_loss: 4.5396 - val_mean_squared_error: 4.5396\n",
      "Epoch 10/400\n",
      "1053/1053 [==============================] - 82s 78ms/step - loss: 0.6885 - mean_squared_error: 0.6885 - val_loss: 0.7417 - val_mean_squared_error: 0.7417\n",
      "Epoch 11/400\n",
      "1053/1053 [==============================] - 82s 78ms/step - loss: 0.4505 - mean_squared_error: 0.4505 - val_loss: 0.5055 - val_mean_squared_error: 0.5055\n",
      "Epoch 12/400\n",
      "1053/1053 [==============================] - 83s 79ms/step - loss: 1.0262 - mean_squared_error: 1.0262 - val_loss: 0.3675 - val_mean_squared_error: 0.3675\n",
      "Epoch 13/400\n",
      "1053/1053 [==============================] - 91s 87ms/step - loss: 0.2168 - mean_squared_error: 0.2168 - val_loss: 0.8514 - val_mean_squared_error: 0.8514\n",
      "Epoch 14/400\n",
      "1053/1053 [==============================] - 93s 89ms/step - loss: 1.0820 - mean_squared_error: 1.0820 - val_loss: 6.2130 - val_mean_squared_error: 6.2130\n",
      "Epoch 15/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.6808 - mean_squared_error: 0.6808 - val_loss: 1.7611 - val_mean_squared_error: 1.7611\n",
      "Epoch 16/400\n",
      "1053/1053 [==============================] - 87s 82ms/step - loss: 0.6705 - mean_squared_error: 0.6705 - val_loss: 4.1248 - val_mean_squared_error: 4.1248\n",
      "Epoch 17/400\n",
      "1053/1053 [==============================] - 92s 88ms/step - loss: 0.3708 - mean_squared_error: 0.3708 - val_loss: 0.9217 - val_mean_squared_error: 0.9217\n",
      "Epoch 18/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 0.1690 - mean_squared_error: 0.1690 - val_loss: 0.7531 - val_mean_squared_error: 0.7531\n",
      "Epoch 19/400\n",
      "1053/1053 [==============================] - 89s 84ms/step - loss: 9.0343 - mean_squared_error: 9.0343 - val_loss: 11.3334 - val_mean_squared_error: 11.3334\n",
      "Epoch 20/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 0.0936 - mean_squared_error: 0.0936 - val_loss: 0.0532 - val_mean_squared_error: 0.0532\n",
      "Epoch 21/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0770 - mean_squared_error: 0.0770 - val_loss: 0.0265 - val_mean_squared_error: 0.0265\n",
      "Epoch 22/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0491 - mean_squared_error: 0.0491 - val_loss: 0.0163 - val_mean_squared_error: 0.0163\n",
      "Epoch 23/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0847 - mean_squared_error: 0.0847 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 24/400\n",
      "1053/1053 [==============================] - 88s 84ms/step - loss: 0.0384 - mean_squared_error: 0.0384 - val_loss: 0.1494 - val_mean_squared_error: 0.1494\n",
      "Epoch 25/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0379 - val_mean_squared_error: 0.0379\n",
      "Epoch 26/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0755 - mean_squared_error: 0.0755 - val_loss: 0.0892 - val_mean_squared_error: 0.0892\n",
      "Epoch 27/400\n",
      "1053/1053 [==============================] - 88s 83ms/step - loss: 0.1213 - mean_squared_error: 0.1213 - val_loss: 0.4084 - val_mean_squared_error: 0.4084\n",
      "Epoch 28/400\n",
      "1053/1053 [==============================] - 89s 84ms/step - loss: 0.0627 - mean_squared_error: 0.0627 - val_loss: 0.6347 - val_mean_squared_error: 0.6347\n",
      "Epoch 29/400\n",
      "1053/1053 [==============================] - 89s 84ms/step - loss: 0.2722 - mean_squared_error: 0.2722 - val_loss: 0.2479 - val_mean_squared_error: 0.2479\n",
      "Epoch 30/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 0.0788 - mean_squared_error: 0.0788 - val_loss: 2.7481 - val_mean_squared_error: 2.7481\n",
      "Epoch 31/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 0.0908 - mean_squared_error: 0.0908 - val_loss: 0.6373 - val_mean_squared_error: 0.6373\n",
      "Epoch 32/400\n",
      "1053/1053 [==============================] - 89s 84ms/step - loss: 0.1230 - mean_squared_error: 0.1230 - val_loss: 0.3883 - val_mean_squared_error: 0.3883\n",
      "Epoch 33/400\n",
      "1053/1053 [==============================] - 88s 84ms/step - loss: 0.0754 - mean_squared_error: 0.0754 - val_loss: 0.5229 - val_mean_squared_error: 0.5229\n",
      "Epoch 34/400\n",
      "1053/1053 [==============================] - 88s 84ms/step - loss: 0.0934 - mean_squared_error: 0.0934 - val_loss: 1.0219 - val_mean_squared_error: 1.0219\n",
      "Epoch 35/400\n",
      "1053/1053 [==============================] - 88s 84ms/step - loss: 0.1436 - mean_squared_error: 0.1436 - val_loss: 4.7812 - val_mean_squared_error: 4.7812\n",
      "Epoch 36/400\n",
      "1053/1053 [==============================] - 88s 83ms/step - loss: 0.0854 - mean_squared_error: 0.0854 - val_loss: 0.5922 - val_mean_squared_error: 0.5922\n",
      "Epoch 37/400\n",
      "1053/1053 [==============================] - 89s 84ms/step - loss: 0.1256 - mean_squared_error: 0.1256 - val_loss: 0.2833 - val_mean_squared_error: 0.2833\n",
      "Epoch 38/400\n",
      "1053/1053 [==============================] - 88s 83ms/step - loss: 0.0672 - mean_squared_error: 0.0672 - val_loss: 0.0774 - val_mean_squared_error: 0.0774\n",
      "Epoch 39/400\n",
      "1053/1053 [==============================] - 84s 80ms/step - loss: 0.0613 - mean_squared_error: 0.0613 - val_loss: 0.1328 - val_mean_squared_error: 0.1328\n",
      "Epoch 40/400\n",
      "1053/1053 [==============================] - 86s 81ms/step - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 41/400\n",
      "1053/1053 [==============================] - 83s 79ms/step - loss: 0.0520 - mean_squared_error: 0.0520 - val_loss: 0.1893 - val_mean_squared_error: 0.1893\n",
      "Epoch 42/400\n",
      "1053/1053 [==============================] - 79s 75ms/step - loss: 18498.9408 - mean_squared_error: 18498.9408 - val_loss: 14258414.3277 - val_mean_squared_error: 14258414.3277\n",
      "Epoch 43/400\n",
      "1053/1053 [==============================] - 85s 81ms/step - loss: 94.4694 - mean_squared_error: 94.4694 - val_loss: 305.3613 - val_mean_squared_error: 305.3613\n",
      "Epoch 44/400\n",
      "1053/1053 [==============================] - 86s 82ms/step - loss: 73.7418 - mean_squared_error: 73.7418 - val_loss: 98.2086 - val_mean_squared_error: 98.2086\n",
      "Epoch 45/400\n",
      "1053/1053 [==============================] - 79s 75ms/step - loss: 52.5289 - mean_squared_error: 52.5289 - val_loss: 31.6911 - val_mean_squared_error: 31.6911\n",
      "Epoch 46/400\n",
      "1053/1053 [==============================] - 82s 78ms/step - loss: 12.1883 - mean_squared_error: 12.1883 - val_loss: 85.2798 - val_mean_squared_error: 85.2798\n",
      "Epoch 47/400\n",
      "1053/1053 [==============================] - 85s 81ms/step - loss: 16.1840 - mean_squared_error: 16.1840 - val_loss: 9.5625 - val_mean_squared_error: 9.5625\n",
      "Epoch 48/400\n",
      "1053/1053 [==============================] - 84s 80ms/step - loss: 9.3510 - mean_squared_error: 9.3510 - val_loss: 15.4649 - val_mean_squared_error: 15.4649\n",
      "Epoch 49/400\n",
      "1053/1053 [==============================] - 84s 80ms/step - loss: 10.0661 - mean_squared_error: 10.0661 - val_loss: 8.3307 - val_mean_squared_error: 8.3307\n",
      "Epoch 50/400\n",
      "1053/1053 [==============================] - 86s 81ms/step - loss: 9.8534 - mean_squared_error: 9.8534 - val_loss: 15.7812 - val_mean_squared_error: 15.7812\n",
      "Epoch 51/400\n",
      "1053/1053 [==============================] - 87s 83ms/step - loss: 11.6794 - mean_squared_error: 11.6794 - val_loss: 50.3188 - val_mean_squared_error: 50.3188\n",
      "Epoch 52/400\n",
      "1053/1053 [==============================] - 87s 82ms/step - loss: 13.5174 - mean_squared_error: 13.5174 - val_loss: 18.9040 - val_mean_squared_error: 18.9040\n",
      "Epoch 53/400\n",
      "1053/1053 [==============================] - 87s 82ms/step - loss: 14.2909 - mean_squared_error: 14.2909 - val_loss: 58.6347 - val_mean_squared_error: 58.6347\n",
      "Epoch 54/400\n",
      "1053/1053 [==============================] - 86s 82ms/step - loss: 10.7588 - mean_squared_error: 10.7588 - val_loss: 368.7337 - val_mean_squared_error: 368.7337\n",
      "Epoch 55/400\n",
      "1053/1053 [==============================] - 87s 82ms/step - loss: 23.5454 - mean_squared_error: 23.5454 - val_loss: 35.4642 - val_mean_squared_error: 35.4642\n",
      "Epoch 56/400\n",
      "1053/1053 [==============================] - 87s 82ms/step - loss: 14.2667 - mean_squared_error: 14.2667 - val_loss: 98.7255 - val_mean_squared_error: 98.7255\n",
      "Epoch 57/400\n",
      "1053/1053 [==============================] - 87s 82ms/step - loss: 25.3883 - mean_squared_error: 25.3883 - val_loss: 4.5137 - val_mean_squared_error: 4.5137\n",
      "Epoch 58/400\n",
      "1053/1053 [==============================] - 88s 83ms/step - loss: 6.5556 - mean_squared_error: 6.5556 - val_loss: 5.7897 - val_mean_squared_error: 5.7897\n",
      "Epoch 59/400\n",
      "1053/1053 [==============================] - 86s 82ms/step - loss: 4.7184 - mean_squared_error: 4.7184 - val_loss: 3.2247 - val_mean_squared_error: 3.2247\n",
      "Epoch 60/400\n",
      "1053/1053 [==============================] - 88s 84ms/step - loss: 2.4689 - mean_squared_error: 2.4689 - val_loss: 12.5654 - val_mean_squared_error: 12.5654\n",
      "Epoch 61/400\n",
      "1053/1053 [==============================] - 87s 83ms/step - loss: 4.2347 - mean_squared_error: 4.2347 - val_loss: 7.0622 - val_mean_squared_error: 7.0622\n",
      "Epoch 62/400\n",
      "1053/1053 [==============================] - 88s 84ms/step - loss: 2.1750 - mean_squared_error: 2.1750 - val_loss: 3.8306 - val_mean_squared_error: 3.8306\n",
      "Epoch 63/400\n",
      "1053/1053 [==============================] - 89s 84ms/step - loss: 2.0573 - mean_squared_error: 2.0573 - val_loss: 1.5824 - val_mean_squared_error: 1.5824\n",
      "Epoch 64/400\n",
      "1053/1053 [==============================] - 89s 84ms/step - loss: 0.9230 - mean_squared_error: 0.9230 - val_loss: 1.8979 - val_mean_squared_error: 1.8979\n",
      "Epoch 65/400\n",
      "1053/1053 [==============================] - 88s 83ms/step - loss: 1.3983 - mean_squared_error: 1.3983 - val_loss: 4.9862 - val_mean_squared_error: 4.9862\n",
      "Epoch 66/400\n",
      "1053/1053 [==============================] - 86s 81ms/step - loss: 1.8149 - mean_squared_error: 1.8149 - val_loss: 19.0264 - val_mean_squared_error: 19.0264\n",
      "Epoch 67/400\n",
      "1053/1053 [==============================] - 85s 80ms/step - loss: 1.3891 - mean_squared_error: 1.3891 - val_loss: 1.0457 - val_mean_squared_error: 1.0457\n",
      "Epoch 68/400\n",
      "1053/1053 [==============================] - 84s 80ms/step - loss: 2.0370 - mean_squared_error: 2.0370 - val_loss: 0.2398 - val_mean_squared_error: 0.2398\n",
      "Epoch 69/400\n",
      "1053/1053 [==============================] - 84s 80ms/step - loss: 1.2983 - mean_squared_error: 1.2983 - val_loss: 5.2336 - val_mean_squared_error: 5.2336\n",
      "Epoch 70/400\n",
      "1053/1053 [==============================] - 86s 81ms/step - loss: 0.7987 - mean_squared_error: 0.7987 - val_loss: 4.6812 - val_mean_squared_error: 4.6812\n",
      "Epoch 71/400\n",
      "1053/1053 [==============================] - 85s 80ms/step - loss: 1.2588 - mean_squared_error: 1.2588 - val_loss: 10.6230 - val_mean_squared_error: 10.6230\n",
      "Epoch 72/400\n",
      "1053/1053 [==============================] - 86s 81ms/step - loss: 1.1032 - mean_squared_error: 1.1032 - val_loss: 0.3966 - val_mean_squared_error: 0.3966\n",
      "Epoch 73/400\n",
      "1053/1053 [==============================] - 84s 80ms/step - loss: 0.2111 - mean_squared_error: 0.2111 - val_loss: 0.1407 - val_mean_squared_error: 0.1407\n",
      "Epoch 74/400\n",
      "1053/1053 [==============================] - 85s 81ms/step - loss: 0.3248 - mean_squared_error: 0.3248 - val_loss: 0.3122 - val_mean_squared_error: 0.3122\n",
      "Epoch 75/400\n",
      "1053/1053 [==============================] - 85s 81ms/step - loss: 0.1356 - mean_squared_error: 0.1356 - val_loss: 0.4273 - val_mean_squared_error: 0.4273\n",
      "Epoch 76/400\n",
      "1053/1053 [==============================] - 83s 79ms/step - loss: 0.1218 - mean_squared_error: 0.1218 - val_loss: 0.5162 - val_mean_squared_error: 0.5162\n",
      "Epoch 77/400\n",
      "1053/1053 [==============================] - 89s 84ms/step - loss: 0.2769 - mean_squared_error: 0.2769 - val_loss: 0.1778 - val_mean_squared_error: 0.1778\n",
      "Epoch 78/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.1801 - mean_squared_error: 0.1801 - val_loss: 0.0827 - val_mean_squared_error: 0.0827\n",
      "Epoch 79/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0888 - mean_squared_error: 0.0888 - val_loss: 0.1678 - val_mean_squared_error: 0.1678\n",
      "Epoch 80/400\n",
      "1053/1053 [==============================] - 92s 88ms/step - loss: 0.1512 - mean_squared_error: 0.1512 - val_loss: 0.2134 - val_mean_squared_error: 0.2134\n",
      "Epoch 81/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0830 - mean_squared_error: 0.0830 - val_loss: 0.2896 - val_mean_squared_error: 0.2896\n",
      "Epoch 82/400\n",
      "1053/1053 [==============================] - 94s 89ms/step - loss: 0.1447 - mean_squared_error: 0.1447 - val_loss: 0.1033 - val_mean_squared_error: 0.1033\n",
      "Epoch 83/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0624 - mean_squared_error: 0.0624 - val_loss: 0.0830 - val_mean_squared_error: 0.0830\n",
      "Epoch 84/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.5668 - val_mean_squared_error: 0.5668\n",
      "Epoch 85/400\n",
      "1053/1053 [==============================] - 89s 84ms/step - loss: 0.0480 - mean_squared_error: 0.0480 - val_loss: 0.3084 - val_mean_squared_error: 0.3084\n",
      "Epoch 86/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0459 - mean_squared_error: 0.0459 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 87/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0182 - val_mean_squared_error: 0.0182\n",
      "Epoch 88/400\n",
      "1053/1053 [==============================] - 92s 88ms/step - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.3500 - val_mean_squared_error: 0.3500\n",
      "Epoch 89/400\n",
      "1053/1053 [==============================] - 88s 83ms/step - loss: 0.0371 - mean_squared_error: 0.0371 - val_loss: 0.0264 - val_mean_squared_error: 0.0264\n",
      "Epoch 90/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0274 - val_mean_squared_error: 0.0274\n",
      "Epoch 91/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0166 - mean_squared_error: 0.0166 - val_loss: 0.0204 - val_mean_squared_error: 0.0204\n",
      "Epoch 92/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0169 - val_mean_squared_error: 0.0169\n",
      "Epoch 93/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0164 - mean_squared_error: 0.0164 - val_loss: 0.0341 - val_mean_squared_error: 0.0341\n",
      "Epoch 94/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0121 - mean_squared_error: 0.0121 - val_loss: 0.0114 - val_mean_squared_error: 0.0114\n",
      "Epoch 95/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0115 - mean_squared_error: 0.0115 - val_loss: 0.0142 - val_mean_squared_error: 0.0142\n",
      "Epoch 96/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0106 - mean_squared_error: 0.0106 - val_loss: 0.0194 - val_mean_squared_error: 0.0194\n",
      "Epoch 97/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.1532 - mean_squared_error: 0.1532 - val_loss: 724.7141 - val_mean_squared_error: 724.7141\n",
      "Epoch 98/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 31.2330 - mean_squared_error: 31.2330 - val_loss: 9.1080 - val_mean_squared_error: 9.1080\n",
      "Epoch 99/400\n",
      "1053/1053 [==============================] - 91s 87ms/step - loss: 0.0872 - mean_squared_error: 0.0872 - val_loss: 0.0747 - val_mean_squared_error: 0.0747\n",
      "Epoch 100/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0469 - mean_squared_error: 0.0469 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 101/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0384 - mean_squared_error: 0.0384 - val_loss: 0.0280 - val_mean_squared_error: 0.0280\n",
      "Epoch 102/400\n",
      "1053/1053 [==============================] - 94s 89ms/step - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0212 - val_mean_squared_error: 0.0212\n",
      "Epoch 103/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0195 - val_mean_squared_error: 0.0195\n",
      "Epoch 104/400\n",
      "1053/1053 [==============================] - 93s 89ms/step - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0357 - val_mean_squared_error: 0.0357\n",
      "Epoch 105/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0180 - val_mean_squared_error: 0.0180\n",
      "Epoch 106/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0167 - mean_squared_error: 0.0167 - val_loss: 0.0254 - val_mean_squared_error: 0.0254\n",
      "Epoch 107/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0139 - mean_squared_error: 0.0139 - val_loss: 0.0157 - val_mean_squared_error: 0.0157\n",
      "Epoch 108/400\n",
      "1053/1053 [==============================] - 92s 88ms/step - loss: 0.0153 - mean_squared_error: 0.0153 - val_loss: 0.0127 - val_mean_squared_error: 0.0127\n",
      "Epoch 109/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0154 - mean_squared_error: 0.0154 - val_loss: 0.0097 - val_mean_squared_error: 0.0097\n",
      "Epoch 110/400\n",
      "1053/1053 [==============================] - 92s 88ms/step - loss: 0.0158 - mean_squared_error: 0.0158 - val_loss: 0.0173 - val_mean_squared_error: 0.0173\n",
      "Epoch 111/400\n",
      "1053/1053 [==============================] - 92s 88ms/step - loss: 0.0136 - mean_squared_error: 0.0136 - val_loss: 0.0108 - val_mean_squared_error: 0.0108\n",
      "Epoch 112/400\n",
      "1053/1053 [==============================] - 92s 88ms/step - loss: 0.0137 - mean_squared_error: 0.0137 - val_loss: 0.0180 - val_mean_squared_error: 0.0180\n",
      "Epoch 113/400\n",
      "1053/1053 [==============================] - 91s 87ms/step - loss: 0.0138 - mean_squared_error: 0.0138 - val_loss: 0.0108 - val_mean_squared_error: 0.0108\n",
      "Epoch 114/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0094 - mean_squared_error: 0.0094 - val_loss: 0.0109 - val_mean_squared_error: 0.0109\n",
      "Epoch 115/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0111 - mean_squared_error: 0.0111 - val_loss: 0.0106 - val_mean_squared_error: 0.0106\n",
      "Epoch 116/400\n",
      "1053/1053 [==============================] - 92s 88ms/step - loss: 0.0098 - mean_squared_error: 0.0098 - val_loss: 5.8992 - val_mean_squared_error: 5.8992\n",
      "Epoch 117/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 0.0110 - mean_squared_error: 0.0110 - val_loss: 0.0107 - val_mean_squared_error: 0.0107\n",
      "Epoch 118/400\n",
      "1053/1053 [==============================] - 94s 89ms/step - loss: 0.0102 - mean_squared_error: 0.0102 - val_loss: 0.0108 - val_mean_squared_error: 0.0108\n",
      "Epoch 119/400\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 0.0088 - mean_squared_error: 0.0088 - val_loss: 0.0109 - val_mean_squared_error: 0.0109\n",
      "Epoch 120/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 0.0093 - mean_squared_error: 0.0093 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 121/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0086 - mean_squared_error: 0.0086 - val_loss: 0.0076 - val_mean_squared_error: 0.0076\n",
      "Epoch 122/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0080 - mean_squared_error: 0.0080 - val_loss: 0.0075 - val_mean_squared_error: 0.0075\n",
      "Epoch 123/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0078 - mean_squared_error: 0.0078 - val_loss: 52.7122 - val_mean_squared_error: 52.7122\n",
      "Epoch 124/400\n",
      "1053/1053 [==============================] - 88s 84ms/step - loss: 0.0083 - mean_squared_error: 0.0083 - val_loss: 0.0114 - val_mean_squared_error: 0.0114\n",
      "Epoch 125/400\n",
      "1053/1053 [==============================] - 89s 84ms/step - loss: 0.0073 - mean_squared_error: 0.0073 - val_loss: 0.0062 - val_mean_squared_error: 0.0062\n",
      "Epoch 126/400\n",
      "1053/1053 [==============================] - 94s 89ms/step - loss: 0.0084 - mean_squared_error: 0.0084 - val_loss: 0.0104 - val_mean_squared_error: 0.0104\n",
      "Epoch 127/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 0.0083 - mean_squared_error: 0.0083 - val_loss: 0.0061 - val_mean_squared_error: 0.0061\n",
      "Epoch 128/400\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 0.0068 - mean_squared_error: 0.0068 - val_loss: 0.0085 - val_mean_squared_error: 0.0085\n",
      "Epoch 129/400\n",
      "1053/1053 [==============================] - 94s 89ms/step - loss: 0.0085 - mean_squared_error: 0.0085 - val_loss: 0.0068 - val_mean_squared_error: 0.0068\n",
      "Epoch 130/400\n",
      "1053/1053 [==============================] - 134s 127ms/step - loss: 0.0069 - mean_squared_error: 0.0069 - val_loss: 0.0097 - val_mean_squared_error: 0.0097\n",
      "Epoch 131/400\n",
      "1053/1053 [==============================] - 172s 163ms/step - loss: 0.0078 - mean_squared_error: 0.0078 - val_loss: 0.0068 - val_mean_squared_error: 0.0068\n",
      "Epoch 132/400\n",
      "1053/1053 [==============================] - 172s 163ms/step - loss: 0.0076 - mean_squared_error: 0.0076 - val_loss: 0.0078 - val_mean_squared_error: 0.0078\n",
      "Epoch 133/400\n",
      "1053/1053 [==============================] - 102s 97ms/step - loss: 0.0072 - mean_squared_error: 0.0072 - val_loss: 0.0050 - val_mean_squared_error: 0.0050\n",
      "Epoch 134/400\n",
      "1053/1053 [==============================] - 107s 102ms/step - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.0066 - val_mean_squared_error: 0.0066\n",
      "Epoch 135/400\n",
      "1053/1053 [==============================] - 139s 132ms/step - loss: 0.0075 - mean_squared_error: 0.0075 - val_loss: 0.0063 - val_mean_squared_error: 0.0063\n",
      "Epoch 136/400\n",
      "1053/1053 [==============================] - 140s 133ms/step - loss: 0.0070 - mean_squared_error: 0.0070 - val_loss: 0.0066 - val_mean_squared_error: 0.0066\n",
      "Epoch 137/400\n",
      "1053/1053 [==============================] - 131s 124ms/step - loss: 0.1130 - mean_squared_error: 0.1130 - val_loss: 102993.8488 - val_mean_squared_error: 102993.8488\n",
      "Epoch 138/400\n",
      "1053/1053 [==============================] - 140s 133ms/step - loss: 0.3311 - mean_squared_error: 0.3311 - val_loss: 5.7677 - val_mean_squared_error: 5.7677\n",
      "Epoch 139/400\n",
      "1053/1053 [==============================] - 157s 149ms/step - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0106 - val_mean_squared_error: 0.0106\n",
      "Epoch 140/400\n",
      "1053/1053 [==============================] - 179s 170ms/step - loss: 0.0131 - mean_squared_error: 0.0131 - val_loss: 0.0115 - val_mean_squared_error: 0.0115\n",
      "Epoch 141/400\n",
      "1053/1053 [==============================] - 183s 174ms/step - loss: 0.0117 - mean_squared_error: 0.0117 - val_loss: 0.0101 - val_mean_squared_error: 0.0101\n",
      "Epoch 142/400\n",
      "1053/1053 [==============================] - 186s 177ms/step - loss: 0.0093 - mean_squared_error: 0.0093 - val_loss: 0.0094 - val_mean_squared_error: 0.0094\n",
      "Epoch 143/400\n",
      "1053/1053 [==============================] - 161s 153ms/step - loss: 0.0078 - mean_squared_error: 0.0078 - val_loss: 0.0080 - val_mean_squared_error: 0.0080\n",
      "Epoch 144/400\n",
      "1053/1053 [==============================] - 136s 130ms/step - loss: 0.0082 - mean_squared_error: 0.0082 - val_loss: 0.0085 - val_mean_squared_error: 0.0085\n",
      "Epoch 145/400\n",
      "1053/1053 [==============================] - 251s 239ms/step - loss: 0.0080 - mean_squared_error: 0.0080 - val_loss: 0.0084 - val_mean_squared_error: 0.0084\n",
      "Epoch 146/400\n",
      "1053/1053 [==============================] - 153s 145ms/step - loss: 0.0072 - mean_squared_error: 0.0072 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 147/400\n",
      "1053/1053 [==============================] - 114s 108ms/step - loss: 0.0068 - mean_squared_error: 0.0068 - val_loss: 0.0073 - val_mean_squared_error: 0.0073\n",
      "Epoch 148/400\n",
      "1053/1053 [==============================] - 116s 111ms/step - loss: 0.0069 - mean_squared_error: 0.0069 - val_loss: 0.0093 - val_mean_squared_error: 0.0093\n",
      "Epoch 149/400\n",
      "1053/1053 [==============================] - 112s 106ms/step - loss: 0.0075 - mean_squared_error: 0.0075 - val_loss: 0.0096 - val_mean_squared_error: 0.0096\n",
      "Epoch 150/400\n",
      "1053/1053 [==============================] - 131s 124ms/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.0089 - val_mean_squared_error: 0.0089\n",
      "Epoch 151/400\n",
      "1053/1053 [==============================] - 144s 137ms/step - loss: 0.0062 - mean_squared_error: 0.0062 - val_loss: 0.0067 - val_mean_squared_error: 0.0067\n",
      "Epoch 152/400\n",
      "1053/1053 [==============================] - 122s 116ms/step - loss: 0.0068 - mean_squared_error: 0.0068 - val_loss: 0.0105 - val_mean_squared_error: 0.0105\n",
      "Epoch 153/400\n",
      "1053/1053 [==============================] - 124s 118ms/step - loss: 0.0062 - mean_squared_error: 0.0062 - val_loss: 0.0059 - val_mean_squared_error: 0.0059\n",
      "Epoch 154/400\n",
      "1053/1053 [==============================] - 131s 124ms/step - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.0091 - val_mean_squared_error: 0.0091\n",
      "Epoch 155/400\n",
      "1053/1053 [==============================] - 126s 120ms/step - loss: 0.0059 - mean_squared_error: 0.0059 - val_loss: 0.0070 - val_mean_squared_error: 0.0070\n",
      "Epoch 156/400\n",
      "1053/1053 [==============================] - 149s 141ms/step - loss: 0.0054 - mean_squared_error: 0.0054 - val_loss: 0.0056 - val_mean_squared_error: 0.0056\n",
      "Epoch 157/400\n",
      "1053/1053 [==============================] - 107s 101ms/step - loss: 0.0060 - mean_squared_error: 0.0060 - val_loss: 0.0065 - val_mean_squared_error: 0.0065\n",
      "Epoch 158/400\n",
      "1053/1053 [==============================] - 140s 133ms/step - loss: 0.0055 - mean_squared_error: 0.0055 - val_loss: 0.0042 - val_mean_squared_error: 0.0042\n",
      "Epoch 159/400\n",
      "1053/1053 [==============================] - 133s 127ms/step - loss: 0.0053 - mean_squared_error: 0.0053 - val_loss: 0.0054 - val_mean_squared_error: 0.0054\n",
      "Epoch 160/400\n",
      "1053/1053 [==============================] - 135s 128ms/step - loss: 0.0055 - mean_squared_error: 0.0055 - val_loss: 0.0053 - val_mean_squared_error: 0.0053\n",
      "Epoch 161/400\n",
      "1053/1053 [==============================] - 118s 112ms/step - loss: 0.0072 - mean_squared_error: 0.0072 - val_loss: 0.0053 - val_mean_squared_error: 0.0053\n",
      "Epoch 162/400\n",
      "1053/1053 [==============================] - 158s 150ms/step - loss: 0.0057 - mean_squared_error: 0.0057 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 163/400\n",
      "1053/1053 [==============================] - 122s 115ms/step - loss: 0.0053 - mean_squared_error: 0.0053 - val_loss: 0.0056 - val_mean_squared_error: 0.0056\n",
      "Epoch 164/400\n",
      "1053/1053 [==============================] - 141s 134ms/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.2883 - val_mean_squared_error: 0.2883\n",
      "Epoch 165/400\n",
      "1053/1053 [==============================] - 111s 105ms/step - loss: 0.0053 - mean_squared_error: 0.0053 - val_loss: 0.0082 - val_mean_squared_error: 0.0082\n",
      "Epoch 166/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0065 - mean_squared_error: 0.0065 - val_loss: 0.0051 - val_mean_squared_error: 0.0051\n",
      "Epoch 167/400\n",
      "1053/1053 [==============================] - 107s 102ms/step - loss: 0.0080 - mean_squared_error: 0.0080 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "Epoch 168/400\n",
      "1053/1053 [==============================] - 134s 127ms/step - loss: 0.0675 - mean_squared_error: 0.0675 - val_loss: 0.8396 - val_mean_squared_error: 0.8396\n",
      "Epoch 169/400\n",
      "1053/1053 [==============================] - 123s 117ms/step - loss: 0.0085 - mean_squared_error: 0.0085 - val_loss: 0.0070 - val_mean_squared_error: 0.0070\n",
      "Epoch 170/400\n",
      "1053/1053 [==============================] - 115s 109ms/step - loss: 0.0080 - mean_squared_error: 0.0080 - val_loss: 0.0063 - val_mean_squared_error: 0.0063\n",
      "Epoch 171/400\n",
      "1053/1053 [==============================] - 123s 117ms/step - loss: 0.0073 - mean_squared_error: 0.0073 - val_loss: 0.0063 - val_mean_squared_error: 0.0063\n",
      "Epoch 172/400\n",
      "1053/1053 [==============================] - 201s 190ms/step - loss: 0.0059 - mean_squared_error: 0.0059 - val_loss: 0.0056 - val_mean_squared_error: 0.0056\n",
      "Epoch 173/400\n",
      "1053/1053 [==============================] - 189s 180ms/step - loss: 0.0059 - mean_squared_error: 0.0059 - val_loss: 0.0061 - val_mean_squared_error: 0.0061\n",
      "Epoch 174/400\n",
      "1053/1053 [==============================] - 164s 156ms/step - loss: 0.0070 - mean_squared_error: 0.0070 - val_loss: 0.0276 - val_mean_squared_error: 0.0276\n",
      "Epoch 175/400\n",
      "1053/1053 [==============================] - 138s 131ms/step - loss: 0.0054 - mean_squared_error: 0.0054 - val_loss: 0.0062 - val_mean_squared_error: 0.0062\n",
      "Epoch 176/400\n",
      "1053/1053 [==============================] - 153s 146ms/step - loss: 0.0056 - mean_squared_error: 0.0056 - val_loss: 0.0061 - val_mean_squared_error: 0.0061\n",
      "Epoch 177/400\n",
      "1053/1053 [==============================] - 169s 161ms/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.0049 - val_mean_squared_error: 0.0049\n",
      "Epoch 178/400\n",
      "1053/1053 [==============================] - 147s 140ms/step - loss: 0.0071 - mean_squared_error: 0.0071 - val_loss: 58.4823 - val_mean_squared_error: 58.4823\n",
      "Epoch 179/400\n",
      "1053/1053 [==============================] - 127s 120ms/step - loss: 0.0080 - mean_squared_error: 0.0080 - val_loss: 0.0061 - val_mean_squared_error: 0.0061\n",
      "Epoch 180/400\n",
      "1053/1053 [==============================] - 165s 157ms/step - loss: 0.0058 - mean_squared_error: 0.0058 - val_loss: 0.4267 - val_mean_squared_error: 0.4267\n",
      "Epoch 181/400\n",
      "1053/1053 [==============================] - 138s 131ms/step - loss: 0.0109 - mean_squared_error: 0.0109 - val_loss: 0.0052 - val_mean_squared_error: 0.0052\n",
      "Epoch 182/400\n",
      "1053/1053 [==============================] - 157s 149ms/step - loss: 0.0051 - mean_squared_error: 0.0051 - val_loss: 0.0053 - val_mean_squared_error: 0.0053\n",
      "Epoch 183/400\n",
      "1053/1053 [==============================] - 123s 116ms/step - loss: 0.0050 - mean_squared_error: 0.0050 - val_loss: 0.0052 - val_mean_squared_error: 0.0052\n",
      "Epoch 184/400\n",
      "1053/1053 [==============================] - 158s 150ms/step - loss: 0.0055 - mean_squared_error: 0.0055 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 185/400\n",
      "1053/1053 [==============================] - 143s 135ms/step - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0051 - val_mean_squared_error: 0.0051\n",
      "Epoch 186/400\n",
      "1053/1053 [==============================] - 138s 131ms/step - loss: 0.0051 - mean_squared_error: 0.0051 - val_loss: 0.0058 - val_mean_squared_error: 0.0058\n",
      "Epoch 187/400\n",
      "1053/1053 [==============================] - 128s 122ms/step - loss: 0.0049 - mean_squared_error: 0.0049 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 188/400\n",
      "1053/1053 [==============================] - 150s 142ms/step - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0052 - val_mean_squared_error: 0.0052\n",
      "Epoch 189/400\n",
      "1053/1053 [==============================] - 125s 119ms/step - loss: 0.0053 - mean_squared_error: 0.0053 - val_loss: 0.0060 - val_mean_squared_error: 0.0060\n",
      "Epoch 190/400\n",
      "1053/1053 [==============================] - 164s 155ms/step - loss: 0.0120 - mean_squared_error: 0.0120 - val_loss: 0.2269 - val_mean_squared_error: 0.2269\n",
      "Epoch 191/400\n",
      "1053/1053 [==============================] - 152s 144ms/step - loss: 0.0058 - mean_squared_error: 0.0058 - val_loss: 0.0069 - val_mean_squared_error: 0.0069\n",
      "Epoch 192/400\n",
      "1053/1053 [==============================] - 134s 128ms/step - loss: 0.0047 - mean_squared_error: 0.0047 - val_loss: 0.0052 - val_mean_squared_error: 0.0052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193/400\n",
      "1053/1053 [==============================] - 117s 111ms/step - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0053 - val_mean_squared_error: 0.0053\n",
      "Epoch 194/400\n",
      "1053/1053 [==============================] - 130s 123ms/step - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
      "Epoch 195/400\n",
      "1053/1053 [==============================] - 126s 119ms/step - loss: 0.0047 - mean_squared_error: 0.0047 - val_loss: 0.0061 - val_mean_squared_error: 0.0061\n",
      "Epoch 196/400\n",
      "1053/1053 [==============================] - 120s 114ms/step - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.0042 - val_mean_squared_error: 0.0042\n",
      "Epoch 197/400\n",
      "1053/1053 [==============================] - 142s 134ms/step - loss: 0.0050 - mean_squared_error: 0.0050 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 198/400\n",
      "1053/1053 [==============================] - 151s 143ms/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 199/400\n",
      "1053/1053 [==============================] - 140s 133ms/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 200/400\n",
      "1053/1053 [==============================] - 142s 135ms/step - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0041 - val_mean_squared_error: 0.0041\n",
      "Epoch 201/400\n",
      "1053/1053 [==============================] - 117s 111ms/step - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.7127 - val_mean_squared_error: 0.7127\n",
      "Epoch 202/400\n",
      "1053/1053 [==============================] - 124s 117ms/step - loss: 0.0663 - mean_squared_error: 0.0663 - val_loss: 11.6993 - val_mean_squared_error: 11.6993\n",
      "Epoch 203/400\n",
      "1053/1053 [==============================] - 89s 84ms/step - loss: 0.0126 - mean_squared_error: 0.0126 - val_loss: 0.0090 - val_mean_squared_error: 0.0090\n",
      "Epoch 204/400\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0088 - mean_squared_error: 0.0088 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 205/400\n",
      "1053/1053 [==============================] - 110s 105ms/step - loss: 0.0097 - mean_squared_error: 0.0097 - val_loss: 0.0079 - val_mean_squared_error: 0.0079\n",
      "Epoch 206/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0077 - mean_squared_error: 0.0077 - val_loss: 0.0098 - val_mean_squared_error: 0.0098\n",
      "Epoch 207/400\n",
      "1053/1053 [==============================] - 86s 81ms/step - loss: 0.0076 - mean_squared_error: 0.0076 - val_loss: 0.0079 - val_mean_squared_error: 0.0079\n",
      "Epoch 208/400\n",
      "1053/1053 [==============================] - 86s 82ms/step - loss: 0.0069 - mean_squared_error: 0.0069 - val_loss: 0.0123 - val_mean_squared_error: 0.0123\n",
      "Epoch 209/400\n",
      "1053/1053 [==============================] - 108s 102ms/step - loss: 0.0065 - mean_squared_error: 0.0065 - val_loss: 0.0134 - val_mean_squared_error: 0.0134\n",
      "Epoch 210/400\n",
      "1053/1053 [==============================] - 94s 89ms/step - loss: 0.0068 - mean_squared_error: 0.0068 - val_loss: 0.0569 - val_mean_squared_error: 0.0569\n",
      "Epoch 211/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0081 - mean_squared_error: 0.0081 - val_loss: 0.0126 - val_mean_squared_error: 0.0126\n",
      "Epoch 212/400\n",
      "1053/1053 [==============================] - 107s 102ms/step - loss: 0.0066 - mean_squared_error: 0.0066 - val_loss: 0.0065 - val_mean_squared_error: 0.0065\n",
      "Epoch 213/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0058 - mean_squared_error: 0.0058 - val_loss: 0.0056 - val_mean_squared_error: 0.0056\n",
      "Epoch 214/400\n",
      "1053/1053 [==============================] - 95s 90ms/step - loss: 0.0068 - mean_squared_error: 0.0068 - val_loss: 0.0052 - val_mean_squared_error: 0.0052\n",
      "Epoch 215/400\n",
      "1053/1053 [==============================] - 87s 83ms/step - loss: 0.0055 - mean_squared_error: 0.0055 - val_loss: 0.0099 - val_mean_squared_error: 0.0099\n",
      "Epoch 216/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 0.0057 - mean_squared_error: 0.0057 - val_loss: 0.0061 - val_mean_squared_error: 0.0061\n",
      "Epoch 217/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 0.0055 - mean_squared_error: 0.0055 - val_loss: 0.0067 - val_mean_squared_error: 0.0067\n",
      "Epoch 218/400\n",
      "1053/1053 [==============================] - 87s 83ms/step - loss: 0.0058 - mean_squared_error: 0.0058 - val_loss: 0.0056 - val_mean_squared_error: 0.0056\n",
      "Epoch 219/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 0.0108 - mean_squared_error: 0.0108 - val_loss: 143.1528 - val_mean_squared_error: 143.1528\n",
      "Epoch 220/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 0.0154 - mean_squared_error: 0.0154 - val_loss: 0.0554 - val_mean_squared_error: 0.0554\n",
      "Epoch 221/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0074 - mean_squared_error: 0.0074 - val_loss: 0.0360 - val_mean_squared_error: 0.0360\n",
      "Epoch 222/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0066 - mean_squared_error: 0.0066 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "Epoch 223/400\n",
      "1053/1053 [==============================] - 91s 87ms/step - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.0070 - val_mean_squared_error: 0.0070\n",
      "Epoch 224/400\n",
      "1053/1053 [==============================] - 94s 89ms/step - loss: 0.0070 - mean_squared_error: 0.0070 - val_loss: 0.0057 - val_mean_squared_error: 0.0057\n",
      "Epoch 225/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0057 - mean_squared_error: 0.0057 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 226/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0053 - mean_squared_error: 0.0053 - val_loss: 0.0106 - val_mean_squared_error: 0.0106\n",
      "Epoch 227/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0049 - mean_squared_error: 0.0049 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 228/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0060 - mean_squared_error: 0.0060 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 229/400\n",
      "1053/1053 [==============================] - 88s 84ms/step - loss: 0.0057 - mean_squared_error: 0.0057 - val_loss: 0.0126 - val_mean_squared_error: 0.0126\n",
      "Epoch 230/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 231/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 232/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 233/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 234/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0473 - val_mean_squared_error: 0.0473\n",
      "Epoch 235/400\n",
      "1053/1053 [==============================] - 87s 82ms/step - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 236/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 237/400\n",
      "1053/1053 [==============================] - 88s 84ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 238/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 239/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 240/400\n",
      "1053/1053 [==============================] - 93s 89ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 241/400\n",
      "1053/1053 [==============================] - 95s 91ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 242/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 243/400\n",
      "1053/1053 [==============================] - 95s 91ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 244/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 245/400\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 246/400\n",
      "1053/1053 [==============================] - 97s 93ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 247/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 248/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 249/400\n",
      "1053/1053 [==============================] - 99s 94ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 250/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 251/400\n",
      "1053/1053 [==============================] - 85s 81ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 252/400\n",
      "1053/1053 [==============================] - 123s 117ms/step - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 253/400\n",
      "1053/1053 [==============================] - 125s 119ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 254/400\n",
      "1053/1053 [==============================] - 123s 117ms/step - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 255/400\n",
      "1053/1053 [==============================] - 178s 169ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 256/400\n",
      "1053/1053 [==============================] - 125s 118ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 257/400\n",
      "1053/1053 [==============================] - 172s 164ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0475 - val_mean_squared_error: 0.0475\n",
      "Epoch 258/400\n",
      "1053/1053 [==============================] - 171s 163ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 259/400\n",
      "1053/1053 [==============================] - 173s 164ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 260/400\n",
      "1053/1053 [==============================] - 169s 160ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 261/400\n",
      "1053/1053 [==============================] - 157s 149ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 262/400\n",
      "1053/1053 [==============================] - 190s 180ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 263/400\n",
      "1053/1053 [==============================] - 134s 127ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 264/400\n",
      "1053/1053 [==============================] - 135s 128ms/step - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 265/400\n",
      "1053/1053 [==============================] - 191s 182ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 266/400\n",
      "1053/1053 [==============================] - 120s 114ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 267/400\n",
      "1053/1053 [==============================] - 122s 115ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 268/400\n",
      "1053/1053 [==============================] - 121s 115ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 269/400\n",
      "1053/1053 [==============================] - 107s 102ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 270/400\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 271/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "Epoch 272/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "Epoch 273/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 274/400\n",
      "1053/1053 [==============================] - 106s 100ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 275/400\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 276/400\n",
      "1053/1053 [==============================] - 93s 89ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 277/400\n",
      "1053/1053 [==============================] - 95s 90ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 278/400\n",
      "1053/1053 [==============================] - 122s 116ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 279/400\n",
      "1053/1053 [==============================] - 120s 114ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 280/400\n",
      "1053/1053 [==============================] - 136s 129ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 281/400\n",
      "1053/1053 [==============================] - 125s 119ms/step - loss: 0.0443 - mean_squared_error: 0.0443 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 282/400\n",
      "1053/1053 [==============================] - 122s 115ms/step - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 283/400\n",
      "1053/1053 [==============================] - 104s 98ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 284/400\n",
      "1053/1053 [==============================] - 127s 121ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 285/400\n",
      "1053/1053 [==============================] - 105s 100ms/step - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 286/400\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "Epoch 287/400\n",
      "1053/1053 [==============================] - 136s 129ms/step - loss: 0.0445 - mean_squared_error: 0.0445 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 288/400\n",
      "1053/1053 [==============================] - 126s 120ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 289/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1053 [==============================] - 128s 121ms/step - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 290/400\n",
      "1053/1053 [==============================] - 123s 117ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 291/400\n",
      "1053/1053 [==============================] - 115s 109ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 292/400\n",
      "1053/1053 [==============================] - 130s 124ms/step - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 293/400\n",
      "1053/1053 [==============================] - 118s 112ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 294/400\n",
      "1053/1053 [==============================] - 132s 125ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 295/400\n",
      "1053/1053 [==============================] - 120s 114ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 296/400\n",
      "1053/1053 [==============================] - 109s 103ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 297/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 298/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 299/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 300/400\n",
      "1053/1053 [==============================] - 94s 89ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 301/400\n",
      "1053/1053 [==============================] - 94s 89ms/step - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 302/400\n",
      "1053/1053 [==============================] - 91s 87ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 303/400\n",
      "1053/1053 [==============================] - 89s 84ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 304/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 305/400\n",
      "1053/1053 [==============================] - 96s 92ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0474 - val_mean_squared_error: 0.0474\n",
      "Epoch 306/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 307/400\n",
      "1053/1053 [==============================] - 92s 88ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 308/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 309/400\n",
      "1053/1053 [==============================] - 94s 89ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 310/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 311/400\n",
      "1053/1053 [==============================] - 93s 89ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 312/400\n",
      "1053/1053 [==============================] - 91s 87ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 313/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 314/400\n",
      "1053/1053 [==============================] - 93s 89ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 315/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0473 - val_mean_squared_error: 0.0473\n",
      "Epoch 316/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 317/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 318/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 319/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 320/400\n",
      "1053/1053 [==============================] - 88s 84ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 321/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 322/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 323/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 324/400\n",
      "1053/1053 [==============================] - 88s 84ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 325/400\n",
      "1053/1053 [==============================] - 87s 83ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 326/400\n",
      "1053/1053 [==============================] - 89s 84ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 327/400\n",
      "1053/1053 [==============================] - 87s 83ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 328/400\n",
      "1053/1053 [==============================] - 84s 80ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 329/400\n",
      "1053/1053 [==============================] - 85s 81ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 330/400\n",
      "1053/1053 [==============================] - 141s 134ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 331/400\n",
      "1053/1053 [==============================] - 169s 161ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0473 - val_mean_squared_error: 0.0473\n",
      "Epoch 332/400\n",
      "1053/1053 [==============================] - 146s 138ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 333/400\n",
      "1053/1053 [==============================] - 165s 157ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 334/400\n",
      "1053/1053 [==============================] - 162s 154ms/step - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 335/400\n",
      "1053/1053 [==============================] - 165s 157ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 336/400\n",
      "1053/1053 [==============================] - 122s 116ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 337/400\n",
      "1053/1053 [==============================] - 118s 112ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 338/400\n",
      "1053/1053 [==============================] - 162s 153ms/step - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 339/400\n",
      "1053/1053 [==============================] - 124s 118ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0473 - val_mean_squared_error: 0.0473\n",
      "Epoch 340/400\n",
      "1053/1053 [==============================] - 148s 141ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 341/400\n",
      "1053/1053 [==============================] - 137s 130ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 342/400\n",
      "1053/1053 [==============================] - 141s 134ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 343/400\n",
      "1053/1053 [==============================] - 130s 124ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 344/400\n",
      "1053/1053 [==============================] - 99s 94ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 345/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "Epoch 346/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 347/400\n",
      "1053/1053 [==============================] - 95s 90ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 348/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 349/400\n",
      "1053/1053 [==============================] - 94s 90ms/step - loss: 0.0436 - mean_squared_error: 0.0436 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 350/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 351/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 352/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 353/400\n",
      "1053/1053 [==============================] - 92s 88ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 354/400\n",
      "1053/1053 [==============================] - 91s 87ms/step - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 355/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 356/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 357/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0443 - mean_squared_error: 0.0443 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 358/400\n",
      "1053/1053 [==============================] - 91s 87ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 359/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 360/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 361/400\n",
      "1053/1053 [==============================] - 93s 89ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n",
      "Epoch 362/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 363/400\n",
      "1053/1053 [==============================] - 91s 87ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 364/400\n",
      "1053/1053 [==============================] - 94s 89ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 365/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 366/400\n",
      "1053/1053 [==============================] - 92s 88ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 367/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 368/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 369/400\n",
      "1053/1053 [==============================] - 92s 88ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 370/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 371/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 372/400\n",
      "1053/1053 [==============================] - 91s 87ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 373/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 374/400\n",
      "1053/1053 [==============================] - 94s 89ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 375/400\n",
      "1053/1053 [==============================] - 91s 87ms/step - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 376/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 377/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 378/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0482 - val_mean_squared_error: 0.0482\n",
      "Epoch 379/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0443 - mean_squared_error: 0.0443 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 380/400\n",
      "1053/1053 [==============================] - 91s 87ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 381/400\n",
      "1053/1053 [==============================] - 91s 87ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 382/400\n",
      "1053/1053 [==============================] - 92s 88ms/step - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 383/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 384/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 385/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 386/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 387/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 388/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 389/400\n",
      "1053/1053 [==============================] - 92s 88ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 390/400\n",
      "1053/1053 [==============================] - 91s 87ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 391/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0437 - mean_squared_error: 0.0437 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 392/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 393/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 394/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 395/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 396/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 397/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 398/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 399/400\n",
      "1053/1053 [==============================] - 95s 90ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 400/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "330/330 [==============================] - 19s 56ms/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'is_scaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5d1455a99975>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"resnet50\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#dnn_model VGG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"resnet50\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_classfication\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpost_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_scaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpredicted_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'regress_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmodel_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"resnet50\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtime_str\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_gray_scale.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mCNN_Regression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./result/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpredicted_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'is_scaler' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred, y_test_ = run_model(X_train_g, y_train, X_val_g, y_val, X_test_g, y_test, model_name=\"resnet50\", model_fn = model_fn,is_classfication=False, nb_classes=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330/330 [==============================] - 38s 114ms/step\n",
      "(330,)\n",
      "(330,)\n",
      "[ 56.97173    39.09628   254.6135    265.0663    524.9647    158.76357\n",
      " 249.61913    65.75686   160.45789   147.94812   222.82823    14.15632\n",
      " 146.38806    55.69239   140.03856    23.388922  112.68035   142.1023\n",
      " 243.96101   155.98552   487.3062    259.1172    275.48492    61.5182\n",
      "  62.932186   58.56553   150.4173     60.131027  121.69925   117.84922\n",
      " 121.12922    92.63883    88.32725    44.95533   133.37387   150.64424\n",
      " 297.5664     30.693964   24.726406  361.60596   256.6332    286.14972\n",
      " 136.44533   231.74408   270.04456   276.81927    44.445755    3.8105621\n",
      "  62.72774   161.20285    35.903336   84.789665  127.295555   20.923145\n",
      " 160.0852    115.94755    81.006294  233.1496    202.4234    -14.231533\n",
      " 229.66307   289.95526    36.227837  436.9232    240.67078   266.18576\n",
      " 259.27576   226.55405   262.921      70.26511   240.27756   -15.753969\n",
      " 175.17888   119.50866    40.634632  189.17473   147.04182   107.22235\n",
      " 111.93927   -18.26769   138.10715   332.8541     70.17914   -10.346174\n",
      "  27.79243   100.07469   100.80367    75.443695   71.763824   80.067764\n",
      " 237.67712     8.665599  198.45572   136.99861   260.0676    268.55728\n",
      "  94.85115   189.17473    93.85043    74.90845   160.36908    78.896576\n",
      "  49.39691    -2.9813573  35.18252    54.685028  123.385574  229.15672\n",
      "  77.87273   -25.145636  -20.258575  185.82008   116.15853    66.32969\n",
      "  73.613434  355.23425    35.769382  262.77365    60.735725  290.04044\n",
      " 242.46678   134.6841    148.2402    222.16037   228.68451   131.4265\n",
      " 301.46332    75.01496   -22.499249  132.60234   238.30983   249.49385\n",
      " 404.72772    42.676254   60.123257  176.6318     80.99797   109.32411\n",
      "  12.419067  498.50235    51.66343    53.214245   35.123787  421.42508\n",
      "  -2.4646297 107.1673    112.14899    84.64153   111.61304   156.79811\n",
      " 262.465      60.686077   91.700035   37.329884  421.28302    58.17435\n",
      " 150.7347     93.646835   36.556393  108.83909    53.890415  260.82385\n",
      "  70.57223   111.36287    37.30306    43.314472  230.90462   109.325066\n",
      " 236.26303    39.26836    91.42462   100.38425    22.530548  148.83441\n",
      " 115.92053   227.12033   260.74194   114.77666   261.0168     41.032417\n",
      " 421.30972   147.88977   105.44954   118.13489    96.82494   109.02225\n",
      " 122.75145   -12.430489  134.28653    32.439545  159.59975   106.728294\n",
      "  30.420319  140.11058   364.3922    -19.193605   88.30911    21.253347\n",
      " 106.37481    54.654743  103.18846   178.50195   149.48904   102.71256\n",
      "  60.23408    62.16182   104.80393   178.5371    106.0615     90.812195\n",
      " 185.79836   163.47078   112.60179   111.99722    76.93234    97.705444\n",
      " 211.92213   267.75516    57.433247  280.7067     80.535675  151.40387\n",
      "  42.661465   94.258575  310.54758    59.69147    29.201672  100.09089\n",
      " 135.01498    68.679245  266.53207    44.512325  229.93538   229.16663\n",
      "  77.33295   108.69445   169.29495    47.475433   84.10004    41.797115\n",
      " 219.19766   107.50848   127.4927    105.537476  326.04257    50.320545\n",
      "  59.326336  136.98387    35.67283   131.52246   157.1631    173.08862\n",
      " 118.38372    52.29273   283.94736   257.3898    107.503815  -17.296017\n",
      "  32.422325   88.37537    72.32146   150.78001   153.48123   493.5658\n",
      " 239.90918    67.758934   96.494156  271.48056   155.9092     95.40089\n",
      " 104.183304  111.60362   264.72763   254.03064    39.413063  124.05318\n",
      " 142.2253    222.16037   119.23465   120.51248   104.813225  210.48882\n",
      " 295.02518   201.15659    78.25232    29.006504  101.43875   -29.559597\n",
      " 300.94116   297.91898   138.09154   141.6755     42.205246   21.45876\n",
      " 292.07736   241.39519   123.963844  130.21252   154.67955   238.14804\n",
      " 201.89244   455.9369     -6.8598313  81.28584   125.29728    40.959194\n",
      " 150.79466    80.98778   404.05444    90.353386   72.151535   15.7682\n",
      " 145.64496    18.042803  202.23781   520.24817   428.63788   108.38302\n",
      "  55.71434   262.8707    235.72046   122.28109   235.80324   271.53403\n",
      "  -8.205593  144.13318    54.642902  152.88696   143.9247     47.96028  ]\n",
      "[  6.  54. 275. 240. 450. 108. 225.  58. 150. 162. 250.  80. 162.  64.\n",
      " 100.  46. 174. 128. 250. 184. 500. 300. 285.  68.  88.  30. 120.   8.\n",
      " 128. 154. 144. 126.  80.  52. 112. 158. 300.  18.  66. 450. 265. 290.\n",
      " 168. 255. 295. 300.  32.  24.  96. 132.   6.  92. 186.   8. 180. 196.\n",
      " 108. 230. 225.   8. 215. 290.  54. 450. 250. 270. 230. 200. 225.  80.\n",
      " 245.   4. 190. 168.  44. 225. 182. 122. 120.   4. 124. 350.  56.   2.\n",
      "  12.  84.  84.  78.  84.  82. 225.  18. 176. 174. 198. 225. 100. 225.\n",
      "  46.  60. 188.  88.  70.  10.  24.  48. 172. 186.  52.  10.   2. 188.\n",
      " 156.  48. 116. 400.  30. 275.  40. 275. 215. 156. 190. 225. 245. 172.\n",
      " 235. 102.   6. 122. 265. 295. 450.  46.  86. 196.  84. 112.  10. 500.\n",
      "  20.  58.  34. 500.  14.  82. 104.  92. 154. 188. 285. 100. 120.  34.\n",
      " 500.  16. 132.  46.  12. 136.  22.   1. 100.  90.  58.  38. 270. 112.\n",
      " 215.  32.  26. 124.  16. 144.  90. 182. 300. 134. 215. 102. 450. 128.\n",
      "  94. 138.  72. 146. 158.   0. 112.  40. 178.  98.  32. 136. 400.   4.\n",
      " 112.  28. 106.  28.  74. 200. 192.  96.  58.  32. 140. 172.  72.  40.\n",
      " 200. 156. 162. 142. 134.  76. 255. 240.  58. 280. 124. 128.  28. 136.\n",
      " 275.  26.  16. 128. 134.  64. 225.  32. 186. 220.  52. 154. 184.  54.\n",
      " 114.  18. 220.  94. 130.  60. 295.  74.  60. 164.  10. 198. 138. 200.\n",
      " 128.  74. 350. 285. 102.   2.   4.  92.  86. 154. 190. 500. 186.  42.\n",
      "  62. 275. 110. 124. 116.  98. 275. 265.  28. 182. 198. 225. 170. 154.\n",
      "  86. 235. 300. 240.  84.  36. 150.  14. 300. 250. 164. 160.  78.  22.\n",
      " 300. 230. 112. 118. 118. 250. 178. 500.   0.  90. 120.  80. 186.  90.\n",
      " 450.  74.  64.  24. 140.  36. 215. 450. 350. 136.  68. 290. 220. 116.\n",
      " 300. 290.   8. 120.  48. 188. 164.  86.]\n",
      "mse =  1048.1315974765887\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('./modelWights/weights_resnet50_fold_11570509297.3577418_gray_scale.h5')\n",
    "y_pred = model.predict(X_test, batch_size=1, verbose=1)\n",
    "y_test_ = y_test\n",
    "time_str = str(time.time())\n",
    "y_pred, y_test_ = post_training_data(y_test_, y_pred)\n",
    "predicted_file = 'regress_'+model_fn.__name__+\"resnet50\"+time_str+'_gray_scale.csv'\n",
    "save_result(\"./result/\"+predicted_file,y_pred,y_test_)\n",
    "# draw_scatter(predicted_file, \"resnet101\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330/330 [==============================] - 26s 78ms/step\n",
      "(330,)\n",
      "(330,)\n",
      "[ 56.97173    39.09628   254.6135    265.0663    524.9647    158.76357\n",
      " 249.61913    65.75686   160.45789   147.94812   222.82823    14.15632\n",
      " 146.38806    55.69239   140.03856    23.388922  112.68035   142.1023\n",
      " 243.96101   155.98552   487.3062    259.1172    275.48492    61.5182\n",
      "  62.932186   58.56553   150.4173     60.131027  121.69925   117.84922\n",
      " 121.12922    92.63883    88.32725    44.95533   133.37387   150.64424\n",
      " 297.5664     30.693964   24.726406  361.60596   256.6332    286.14972\n",
      " 136.44533   231.74408   270.04456   276.81927    44.445755    3.8105621\n",
      "  62.72774   161.20285    35.903336   84.789665  127.295555   20.923145\n",
      " 160.0852    115.94755    81.006294  233.1496    202.4234    -14.231533\n",
      " 229.66307   289.95526    36.227837  436.9232    240.67078   266.18576\n",
      " 259.27576   226.55405   262.921      70.26511   240.27756   -15.753969\n",
      " 175.17888   119.50866    40.634632  189.17473   147.04182   107.22235\n",
      " 111.93927   -18.26769   138.10715   332.8541     70.17914   -10.346174\n",
      "  27.79243   100.07469   100.80367    75.443695   71.763824   80.067764\n",
      " 237.67712     8.665599  198.45572   136.99861   260.0676    268.55728\n",
      "  94.85115   189.17473    93.85043    74.90845   160.36908    78.896576\n",
      "  49.39691    -2.9813573  35.18252    54.685028  123.385574  229.15672\n",
      "  77.87273   -25.145636  -20.258575  185.82008   116.15853    66.32969\n",
      "  73.613434  355.23425    35.769382  262.77365    60.735725  290.04044\n",
      " 242.46678   134.6841    148.2402    222.16037   228.68451   131.4265\n",
      " 301.46332    75.01496   -22.499249  132.60234   238.30983   249.49385\n",
      " 404.72772    42.676254   60.123257  176.6318     80.99797   109.32411\n",
      "  12.419067  498.50235    51.66343    53.214245   35.123787  421.42508\n",
      "  -2.4646297 107.1673    112.14899    84.64153   111.61304   156.79811\n",
      " 262.465      60.686077   91.700035   37.329884  421.28302    58.17435\n",
      " 150.7347     93.646835   36.556393  108.83909    53.890415  260.82385\n",
      "  70.57223   111.36287    37.30306    43.314472  230.90462   109.325066\n",
      " 236.26303    39.26836    91.42462   100.38425    22.530548  148.83441\n",
      " 115.92053   227.12033   260.74194   114.77666   261.0168     41.032417\n",
      " 421.30972   147.88977   105.44954   118.13489    96.82494   109.02225\n",
      " 122.75145   -12.430489  134.28653    32.439545  159.59975   106.728294\n",
      "  30.420319  140.11058   364.3922    -19.193605   88.30911    21.253347\n",
      " 106.37481    54.654743  103.18846   178.50195   149.48904   102.71256\n",
      "  60.23408    62.16182   104.80393   178.5371    106.0615     90.812195\n",
      " 185.79836   163.47078   112.60179   111.99722    76.93234    97.705444\n",
      " 211.92213   267.75516    57.433247  280.7067     80.535675  151.40387\n",
      "  42.661465   94.258575  310.54758    59.69147    29.201672  100.09089\n",
      " 135.01498    68.679245  266.53207    44.512325  229.93538   229.16663\n",
      "  77.33295   108.69445   169.29495    47.475433   84.10004    41.797115\n",
      " 219.19766   107.50848   127.4927    105.537476  326.04257    50.320545\n",
      "  59.326336  136.98387    35.67283   131.52246   157.1631    173.08862\n",
      " 118.38372    52.29273   283.94736   257.3898    107.503815  -17.296017\n",
      "  32.422325   88.37537    72.32146   150.78001   153.48123   493.5658\n",
      " 239.90918    67.758934   96.494156  271.48056   155.9092     95.40089\n",
      " 104.183304  111.60362   264.72763   254.03064    39.413063  124.05318\n",
      " 142.2253    222.16037   119.23465   120.51248   104.813225  210.48882\n",
      " 295.02518   201.15659    78.25232    29.006504  101.43875   -29.559597\n",
      " 300.94116   297.91898   138.09154   141.6755     42.205246   21.45876\n",
      " 292.07736   241.39519   123.963844  130.21252   154.67955   238.14804\n",
      " 201.89244   455.9369     -6.8598313  81.28584   125.29728    40.959194\n",
      " 150.79466    80.98778   404.05444    90.353386   72.151535   15.7682\n",
      " 145.64496    18.042803  202.23781   520.24817   428.63788   108.38302\n",
      "  55.71434   262.8707    235.72046   122.28109   235.80324   271.53403\n",
      "  -8.205593  144.13318    54.642902  152.88696   143.9247     47.96028  ]\n",
      "[  6.  54. 275. 240. 450. 108. 225.  58. 150. 162. 250.  80. 162.  64.\n",
      " 100.  46. 174. 128. 250. 184. 500. 300. 285.  68.  88.  30. 120.   8.\n",
      " 128. 154. 144. 126.  80.  52. 112. 158. 300.  18.  66. 450. 265. 290.\n",
      " 168. 255. 295. 300.  32.  24.  96. 132.   6.  92. 186.   8. 180. 196.\n",
      " 108. 230. 225.   8. 215. 290.  54. 450. 250. 270. 230. 200. 225.  80.\n",
      " 245.   4. 190. 168.  44. 225. 182. 122. 120.   4. 124. 350.  56.   2.\n",
      "  12.  84.  84.  78.  84.  82. 225.  18. 176. 174. 198. 225. 100. 225.\n",
      "  46.  60. 188.  88.  70.  10.  24.  48. 172. 186.  52.  10.   2. 188.\n",
      " 156.  48. 116. 400.  30. 275.  40. 275. 215. 156. 190. 225. 245. 172.\n",
      " 235. 102.   6. 122. 265. 295. 450.  46.  86. 196.  84. 112.  10. 500.\n",
      "  20.  58.  34. 500.  14.  82. 104.  92. 154. 188. 285. 100. 120.  34.\n",
      " 500.  16. 132.  46.  12. 136.  22.   1. 100.  90.  58.  38. 270. 112.\n",
      " 215.  32.  26. 124.  16. 144.  90. 182. 300. 134. 215. 102. 450. 128.\n",
      "  94. 138.  72. 146. 158.   0. 112.  40. 178.  98.  32. 136. 400.   4.\n",
      " 112.  28. 106.  28.  74. 200. 192.  96.  58.  32. 140. 172.  72.  40.\n",
      " 200. 156. 162. 142. 134.  76. 255. 240.  58. 280. 124. 128.  28. 136.\n",
      " 275.  26.  16. 128. 134.  64. 225.  32. 186. 220.  52. 154. 184.  54.\n",
      " 114.  18. 220.  94. 130.  60. 295.  74.  60. 164.  10. 198. 138. 200.\n",
      " 128.  74. 350. 285. 102.   2.   4.  92.  86. 154. 190. 500. 186.  42.\n",
      "  62. 275. 110. 124. 116.  98. 275. 265.  28. 182. 198. 225. 170. 154.\n",
      "  86. 235. 300. 240.  84.  36. 150.  14. 300. 250. 164. 160.  78.  22.\n",
      " 300. 230. 112. 118. 118. 250. 178. 500.   0.  90. 120.  80. 186.  90.\n",
      " 450.  74.  64.  24. 140.  36. 215. 450. 350. 136.  68. 290. 220. 116.\n",
      " 300. 290.   8. 120.  48. 188. 164.  86.]\n",
      "mse =  1048.1315974765887\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_g, batch_size=1, verbose=1)\n",
    "y_test_ = y_test\n",
    "time_str = str(time.time())\n",
    "y_pred, y_test_ = post_training_data(y_test_, y_pred)\n",
    "predicted_file = 'regress_'+model_fn.__name__+\"resnet50\"+time_str+'_gray_scale_pred_color.csv'\n",
    "save_result(\"./result/\"+predicted_file,y_pred,y_test_)\n",
    "# draw_scatter(predicted_file, \"resnet101\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9e83793ef0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "get_layer_output = K.function([model.layers[0].input, K.learning_phase()],[model.layers[-3].output])\n",
    "layer_output = get_layer_output([X_train[2].reshape(1,512,512,3), 0])[0]\n",
    "plt.imshow(layer_output[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-369.41626 -526.9378  -430.79715 ... -362.27216 -358.80005 -254.3877 ]\n",
      " [-519.6171  -739.9861  -589.9607  ... -488.06784 -481.176   -342.8683 ]\n",
      " [-482.4566  -677.2405  -518.43726 ... -421.33786 -411.26694 -290.80206]\n",
      " ...\n",
      " [-469.83298 -654.7402  -492.50415 ... -401.03516 -389.57565 -274.44693]\n",
      " [-465.01306 -644.79114 -479.0014  ... -389.45715 -381.78152 -269.61908]\n",
      " [-310.00427 -432.5793  -319.30478 ... -258.98465 -255.72847 -181.82275]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD7CAYAAABqkiE2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO19e6wtV3nf75vZ+9zjew1+8KqxrdpIblpaKYVaBJIqRZA2hKKQSkkFilo3pbJaJW1eUoDmD1SpqUIb5aVWSa2QxKkIjxBaLJo2pQ4oqlTcXAIlEIfgQAI3OBg3GD/vOXvP+vrHen1rzTezZ/brzDln/Y6OZu95rFmz9qxv/db3WsTMKCgoOL+oTroCBQUFJ4siBAoKzjmKECgoOOcoQqCg4JyjCIGCgnOOIgQKCs45diYEiOg1RPQZInqIiN6yq/sUFBRsBtqFnwAR1QD+EMDfBnAFwO8AeCMz//7Wb1ZQULARZjsq92UAHmLmzwEAEb0bwOsBqEKgvvYSz264EbOnAXNg95kDJ5zInVQxyH0mssfqyqCu7Oc5NXYfGVTueIVUwBE4lOeLZfF5GLh1Pol9hFhvf3/KtvL+XffoAiVnUOfZ7VrGfaydx6vuPKKdSH5sl9hqq+TYsEFJe74hZcjrTFYGg1oNIH+xvpr11Uee07A9r+EqbJfiMwAYY88xLO7prmMmwLQqB2S/o0R9YQkAeOahP3uUmZ+XH9+VELgZwBfF9ysAvkGeQER3A7gbAOobbsALf/gH8PzLwOO32YZ4+lZbccztE1eHDeYHdt8Ft7328Ag3HD4DAHje4ZMAgOvmz+Da+ggAcFgt7LWuZayAsOXVbl8DCp+HoCITzo9lmSCE5rR092Ic0iLcNzkGRhV+yTb8+eoxUe/w0ohZXYP0JfMwqOKLB/+SVWjgX0BqlZWjr85p/WN71tk1sv3kc/qy+55dIn++9P7dZcg2O+bafg7tUoV28DCIHbRR2sb4Tj1gZn1k5niiOQQAfHV5EQDwtcU1+H9HlwAATxxfAAA8eWS3R8say6Wt43IRt7xw9/JbBsgJDt/c1DihUTFuuP2rAIBPvO7f/IlWr10JAU0sJj2Nme8BcA8AXHrOrfy83wWe/asfxcVX/Q0AwKPP2IZY2jbD8hJjeY0t4uiifdInrz3EE8+y5z1z7RwA8Lxr5rhubgXDNbXthL6DVsThs4f2wlTg1ighj0nW4cv3+y5U9oc5wDJcMxefAQC0hKaO8R3Bv6ha3WQnDy+oqGsuGGTHly87ACy4FsfddUydL3TeoQHbpr4jeAbmf+kaJjxD6OSoYPxzunrXgqH5OvrzDaqW8EmEntJWJhMQ8np/7THXWPDMlREF4LHbl3duKzC7R3vD7d8iP/a0OcBXF7bzP3p0LQDgK1evxaNPWiHw5FP2ZV8+47rlUYXqyLXHsS334Aior7r2XrjnWwL+tfbbaml/BFNX+PP59Z31BnYnBK4AuFV8vwXAl7pO5ho4fhZhdusteOxmOx94+gXuIQ7d9poG1UXbma65xj79dZeewU2XHrc3uPgYAODmC4/huvppAMDFyjKCgzBKN62OpY1uNZnOkaaWTMBde0BNGOUP3XYuzptn7KMCUGfvina32m0b5RgQWWGjEJlwLAgNagm2hXixU3aQntfHlCpFUMnzI2tS9gVGwKPYmKzv0Hrm18n2iM9Oant03S8tNwqL9r3svqs8x583tvN/+cJ1AICHD6/DlQPbSR+9xh772jNWGFw9nuP4yHbR5ZFjAsdtwUCNFAJOQCxd/WfA/Pqj3rrvyjrwOwDuIKLbiegAwBsA3LejexUUFGyAnTABZl4S0fcB+E3YAe0XmfnTnedXwOISoXnB9bj6HCvBljfa0Z4OrYg7uLDExcNjAMCzDq1ke+41TwYGcOvhnwMAXjh/DM+prX4gMAH46YBpjRY1OBmlhiAfyQ7IYO4Girk7Z06EGp4mO8oqRpMqk7819Y80ORrFqmMEq2lc3UxyHqfHYEI5nm3Icd0zjJy1SPQxGHst9ZyXto/c14dmJGuQ0NvDTz3Q2x69dcqqZJCyDgA44hrPrq4CQNAXzakJ06yDyt71cGaPPXl8AU8dWGZ89YJ9sxaLGuY4sgL7gQIDiFtXjxlww7XP9NZ9V9MBMPNvAPiNXZVfUFCwHexMCIwCATwDzIUZjB9KZ260nVkpOZs1mNVOEVc7dlA3rVE81+4CaFkE8s99+zTIuWzc58oQI3pFXvGV1kmygOR8P68cPP4MqCv5uW372SoAwe6qHe8ZlOvuQ/EcohYDGDLSrywXNJoNaM8/BL7+q34Rz5YkI5CWnFa5XmFKBhcqp+tyiuwD937P6/jO127bLCuYylM0tzXRbBhe//BCcjCjd6G4DRcUnHNMggkwAaYGmgs1jKsR1V5SOkZQMerKMQE3d5qRCfOpWhmdc5OWHOl9ucm+MFemlqNRH/rmu+sg1xfYOm3GDiqiMBrWQQMu2iqwFl3fkJ83Flp7VKKsrvaqhElRK28oI+hjREm5GTPqssyMQa04r/n37pAWgQnMwnvttpXBzLGCurLciypG5RhA42lHxfBGiWCsCZ51CP2mC5MQAgCACuAZxYdxFCZsKQoB34HnVYOZFwzedg/TMvtJM1Wu1ANSwSHP19AwhfOj4BnzoNtBTdTbWTVoHSEXNwaxI+Tla/R+HVQjBUmfUJTCY4hAGCoMPOS0x79VvW2QunS6evmyOJiS47YRjmZu6ltFx6kgQMRgaNxncsKAG45zNy/DwnZAm6w8o6Cg4ExjMkyACeCKgljyVqMQJ0AcpKKXlNJ7rxLee3WmCJTbXKmXTgcseskTpU4/HkMUZdqINgQaHR7LAjRoCrYK8fn7qP/Y6U7f6N9XlnE+g63y1LbUfzmNIWjTI4Bbz96Ec7R6y3v4ejmIR/L3WUCyRznaRyWh3SoxFYEJGJDrHJ4lcwXAX+MZQZgW6OVJFCZQUHDOMQ0mQLA6AfcPCAbgFYRVOzqwIhajvpCsWYCPvM4zgHmiE0iro0n9oINRrpOSdJtSdZumwm1g7LOtmvtvw1yY3C+roaY3SBDiFVYrQhvmXraXH6vRZgerlM3+PZ2J99bvIzmah33iu/vC+bEBmIYQAGBqhpkTjPcPyOyjdSUVg3ErBYJH7hfQ5x9Q08CXW2nUYIqFTp2DR9wA78B1pwq7QF9NWj4PA9+2bXf4IZBt2mdhSJV56fsxZGrUCSdcTFL+alRCQRgGMD/giJB6KQyCf0DuJ0CrQ8Cn8+YVFBScCKbBBMhGEpo5wYV4B0+pYAKsm+gfEEwomhdclPjtUb+fymuKnlhuG/58aTqL/vAKMxjJAHbhQdiqU884kSvUktiHPTGAbTGkrqlCcg5RGK0bRfk7Bl2/mBZ16RGntGL66t91YR6vvELQOKZRsfAetJvwy1Uc+ksXChMoKDjnmAQTYFiFYCOZwMyN+o4RHNRN8Kk+EJ5VQzPReKyKesv3aR5jQ8yB20SX11zfeWM96jRoZaxr6huLPgawjYjLpDzxnOH5lEu0thzTzjWxWq6mwAasB6FXEkpG4PVkPg1ZNTNwpADsNes+j82MT4nHoJ8OzAjsFINzEUQB2I7v3Sn9tEB6W6XWgfXos3y5+uzD+fkV2tMArUOEMolaL3mlZaRJfBiGTQ3y8zYRBoNe7C10/KkoRaXfhOZZuI1nzbMe2XLdOx/chWMmrOgqb7eL2qAxTghIs1bIjmSPsWtTmpnQX7owjdYvKCg4MUyDCQDgmmFmFAKI5mIaAFga5BUcByHgQiQOFWbBPKVVJUyFmkJQw7qBMhKbJg7x7EBjBBDToHW9BzVqP3Tk2xcD2MbvoDG8oYFJY2MNNCR+JEp6u+DtWmUxBCJztpwO+KAiI8Lm2U0NuPbJR72pnUMfGlK/goKCc4hpMIFgIkRIJhJH/agTOMgkZRqBtV7QZ5ejTx92kSij/36UsIEcWtSfphvIRzf1XsKnXjs2OmZgzXFmGwygr9yGudds2MUIkvO3Eb+hRLZGZ6GoGJxR1A00LqO1ZwlccXCuY68bmAlP21Weihs/RUFBwanGNJgAYBMj1ASuUyZAIXJQOguJFOJhvh/z27fchkcmElWrt3EJm96/rR/IoeUYkIxgqKVgbLx/6/qRrTWeienlj3Gq6mqrvIy+fAV9iVpk6rbxeR/ie+vfdZ9URFoM/NYwRQeiOs16UM8MDups3YsM0xACPoCopuAP7R9wJj2lNrB5A6s7shZKPPR17jMN5i+5niSDVTNh5/0G+g5oWEXpu4SEli15nTqM6fRjBMo2hEPfc+XtJv0K+rI2DUUeDFeBk/cfsP2iMq6Du47fmJhoJAgDjtOB2QpfmpMe4AoKCk4Y02ACYLDPk+ZNeHlaJRFWKR2Dcmhr6Y1d3WaoZOwbUffhAKMptrpSg41hDmOeS5a7TZPfNtuvKz1ZV1vJa/rabJVzkQbvLJTmu4wjv/2uRcbK8PncbKgtluruU5swpehCYQIFBeccE2ECFkwAqsxcoiQGleiLysoxOHfAqnLWjKnvg6YPqF0aqYaHPWMSO5DVSTOJaegb+bqu79q/jpmvP6pylWs1dR7T7pO31TqMoNOUyPr98/e1hmkx2zjq6+a93IFoVlMwDaLOyicOJvUurC0EiOhWAL8C4C/A6tLuYeafIaIbAbwHwG0A/hjA32fmr64uELaH+jwPg7KkjqP5MinrukFA204l3tf55XcvCFZZCbpe2q4O2Tdt2KeNf2znH3usTyB49GVwXkfp2PYnkQJLJMYRnd6eF0OKW9m0wKFv+KerK4Mm5BZ0d/Wh+LXBLnMMLgH8MDP/FQAvB/C9RPRiAG8BcD8z3wHgfve9oKBgolibCTDzwwAedp+fIKIHAdwM4PUAXulOuxfARwC8eVCZPQOIVQx2Z2Ptg18GSmMOq3Lpn0RaLA351GBo1GE81kFne7wN++uzng1cYt3Rf7179cRhOEhloceq54tl2PbVw43lZ+146tciR3+ZSg9wJkKnXPTmQzZVMBeyqy+5zlRXe4oiJKLbALwEwAMAXuAEhBcUz++45m4iukxEl5snn9pGNQoKCtbAxopBIroWwK8D+AFmfpwGzgeZ+R4A9wDAhdtuYZsxFa2siFKRkq/G0pVQxJthGleXuXrW7tE3N153pOtTFvbFGGziXBTvrTtCyTwJXRiqXxjSLlJnItsh16Vo5/TpVFSPwZ5nGpqsZKEy0HT07ywje+dnlcgn4OpG4nhcfFREHa6Iq9lICBDRHFYAvJOZ3+92f5mIbmLmh4noJgCPjCtUt5V2aUo9mi34B8Tr+gNs+q7ZxL7d9RJ3ndslCIBhirC0vHa47TrX7hJa+wxpM01odAnModMoW25bkSgzGJtMOFYcg4XkIiT54iP5dyBS/6URWbc5BhIFZWGWcnzIL7P2G0t2yH8HgAeZ+SfFofsA3OU+3wXgA+veo6CgYPfYhAl8E4B/AOD3iOgTbt+/BPDjAN5LRG8C8AUA3zW0QKkY1LymzLoUesOYA1uPcfeWo+MwRdt4ebxqagCsl6JsG0q/seiaBqzTLn2QbTbEr2DdtG5AfGeqwAh0pXaVKQZlePys5U1o4FWNmjdtWPBEeN6uUqRvYh34X+hmG69et9yCgoL9Yjoeg04p6BdZ1CVmTxjtFnLzjzUHalF1Q7Bt89c6noUeQ5SFm8z513c42r9H+yrFqkeSfKRrGXcRV1ALE3VriTzI5cfTUV+mz9NGfX8eESeOQwDQmKgvWNU3SuxAQcE5x6SYgIwi7IOUbJqU64onyBce7cImC2nuQxewLQzVDewa22ZGm2BTPcHYPAsyRb4WHZuzBPs5tQRIduBry8FkuNpteEJCgBMBkIdVrsoOJOlVH+TSYfZ8QdfWfOnXsYGfZOdfhS6F4LaFwpC4iV2hy8SaY6hfgTYtqHuulV6CbcofhUI+BVanySLbUIgdkMl4dhg7UFBQcAYwHSawJvqkXIy8QrLdBF3KwH1R57EY6jw0xBzYMG+cGmxfpsAh0O7ZxQ66IhJbWZ1DKLGWrzC+k31maxlDEB2IhplrfX/waxJYZWRRDBYUFPRgWkxgR74pUiEodQGDrlVGrrFmr3z029WoN3Seq1+7PQehMQzA3vt0jUVD2ZVvh5gWjxRzYFvJLd2HW/kERMLdsOS4iXEEIdewUB6eHsVgB0xPfPHQrEKNa4P5Hhn7SS2y2eUzsGoBk5PAFDv/GJ8L2abatKDJMhDbz0rsgPgsj9mU+pnSUCoSg0Kbg2fh0tWDheVg9XMUFBSca0yeCWwTDdIED8B2koYMVZZNceTrw9g0Y2OnAVPGUKVhPjVIFnsJGYj9uXHUlYzAMwAfMyA9B3OzYeIxyHG0N9m0wQxUJPp6FRQUnGNMngl0OUds9x6K00rHCLZqVJTHpzQKDk2iYc816vcK1aAEImcVOTvo0xtoCVvrJE5A5sloZyC29xNLk8vzcy9C3mwsn54QcO+qDxvuUwx2ocsGu26G4VY5SgdoL8pxOrwDPfYZNnwa2mMI1rHG5NMBW07qNiw7uVySzJ67Wts/Fmfj1ygoKFgb02MCPdBMgnmmVi34Ymjg0LrLip2UOXBTbJpzsCCiN5uxNOkFJaFXDBrk4cUetaYYRHth3l6vWWJ05eKM9SwoKDjXmA4TICTZhluLM6qRWF1z/9QhI8zDxBoDqyIH111aa0rKQEBXCPYxgMIOxqFrPYgux6wYxRrf0Ty9mPQcbC9NJpiGMBWGUGLm1vmrVuoqTKCg4JxjOkzACStS5kAe2noD+XxnnaSirQVGT7Fs7NNWa6O8twoMYQAGJrSNZioc4lx0ViwDq6C5advYAR9l6M4To73HnJbufA4ORJId9LGCJvs+ZL3O6QgBYHQAUdUjDLqwaQKRzrqcYIIMDykA8hewTwAk+3pDXKnV0ceGFxdY5FNWQJgKyccOLIUXYbe/TEUm+AqEhXx9KLEiZFrlrPsQBQUFZwPTYQIrnIIS6qMs8bwrbGNpralBMgA/svcxgKE4re2xDQyNPswT3NQyA3FPvkzpOTjEU3DINCCvU0FBwTnFdJhAh8ODZAB5mqRaYQR9OQY0iXfa0oVpWDeRiIHZCgPowtTMpftGfH7/XukLg+ZrEObuw8k+sJpqbBNX4o2ZABHVRPRxIvqg+347ET1ARJ8lovcQ0cFaFct8pL1W1HtA2eAKG5CRN8qQDKvJ+e7P4zQJAMBS0ZPSuo8Joz4PloG+51RDrdUObULcgJ8u9L3TuZVgrEDYxq/y/QAeFN/fDuCnmPkOAF8F8KYt3KOgoGBH2EgIENEtAP4ugF9w3wnAqwC8z51yL4Dv2KiCZFoUX/Of9pBTBp/EoYaeTGTdhCI5c1DrcUpGvTHtsMupw3lDTe2YFqkktOeY1vuffO7pB+SWK99HerGfBvAjiPkNnwPgMWZeuu9XANysV5LuJqLLRHS5efKpDatRUFCwLtYWAkT0OgCPMPPH5G7lVFUUMfM9zHwnM99ZX3up++oVsPMmE/QEdh+7tE1e4kZPrYoIFdnFIvORTZ7XhSmbwST7qECjFHNDGEENUllQHzM6L7qAHF3PrLWzf4fj95QR2PK8Hox7GYCmE+g7H9jMOvBNAL6diF4L4BDAs2GZwfVENHNs4BYAX9rgHgUFBTvG2iKamd/KzLcw820A3gDgt5j5uwF8GMB3utPuAvCBTSpouILhmL+9y2QYRn90S8ptJBU9rdCTgK4/QnvWlKdTk/8FKTSWOTTWRb7Tvh8k/ytG+/6yt483A/ghInoIVkfwjk0Kq4Qp0COhRl55AvdPbcWgzPIaythAMTgE69ruN0VOv2WH9LQ9mR7l5lHXLtq/vFZeVzq9Dv9baO2TZx5eJ/BNgyYgVmErzkLM/BEAH3GfPwfgZdsot6CgYPeYkMegCx/oGVBWLawYzhvpKLTJcXvO9EZBLdlFnhsfZEIcQb6CTnJeD87S8mL7QJWFEifH1oyDkQuXGq7DPr8tUYQFBQW9mA4TyKCZOeKx6GPdv8RzXuZwPcCUTYHbRL4QaX9CEFpx/Hy02TYxVplXC72XFk0YViAawU4nJwTWddsfmlTEYxudfIrTgHXRtyrxaYulOEvQsmiHY8kipa7zh4xFwzMLFdFdUHDOMTkm4JEvv5TsU1Iyhe9rmFt2MdKdVmpcRv39QFsNK6YX88pcJW5GiZpdhb7wenufgoKCc43pMYEOhaC6+tDI5KLqsS2PfKeVARScDNJ0Ye1jHvk6HHKfRG4qHIJpCAGx8EjMxaIpqFZnD9qW51UfTqtCMF8YQypHy6IjJ4d+C5eSd3DL73gZtgoKzjmmwQQAgBhMcvERu51VNi/bnJogAbXsrGm+QbdvTarfv/ioXuZUpwHDs+C2PQa7zinYDfqyDvdNfdXwYW5PHzqvH1PJgoKCs4fJMAEiWJGULUNWKz7Q8lieZbhP8vlIuC6swwDOE+QyZBJTZUGnHZo+IPUY3FLk4VZKKSgoOLWYDBOIOgH7tc7cHitqL87o04rJffZauGvdVkvmMFBfMIQBTHkkHJrXoFgHTg5NzztWZcy4O0X5+u/gNISANxFWQjGY+QfMqdF9BTJl4T5MhKcV+SKldl/p/PvCKsVqnzCQMQS5YND8D1d5Cab1KigoONeYBhOAzZEupwP5Guw1xbDhtiSM2JYjxXlQBA5drnzV1MlPOaY8LZoKxqa061IOJt8p5hfMQ4iHMILyqxUUnHNMhAk4uFVTJORCjJWiBNQQEzhSsu2ak7Xz6J99FpBDYwAF00ZfNGGalbv/t52WEBDojQ8Qi4ysm5etoBtjPS3LNGA86p5cg2Mhk4qEaQG7AVAGKHVeX1BQcK4xHSbgBCMFRWCPYtD7CYilx6QX4fAgyoIc68RbFMXg7tC4cbrPfLgpyq9WUHDOMR0mAKQmwtwcuMbkaRNPwfOG0i67hzTJRkXsybf7RkyAiK4novcR0R8Q0YNE9AoiupGIPkREn3XbGza5h1yxNV+rPT1vvaWcthEee1JLju0aDXPyX7A5/GrYQ1bAltjmUmU5Nu0BPwPgvzPzXwbw9QAeBPAWAPcz8x0A7nffCwoKJoq1hQARPRvAN8MtOMrMx8z8GIDXA7jXnXYvgO8YXuiw02qXcbVyi48k2Ye3IC01H/vThoZNi6HkC2OWJCHTRrOl32cVi9jkLi8C8BUAv0REHyeiXyCiSwBewMwPA4DbPl+7mIjuJqLLRHS5eeKpDapRUFCwCTYRAjMALwXwc8z8EgBPYQT1Z+Z7mPlOZr6zftaloBAk5zWYz/+tidB0egtKaZfPt/Llt7eN02IaM+CE5WwSQViWI98e/LvbZrVCkQha20y46tpN3t4rAK4w8wPu+/tghcKXiegmAHDbRza4R0FBwY6xthBg5j8D8EUi+jq369UAfh/AfQDucvvuAvCBwYUO1QkMtBi0rtuRGWyK1oGaqrUZSm4NWKXJ3uRe5xWyjceM8k22+Og2omY39RP45wDeSUQHAD4H4HtgBct7iehNAL4A4Ls2uUFf2PDYRUjPE3YhmIoicVoYs/JwHzYSAsz8CQB3KodevUm5BQUF+8OkPAZXYVvZVYcgrtIzlKadPv/5CpWqHCzeg/tFl/luWybCVTg9b2xBQcFOMC0moAhEOe/3MdIhsoo3l2FdufTj8XGMYKrYlgPUaW+Hs4q4qOn4PjEtIaBgWx0dAMCVSnVNyF509ojRWfB+PM/wA5/EmGlCwwSzog+dvbe+oKBgFKbFBHqYpuFKTANWZb/lleWtQqG9BecFhQkUFJxzTIcJOPNfnm04KAM75jVhvjNw4M7j4rvMYesqBE+TqbCsPjRNaEvrJccH/G6aLqEL039TCwoKdorpMAHY9GLrwvte261PuxyjCteFpl0fwg4aNifCBoa4CxcGME1EM5/clzNX/bdbZQHow3SEABOIAc4kgcw2nMOgyjq/VRo2a3oWap1DMxvmgmEqSsRVAqCv8+cBQxJn0XR6GrB6vYBhi4+svM/gMwsKCs4kpsMEMuwzTqAPZ9mRCNCXH2u2MI0qGA7PYjeh9B5eIVgUgwUFBYMxLSYgBqUxkuykYcAnqhfQdAG53mLoMuQazjobOkmscnwbYg7cFNMSAoiKQU+NpJ9A2LfFAKLTjCGdX79ue1Ot0+APMWXUxK3AuSa8+1VvnIBMKtI3aJYFSQsKCnoxGSbADIDdVsBLQiPysEkGIKUmgMRzsAlef77Q/rDhdTEVE6HGAopPwMljFTsbs1ZGo7CDTdOMFSZQUHDOMRkmAITwAQC6dMtNKFqGVgMaNfY1zGfSFJYzgDF6gL72yGMqTsoz8jRg02SvDSph8mu38Sql4lBMRwi4dzT3GFwVQNQH7SfQNN1D7OJjNeS77hhTTHNesB5yr1fTpwxkGm05K0lFCgoKejEdJpChUTyfosSMsiuYC4U0bbKl37VAom3avvfpJzA2PqBvGtAVK1E8BneDhjm8i03YtyJBTo853I/wWhkmV5j3oDCBgoJzjmkxAaaWs5CH5hhhnSlSdmCYOhOMNGDUPSP2UN3AEPawbYVZHwPQTFA5AxhqKhz6fAXbQxzRM8X3iLm/QZs5D8VGvzYR/SARfZqIPkVE7yKiQyK6nYgeIKLPEtF73BJlBQUFE8XaQoCIbgbwLwDcycx/DUAN4A0A3g7gp5j5DgBfBfCmTSrYoN91UoOBbhmw5XHyb8RfOMctFin/0/LN4JF1V/DLjOdLjfv6muwPaD+79i+hPXt+f3ueKdaKNdHn6FOviKT1loJV8/5dLk0O2OnENUQ0A3ARwMMAXgW7TDkA3AvgO1YXY6cB5DwGmeUD2n8fOyAfuEFlPajEMfmwoRP7Ds/xP5aRdqKuzr1KGOQd0l6znc7RsEk6vb9P3tFl588794JN8vz5v2yPrnZYJfiKMIiQ7ZD8RmHwiQNVVyfVpgPWd6AapPAbik2WJv9TAD8Bu/LwwwC+BuBjAB5j5qU77QqAm7XriehuIrpMRJebx59atxoFBQUbYm3FIBHdAOD1AG4H8BiAXwPwbcqpKqdh5nsA3AMAF26/hWEIMG1nIY8xCo+oLLS3nqvl+SXPKZxXB2eN7jRjmoehVAwkQqkAACAASURBVKZtc9kyPUqwOyTYjzRAfL6c4ndCnhaqni5tNdRkeJoyLp8UvBm7j6YDwtQnFePhPY3KwDyZyJh4gk1+pW8B8Hlm/gozLwC8H8A3ArjeTQ8A4BYAX9rgHgUFBTvGJibCLwB4ORFdBPAMgFcDuAzgwwC+E8C7AdwF4ANDCmMGyMQowijl2hGDciWiPhdLjzh2tk2EhhkVpcxBMyNK56IhaxdIB6J8RF81QubnGzdPt8e6IwW93kM+y/AZeveza/eSzKeL9Zz3uAI9qnM1NMcgv29pqg7noPUZ9NpCgJkfIKL3AfhdAEsAH4el9/8VwLuJ6F+7fe8YXigG5R2XyUWiYJAeg06ADMxTKKcGtoz2ddpUQdrTNQ/ErqnBUOWZVADqx5WpQdb5+zwGh9J7bUFX+bx92ZfP29Sg77cdPDVzkNm0TeYPY+/Vpv55Mh65rwsbOQsx89sAvC3b/TkAL9uk3IKCgv1hWh6DMiFIxggaVL3KDkmh8vMaRQnosYr+xvt302WpLBzDCLow1gNw7Agjy4z19hDprlx1ExaUKQk1D0PteeUIeRZZQS8DGJzLMWO1whSYvN9qHEHXcnqr37mz92sUFBSMwmSYQNf0VPOJlpIyj8E2XIURzJth5j6aEG2pJ+MJct2AhuR8ZdSP9e4eIYdirFeiGTjiSAyJl5AsSOoH/HVjIzLPg56g67eO5lx3HlM7XZhQgOcmRG3ET+b/a5imJyME8vdXC5NcN7uwD9u0Joi0kbQStc4kBUObfkea7LFO5xiLdacBg8t1m16huGIq1HufMy4MciGeemamiUTyz62yEkVfd7ah9jllOlBQULACk2ECAFy24f0nshiiLOxjB/b6dBTcZITsSwyi+vQL34Au02ADHXV+PlGrluHZk2bpnwohHD3biUnGrgKd/w4NCAuuXVmpH4wWHGQSBWG/aTDWsaQXKygo6MF0mICLItw1WuYaZeTrm2vXiWPGakXittBiBx3egWNXF/IjUy32JeZCIFGcth2q2mbDtN7djkRnEUOVv5quK66sFR2DcnZgz2srC3MFetEJFBQUDMZ0mMAA1CvWVFsHzUCLgYaxDGC89lxxGsqiBFeWMbBu0oKSj+iJibCH/QxLz3ayi7dODVpsjGcEC66DvkBGB+buwloU4Rjd2qSEgFZvmV1liIlQmllMtk3KlddMJLtuX4yANkXR4gSGdHp5l74WNdk5XTkau4TbWc1XuEnilPhORhOh5ikIWAGR0/xGKAuHmgpX+Q6cvV+ooKBgFKbDBDq0gtr6A0NyqnWh7jwSoXkWroMuhqF7Ew6MLBybLCSUL+sFVy9ttEfw3MrrPTjOQlw/JJbitIQcb76smMbmhNdrHioPCqzATwuWJr7BmgMRZ1OFIZh+yxcUFOwU02ECA2AyV8sktnqLyiZNMvaNgkNHSIm+kX+smU8vXyu357t7hF2NCrt2od41ulhAl0nQJ34FchOuPb4Q8/4uXZdNKNqe9+dmv75Rf0hC0ukIgTU9BdsrFW/3JdM6+FirwFDFY1e4sOYTYI/r1+llD61tTxkrFm9p33P4is9nPY4AgMsG7T6rsQOpws8GyKU5BheKslB+VgPuisdgQUFBH6bDBNZAl7QL+wcOWtpo1TXiSRYwdFQca3qUDACwysBcqaTS/VF3WQ0ZTwCsHjHGx0hsLzPzSUDPIdgX26EzAM1T0O6nxDTYWY+e8GKDYiIsKChYgUkxARKZrYauM9CnA/DSdj7QnOZLWkfRt6nCa3gKqvY1jXJ8XR2ALF+LMAy/kEhBtk57nTUMXbGpyRR9jVAMJolx4NOLpecbrrA06XmAMA3mSsNdZhveF4YutyQbsMndXkNyjHbAjEbVxyrA7P3bVHjdaYBWn2HXx2cdbdFWfAfa50gXayEM3K4hGZfOAvL1HzVIoS6tA7mFS0uZLzMMt7MOyWlEd5BQl1DQcDZ/pYKCgsGYBhNg+a97PMmwyiQH20jTYh8DSO+nJxoZuljJUMglxPJ7yyAhzRzYtI6Jz6PrFEf2mtJy09NSZeEQD0yJVfEEZ8lUOJTBaTEDHtryYlq4cB8rKCbCgoKCXkyDCQiEZcgyabc0NRZVmoZpZQyBu3a+IlvJVCRh18ihJQtpMEwJOJgphdM40Q/k9fAISsOedQrs/deL0JwKI9jGUuvyd+pjaDJmwJ7fnUhEgkUocTh/BENe2cJE9ItE9AgRfUrsu5GIPkREn3XbG9x+IqKfJaKHiOiTRPTSwTUpKCg4EQwRs78M4DXZvrcAuJ+Z7wBwv/sO2KXJ73D/dwP4ucE1YbsgqdcN+EQJ3tnBz4XkvrFYR6bXoJVz/m2n/jbMwUEoL7tB2ynIiP8GFNZjbPL2c/8LVFisWNEJcCNXx6M1zIlvvHdtlvVO9BzufOs6a5JjdsFVVp1vtjESj0HDJvlfhcr9xeu5xdok/DtsowdtBKERKw3Zc+z3BddYuv/we3L7fAlmShKKbMVEyMy/TUS3ZbtfD+CV7vO9AD4C4M1u/68wMwP4KBFdT0Q3MfPDK2vicgzmpg2NzspFSIesSpxcG+4nw10ttkE8tdWLVTPkiCAheW6k4cp5IuuM3NdZrjhWKzEJIbxYCTQKSOID2j4EjatpIkj9CywyRclVju13OaWYxtRgqIdg9/XC/Cf9BJSYAf99iKdgl7IQ0KcKOdZt1Rf4ju22z3f7bwbwRXHeFbevBSK6m4guE9Hl5omn1qxGQUHBpti2YlATOeqQx8z3wC5ljgu33dI5LMqViNqRUiIbq2AH+XLOQVYLxxZRkdainFIyambAPL+fXOhUOszINQg6ny+LE0iP9aNPIahlpI311Wh3O94iZxu1ogQE0DIbthhBfr57MuOduFAFVjCEEQD7ZwVjWYCBiTEDYdqk+Pgn4fBpxKBcd0CmFOvKLCwRc0G3HY5yrNuSXyaimwDAbR9x+68AuFWcdwuAL615j4KCgj1gXSZwH4C7APy4235A7P8+Ino3gG8A8LVB+gAgdRhCKvn8d59iaeFSLC2qmI1VXc7ZjUzH7vsBmf6RzEM1Z7VHgv58/ACQjm5IjvSvJJQjH5UXimNQI0aJBt2jRLKvxYxap4s6UEj8GrIOAzCuDD/iHeSMACkb8nWMEZlG1RMg3MF/0lmBx6bsYBMlZB/ba8I5kaVKVhAzCqesViJxDFIyCo9xE86xUggQ0btglYDPJaIrAN4G2/nfS0RvAvAFAN/lTv8NAK8F8BCApwF8z+CadHgM+u2SKyy9EBBbT3UWbB9lgRrHTn02Z6eUIknHPG2HK1+rS38nbCnCso9AOkXoI/ZjU4gvRGyA7Px2H7U6v0ZBEyXgyLBrf6kvt6aY8MSv/nzcEgYdhSQCOV3tOCARCrpA8NiXAnGoMlCd4uXTVzGlDcJAZM7Kp8C2jHRfY+J5eQCeGaAYHGIdeGPHoVcr5zKA711VZkFBwXQwDY9BxwCqBmCjKz0apjANMFWk/nExR7tvwbPWdCCMhly1RucKaI2CK7MNK8k2WlKf8wVLdQxJG9YgZQCAneLk1B+Iues8BscQaNOkVl2ppVRME56INoVVpuXtWAO9kYgt1sSVurDrNpY30+h/3zJiY8yB6XV+q7CXJL1YaiJcmlpRAlatviFNiY1Jp9GSJXRhKh6zBQUFJ4RpMAEAYXVvJzZjzICTbFUFw96clppS7Of+VVgAZOavOBpFU5VAh/BMzIct01h6fOyCpTkDWITvwCKb/y9kHn8x/8+dUfruU2nLuikDYTJatTLdcmQHIeYg6jlCohbZ9t6JypdBwLwj/sBIpaFHh5ORXMdgHWyDBfSZfFPzdds0GMrgdntLPUCuBNSWIWtM7D+7MhEWFBScEUyKCZCwDjSCAQCpdOxbjci6YaajYUPK3FM1SUmtud22pKRqPRRSX8x3pQkRSBmBtqy416r7Gnk9gDb/T0YLccwIpykJORrUfrTiOlgKAgshE/b1sYqERbh6Lsj/VvHZNJNiHp0IZlS5wUUwgjxTUVcq810sg76KAfQ7gvlz3HeOjjuybbvWFDSQzkJxZPfvgmcEjNhfwj5xbGPrwH5AICbbH/10oOX1p5mEqt7PXjE477lzQm/Vtmor/LTztOQaLS9C5YWRU4C886dpprziU9LITNgJ77D2egztqVMlGzw8X9XyP1ChCNFoqvRCT18vwQsZn/vREIIg9T4H+fTA1xxI4zK0qUE8m8Wx7mcZ6w2odXw1Q3Qm1DvLC8Ktyr5TS0AYpjBF9tvGVKHT+2kAJ1OFMh0oKCjowUSYAKJisCMaKvWZFlFWwrHCbtuRhcGxRZX4hMrTX44OMPm1SUWDAsyXIQ9HZWGIwlPiD5pwz2gGbDIGsBAjfO78YxRGYJ1LuhNTxPMR6pOygrZnYjfilCIfSRtXVs3t8GCNHRzAhBE/jvDufBZJX4UZMY/LkIufhntJ5engZK3rMYCxkKN+eJ+9Y5zIJpwzYRkqLJWGGgPw38u6AwUFBb2YBhNwLsNk4iibOz1IhwijjHwaPDuYu/GtkaN+73w3fqwz5xjpeuyVh3OlqEak584ZgX0Gv89ioZgBpRLwOJvrH0MuUd1WBno36rTe7plC/Ss0ria1wpb6XI4lg2hFGwrdgDdzBkUixzJiWSTqweFawLbdgWAFAACqoOkyWsgiE1dhKAtQ1xPI3L8bpR2l4jaWK82BqU5Amv6CHkCsOyD1AJrZ0B5bnYx3GkIACNYBMulDS2Egw4r9vj5EmiWUWNmPUIOFJjq+lPkUIUF2ftOhLNQ8C235aCmNFqCE/svnOxbCznd+dTqwYl885upP0TcyChLNi06Wa8S1djqQY+HqWIND569FO3rBEK0QTWvK5u8zF1FlQuypQUhdnX0MZR+V7GXFFCMXjpqfgIGwZmW/+yJZaKQ9GIa+ITq6CftcP6oqLE1/TugyHSgoOOeYEBNITYResi29HzXXWLqowEQxOGCFIhk70A5VxSC/eY8K3ErAUSk0X6uVNAfKaYDdVi0mIJWAmjkQobz2qJIvbaWCc1aQ1lybDjRIIzmBtsI1GBkF85L1aCkhxeUmm641iPkIw7SLWbS9NH22/Qnssw3zF+ij/DZeoTv8u79c8Tsqfi15iLyPll2aOkbOumPHTY1l44430VToR/4WE6irVjxJjsIECgrOOSbDBMgA1ADUOAnmpF1T2/FiaapEOQI4xaDiQdcHP5LG0YiEoszPM7k1Cmr6glAm0Pa5l5crawZIb0DAMoFc+RfPp6Do0xjAQsyWcwbQx5QaVJEBBDakKL2S/PeajiHVE4Sxhdvl1WD1N2g583gvO6LQRsFZiETcgTAprrsW4iqln9/fWh5ezTsRdT159KA0Xx+HZDgVjox1Z3vaHAAAnmns92NT49j1g4XYegbg9zVNZAJ5FO5yWeNq0+cuNxUhwFYAVAuAlnaXf9Aj96B1NcOsMm6fFQzH9UwkGLGPIqlzH+SLmCultHx88pxgd02mEVGbDfQoC2FfDm8J8IFAWkipx4JnKs1X93W4Dctnjs8W6XrokFpWGyFQVOEYFKXdAqRWhK5at1wIcdUKUHIOJa7eiMdaj9yRqKQDWkeP9VulBMzcr5lbfh8N2tO6IzPHVeM7vX2Hj5pZ+B6yaPkBsKlC518u3e/eVGEa4KvphcESwHHT383LdKCg4JxjOkxgSagXDFq6EdJJu1ljZeuiMjgiW92DytKFhamDpMw9B4fCCGWhPkJlw4tqCpT7U0agwS7qkdlzFRuynAJodn+N+rcViMIPPatVxQYHlBqyajJt5VXwYGy/Lnb0pOx8254HcnfwshTnh9HflmTr6VNsOQZGBnXmRQiOysfwm5FQBCoehuGZEy/ClHX4BVTyfUOgJYLRJha+bf104CrPcNRiAFEJeLR0+5ZxOtA0kQEAdjrgR35202kfhNEAuFqYQEFBQR8mwQQIQNUA1YJROUnmpZ2XgHVlUDudgJ87LYV5JXckGooGFEak48wJSELqC7S8/UNgxLYdGly12IwcgVujLVct5Z9MvBq9KyVLyB2lRASgVOaFe6RlHCtMoIbB3ClyjAslnjt2YZxhDYhmutTJKXortuI9gsKyjqHP3qhIba/DvpWQbOITSsqoQa1RPk2VlmJshIBB2+lrwXVMiOu3Juq1PCPw7/fC1FEZLpSATZOaA7khsOsvCEzAbpiqwJa7UJhAQcE5xySYgLcO1EeMauEkttN8NnV0H87jCaSE00xnfUYCVRsfNNn9sXR5urLcZLgK0n3Ub49RR9fgMC+O3/tiAeR1OTuIKdlr1TqgMYCop4jXyvv4awHvGmy1254BHDhmcEgL4TassCvv288mfPYjexXSh5sWS6jBmAd2IKwEmZu2jD/If6JVUYX5yC/NiDEKs424xkBkUtKKE9lddLo6Nm1dAAAcLWdYNPH9B7w50OsEvB6gApapLiA8AEX9WhcmJAQI1DB8//MP6s0gx1WNmfAZANIACxMo8axli/VoNG/BBN6+3D+lyKcLpsPslZaKaDLiGCcgO5iv7wKp2fOqmbeeJS2/Tfnzjqx5HVagxPOv67yogGy3SwUOHdgLAb+9SvMwdYqUvmqZEmsyYdrgFZVeeBxQE6Ybh8JMu8iVuczh/V+ILEb2nNg3Yr11aAo+vz9f9m2BdpakkEUIsb0WQqhHZa+YwmVBQo2Y2sppAGCVgRw6v7t5Q63O7/1tuFo9RS7TgYKCc45pMAFYK13VcFRo+DRjwgOqySSlVAwm9Mp9PiTPCGxZtTKSyUg3jRp7SProGcWBryQBlZaQpDVyRAehq5mC6JhrlQEAwFWeB6Wc9Gps1ZH704vlo76MAJSjfj6CyaWyNfi65Eyg7mBeeZZj+Rv4aw9p4b4vcVi5uEP3SHNqwnQhPp9pKQtDC8ipQji/f8n4VmxHBztchHcrdSqTn8OUT4z6abqwurUPQLJmQPAIRHQESkb/EK6RH+PQb7qwkgkQ0S8S0SNE9Cmx798R0R8Q0SeJ6D8T0fXi2FuJ6CEi+gwRfeuq8gsKCk4WQ5jALwP49wB+Rez7EIC3MvOSiN4O4K0A3kxELwbwBgB/FcALAfxPIvpLzNyraaPgNmyC12hwfhBRUT6FUnCbNFUwq0j3YTm6AtFpJVFshTlqO6FGGj/fHgF8khKfTuMQS5h8tSOwUAilLsILMXeXeoDckSRPQJnuS+fZHl0JRuV+beFLI5RXUU+R6xWkMrLdLt6ZK9cDSEgGExSDQq9wwY36C/JsbhHPDwlKm9bwZUBBWRhdj+35C44Do7wsX//APivCNUA70UsX8vfEZgpOnaykTiAyQbHKUGZSlKnEEndgzkZ7JsEAsiYw21mL8LeJ6LZs3/8QXz8K4Dvd59cDeDczHwH4PBE9BOBlAP73qvuQAWhhULlfgQX9Aayi0CsLvR/1sZkFzaqnztYWm9Jqj1qk0w6ZcWDEYqZtaMk5gpdddQTAvpQ+RENqhLvShGvegbJOOWqxmrJJ6GZ6vhY3IUOPTaaMTJ5THMuFwCKZNnRPR8L0R5lOeYFQUVysRCtr4X7PuoqegEHxGaZTNWpvPYCYkgXLQmq9kXkh+3S+puNz3Nd9sTZYHGf1Ppbvphi8QghxlkinMSQCg5xPgKE4HZDt7ZvLHSMxreY9KAb/MYD/5j7fDOCL4tgVt68FIrqbiC4T0eXl009toRoFBQXrYCPFIBH9KGyg0jv9LuU01XbGzPcAuAcArrnpViYDVEsTTISe8gRGwG1JuWjaUtQqBtMpQhIuS7lCqUbuK59MB5TRc+HNWN7jrYoMo0thZ+/VpuZ9iHSfYjZelZpLj8HV5j1tX4hWU5jAMigI4/leAdrnIyEVgK0FRwZCm+oseNb2GJSfw4wssoRg6hMKQl+ynKsGE2FQjnb/Vnqkpva7RCWw/3zktjZxSPZeK+sJhFqLUR9h1G+bCCUzMN0/EYANhAAR3QXgdQBe7ZYkB+zIf6s47RYAX1r3HgUFBbvHWkKAiF4D4M0A/hYzPy0O3QfgV4noJ2EVg3cA+D8rC2QABqBlVAx6H+iQJKGpYGo3MsrMq9lILUcyr2AL3mcdKaKGxOBLZx0/Ovl56QE3OPZmMW6PTLmpqGthynUhy/VzTanok9+TfcKkKJlUntJKLoaZr80wIxO9AnMWxFXLHFgRB4Yzrzyj4l6zomd2nnnl5kEPzYkLQFQYInoY1sTItdWS6SyU32VI4hrtN5ZegnkqsSMzCx6C3mPQOwgZplayEDbUig9AEwlu3HoGzSt1AiuFABG9C8ArATyXiK4AeBusNeACgA+R1Yp/lJn/KTN/mojeC+D3YacJ37vKMhDu44XA0j9Y6hVlZhQUg43wHfAvr7QSSA8tu893jHbYq1E071pgjfTK84IkhoPOw4t5IF6t3EMxp4cSjZZ/Tmak1RSJoTN7GjlD7jehvbiy4/vzZMcPgjULPJELwPhpwII46cyA8BuomuCK7deDrJhbUwOt4/vf6cjMhebqwFUEYV8QDIjCyMuCRnzPk8PoWYG6rR96vsXuzmW4Qh4s1HAVFNghfNhEt+Eme7+TEGFp/5fTAHhvWyT7pJVglZ/AEOvAG5Xd7+g5/8cA/NiqcgsKCqaBaXgMsjcRNqiWVkJTxgS4IRg3hDRCcdKKHTAyECelkSAt26sizanqPC5HLTlVmPvpgju9Rkzd5eHXDJAJQeS0IE/eEac1M+RmvTT0ONL1LvNeuopxNLFGWioz3PZ7CErYZcVSU59nBjDxs5xShJyIrikbqsJ0IH9OmYRkHkyz8dqEtWf7DgIj4JZ6usvc106Q0lb0DUEyHVDMr0cipVjIFdhaT4CScGEAwJJC3/AJeMgoDED4Faz6HUvsQEHBOcckmADBegxi2UQToQ+NdKHEPDetTKoNU5jLSu+2VjIHRJ2AZwVaNt5aelj0oEI21xeeYHKfn5PmawXIJCFJ7IDCAPxz5GVIJrBQ9CHBnCpMirnCb8lVSGnlR//jZtbLBHIT6IwMlu45fSLYcF29DOGujcsIuqQ66hMqn+DDtLwNPWqY8EwXgsIIwUMzViz+frUIQ7aH9CjPvnm+5nk5JnWdXGNA/tZRF+B+9yYyr2OXSmwpIgaN7wcumhZNtHcGPUBDLQYQSBPFuIMuFCZQUHDOMQkmwHDT51kNM3M6gLkTZXMnzecGM7fvcO7iy2dLXJwdAwCeNb8KALiufgbX1c8AAC55t143ggxdbBOQa+1VyXdAOAn5yDc0wZVYvVe2MGYt4gp8HMKcaiycISVEQfLC1TVq8dMlrd0I40bROc/CaHlUzZLzbVIRl6C18iapWRyVQ+pxVta/EzEX2ag/I4NZ5Z/BH4vfZ2K09+XnsQUVcRJHINtWiys4pEWILPTbS9VRiDOI22WoR4ws9G7jFF29xbsQ1kwUKdIBYI72O2MUE2g4JsZXX5+L1RGure07eTRrWwf8vsAEhM6rCXP+6J4V8oZUceQPTkNuw1XIsdKJSQgBc5HxxEuv4k8uPgdP32E79W23fgUAcM3MNuC18yNc8h1+Zjv8DfOnccPMuhzfWD8JALi+fhrPruzxGI7abaXUVuvNP0skATDiHN8BZBhr7n0W7im9yoRnWq7oC+ckZsw47dGmGVpWIo9oqornB+EiklxomYRyyLDhmGUoTy4S6XudtJvv/MKjsNX5pbdh2rZzWrZCjg+pCf4AByEsGa4sPQuQh/Z25F1bhoXLY3kLyWPHbDv89ZUdlK6vn8bzZo8DAJ64cA0A4LHmIr62vAgA+KrbPr48BGAXIfELhzy9tObRo2YWfTqEQjE3L3qFIhHj659v/fUeVJ5Te4aCgoJzBuIBCyruvBJEXwHwFIBHT7ouAJ6LUg+JUo8Up7kef5GZn5fvnIQQAAAiuszMd5Z6lHqUeuy3HmU6UFBwzlGEQEHBOceUhMA9J10Bh1KPFKUeKc5cPSajEygoKDgZTIkJFBQUnACKECgoOOeYhBAgote4dQoeIqK37OmetxLRh4noQSL6NBF9v9t/IxF9iIg+67Y37Kk+NRF9nIg+6L7fTkQPuHq8h4gO9lCH64nofW5NiQeJ6BUn0R5E9IPuN/kUEb2LiA731R4d62yobUAWP+ve208S0Ut3XI/drPdh0w+d3D+sR+cfAXgRbOqY/wvgxXu4700AXuo+PwvAHwJ4MYB/C+Atbv9bALx9T+3wQwB+FcAH3ff3AniD+/zzAP7ZHupwL4B/4j4fALh+3+0Bm5368wCuEe3wj/bVHgC+GcBLAXxK7FPbAMBrYTNtE4CXA3hgx/X4OwBm7vPbRT1e7PrNBQC3u/5UD77Xrl+sAQ/7CgC/Kb6/FXZhk33X4wMA/jaAzwC4ye27CcBn9nDvWwDcD+BVAD7oXqpHxQ+etNGO6vBs1/ko27/X9kBMW38jbGzLBwF86z7bA8BtWedT2wDAfwTwRu28XdQjO/b3ALzTfU76DIDfBPCKofeZwnRg8FoFu4JbXOUlAB4A8AJmfhgA3Pb5e6jCTwP4EcT4k+cAeIyZfQTOPtrkRQC+AuCX3LTkF4joEvbcHsz8pwB+AsAXADwM4GsAPob9t4dEVxuc5Lu71nofGqYgBLRAx73ZLYnoWgC/DuAHmPnxfd1X3P91AB5h5o/J3cqpu26TGSz9/DlmfglsLMde9DMSbr79elha+0IAlwB8m3LqFGzbJ/LubrLeh4YpCIETW6uAiOawAuCdzPx+t/vLRHSTO34TgEd2XI1vAvDtRPTHAN4NOyX4aQDXE5EP9d5Hm1wBcIWZH3Df3wcrFPbdHt8C4PPM/BVmXgB4P4BvxP7bQ6KrDfb+7or1Pr6bHffftB5TEAK/A+AOp/09gF3Q9L5d35RsrvR3AHiQmX9SHLoPwF3u812wuoKdgZnfysy3MPNtsM/+W8z83QA+jLjG4z7q8WcAvkhEX+d2vRo2dfxe2wN2GvByIrroamfKPwAAAOxJREFUfiNfj722R4auNrgPwD90VoKXA/ianzbsAmK9j2/n9nofbyCiC0R0O4au9+GxSyXPCAXIa2G1838E4Ef3dM+/CUuZPgngE+7/tbDz8fsBfNZtb9xjO7wS0TrwIvdDPgTg1wBc2MP9/zqAy65N/guAG06iPQD8KwB/AOBTAP4TrNZ7L+0B4F2wuogF7Aj7pq42gKXh/8G9t78H4M4d1+Mh2Lm/f19/Xpz/o64enwHwbWPuVdyGCwrOOaYwHSgoKDhBFCFQUHDOUYRAQcE5RxECBQXnHEUIFBSccxQhUFBwzlGEQEHBOcf/B0uYGD9kpcspAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_3rd_layer_output = K.function([model.layers[0].input, K.learning_phase()],\n",
    "                                  [model.layers[10].output])\n",
    "# output in test mode = 0\n",
    "layer_output = get_3rd_layer_output([X_train[2].reshape(1,512,512,3), 0])[0]\n",
    "imshow(layer_output[0,:,:,0])\n",
    "print(layer_output[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa0d0edff28>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD4CAYAAAAjDTByAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAASUklEQVR4nO3dfYxc1XnH8e8zu95dr1/XBoLBBtsIgQCRQg0loYI0jqmhCCdtqhqV1oVUEW1poWoUjJCSqH81JU1fEhTkAC1pKUQlECxkGhzy1kjBARxj7JiATRxjbGyD39Zrr3dn5ukfc43Gy64958y9Fzvn95FWOztznz3P3tnf3Hk7c8zdEZH0VN7vBkTk/aHwiyRK4RdJlMIvkiiFXyRRnWUO1jV1vI8/fXJwXd0tuGZG177gGoBJlXpwTZ24V0xiX2eJqot8VadG+L4HqEbUHa6PixrLLPxvm1QZjhor9mi5L+Jv84h9+Pabh+nfPdxSYanhH3/6ZK5c9kfBdYeq4TvuztlPB9cA/M74weCawx73jzTs4Tc0AMMR8a9Fhr8/4oYXYFdtfHDNpqHTosbqslpwzUd7t0aN1WsdUXVPDcwIrqlF3NR87vfXtbyt7vaLJErhF0lUW+E3s4Vm9gsz22hmS/NqSkSKFx1+M+sA7gWuBS4AbjSzC/JqTESK1c6R/3Jgo7u/7u5DwKPAonzaEpGitRP+M4E3mn7emp13FDP7tJm9YGYvDO091MZwIpKndsI/2mtA73k9yd2Xufs8d5/XNTX85R8RKUY74d8KzGr6eSawrb12RKQs7YT/eeBcM5tjZl3AYmB5Pm2JSNGi3+Hn7lUzuw34DtABPOju63PrTEQK1dbbe919BbAip15EpER6h59Iokqd2FN3Y2C4K7jurb3hMwHXnT7r+BuN4vyutcE1m6sTo8Z6tv/CqLpntp0fXLP3QG/UWId3xNVN2Bw+AWb8rrjJR9WIFr+6aEfUWI9e8I2ouphJOrWYSVUBu1BHfpFEKfwiiVL4RRKl8IskSuEXSZTCL5IohV8kUQq/SKIUfpFEKfwiiVL4RRKl8IskqtSJPd2VKnMnvRNct3N/+MSZ/lpPcA3AK0N9wTW3Pn1L1FinvBh32ztxW/gKQRMPVqPG6uzfH1VX2XsguMYHD0eNZT3dwTVv1WdGjfXUWedF1V02/pdRdaF6ApYh05FfJFEKv0iiFH6RRLWzYs8sM/u+mW0ws/VmdnuejYlIsdp5wq8K/J27rzazScCLZrbS3X+eU28iUqDoI7+7b3f31dnpfmADo6zYIyInplwe85vZbOASYNUol727XNfg3sE8hhORHLQdfjObCHwLuMPd3/OicPNyXT1T4157F5H8tRV+MxtHI/gPu/vj+bQkImVo59l+Ax4ANrj7l/NrSUTK0M6R/0rgT4CPmtma7Ou6nPoSkYK1s1bfjxl9mW4ROQnoHX4iiSp1Vl+n1ZnWNRBc19sdPottnNWCawDu+dXC4Jo5346bMde9ZU9UnR0Kn/3mw+H7EIDDcTPt6kPh4/lw3H60jvBj2Kk/DV8CDuBLaxZE1a248t7gmrM7w5e2Gx9wX1xHfpFEKfwiiVL4RRKl8IskSuEXSZTCL5IohV8kUQq/SKIUfpFEKfwiiVL4RRKl8IskqtSJPR1WZ0rnoeC6CV1DwTVTOg8G1wC8M9AbXHPa/vD+AKwaN/kI9/CaekRNG3VeC//bvBo3+chr4cewji3bo8aa8sO45bqevPji4Jo7+l4NrgmZY68jv0iiFH6RRCn8IonK46O7O8zsZ2b2VB4NiUg58jjy305jtR4ROYm0+7n9M4HfA+7Ppx0RKUu7R/5/AT4L1HPoRURK1M6iHdcDO939xeNs9+5afQN74l4PF5H8tbtoxw1mthl4lMbiHf81cqPmtfom9IV/GqmIFKOdJbrvcveZ7j4bWAx8z91vyq0zESmUXucXSVQu7+139x8AP8jjd4lIOXTkF0lUqbP6KlZnYsdgcF1PZ/hsr+kdB4JrAKr1iNvDauQrnTGz8wDqEeP5r/GrsRF/W31/3P/H9LVxs0Xv+274Ml8LFv08uOZQwP+UjvwiiVL4RRKl8IskSuEXSZTCL5IohV8kUQq/SKIUfpFEKfwiiVL4RRKl8IskSuEXSZTCL5Kocmf14fRY+Ay9ro7wdd8mdYSvCQhQj5jVZxHr0jUKQ1ZWa1KJuM22uNt5j555GFEXO1aE2HUBOzdui6qb8X9zg2seufq3gmt2155teVsd+UUSpfCLJErhF0lUuyv2TDWzx8zsFTPbYGYfyqsxESlWu0/4/Svwv+7+STPrAnpz6ElEShAdfjObDFwF/BmAuw8BWpJH5CTRzt3+ucAu4N+zJbrvN7MJIzdqXq7rwJ64l1dEJH/thL8TuBT4mrtfAgwAS0du1Lxc18S+cW0MJyJ5aif8W4Gt7r4q+/kxGjcGInISaGetvreAN8zsvOys+UD4B42LyPui3Wf7/xp4OHum/3Xg5vZbEpEytBV+d18DzMupFxEpUakTewwYZ+GTYLoq1eCamAlEUOrckviJPbF1UUPFjVXmbowSe0UPx72aPXFL+DJfL7xzVnDNQLWr5W319l6RRCn8IolS+EUSpfCLJErhF0mUwi+SKIVfJFEKv0iiFH6RRCn8IolS+EUSpfCLJErhF0lUqbP6wOmwenBVxcJnYHWUOK/MOzri6jrj6mw4fJYjlfJmAkaPFztbMWaGXvSMyrjjZa03PGpTusKXnAvJl478IolS+EUSpfCLJKrd5br+1szWm9k6M3vEzHryakxEihUdfjM7E/gbYJ67XwR0AIvzakxEitXu3f5OYLyZddJYp29b+y2JSBna+dz+N4EvAVuA7cA+d39m5HZarkvkxNTO3f4+YBEwBzgDmGBmN43cTst1iZyY2rnb/zHgl+6+y92HgceBD+fTlogUrZ3wbwGuMLNea3y4+3xgQz5tiUjR2nnMv4rG4pyrgZez37Usp75EpGDtLtf1eeDzOfUiIiXSO/xEElXqrD7HGKyHP+NfrYffRtWIm7XV0xX+cmS9Z3zUWJWhyN1fDV/v0OrhsykhfsZizBp/HjljDiL+tsixrKc7qu7tC8P/R/7w1PXBNT/rbH0moI78IolS+EUSpfCLJErhF0mUwi+SKIVfJFEKv0iiFH6RRCn8IolS+EUSpfCLJErhF0lUqRN76lQ4WA+fGDFYC58MNBAxDsCk7qHgmlrvpKixKoNxu78SM7GnFl4DQFfcR6/FjGcxy24BxPxtkROWamdMj6rbe9nh4JoFva8G19xbGWx5Wx35RRKl8IskSuEXSdRxw29mD5rZTjNb13TeNDNbaWavZd/7im1TRPLWypH/P4CFI85bCjzr7ucCz2Y/i8hJ5Ljhd/cfAbtHnL0IeCg7/RDw8Zz7EpGCxT7m/4C7bwfIvp821oZHLde1O/xlNBEpRuFP+B21XNe0rqKHE5EWxYZ/h5nNAMi+78yvJREpQ2z4lwNLstNLgCfzaUdEytLKS32PAD8BzjOzrWb2KeAfgAVm9hqwIPtZRE4ix31zubvfOMZF83PuRURKpHf4iSSq3Fl9bhyshz/jf7gW3mZ/PW4Jrcndrc+KOmLP5Ljd2HEwri526a2osSqRx4eIukolbok1HwpfYs064/b9rg9Ojqq7dd53g2umRcw87AxYJk1HfpFEKfwiiVL4RRKl8IskSuEXSZTCL5IohV8kUQq/SKIUfpFEKfwiiVL4RRKl8IskqtSJPTUq9Nd6gusODocvGRUzDsCV0zYF1/z32XOjxsLieqyNC5+0dGh63O185G6kd0f45KOprw5EjdWxdVd4UeSEpb3nRZUxr/f1uMIC6cgvkiiFXyRRCr9IomKX67rHzF4xs7Vm9oSZTS22TRHJW+xyXSuBi9z9YuBV4K6c+xKRgkUt1+Xuz7h7NfvxOWBmAb2JSIHyeMx/C/D0WBc2L9d1cM/hHIYTkTy0FX4zuxuoAg+PtU3zcl29fd3tDCciOYp+k4+ZLQGuB+a7u+fXkoiUISr8ZrYQuBO42t0P5tuSiJQhdrmurwKTgJVmtsbM7iu4TxHJWexyXQ8U0IuIlEjv8BNJVKmz+qr1CruHJgTXHRgMf5Wgvx43He0Tk9cE1wzeHD7rEODHb58TVdfXHf40y8WT34waa8a4vVF1b1cnBdfc95OPRI111vKzgmu634l72bnvorej6gbq4f/DW6sHgmuGAp5615FfJFEKv0iiFH6RRCn8IolS+EUSpfCLJErhF0mUwi+SKIVfJFEKv0iiFH6RRCn8IolS+EUSVe5afW7sGw6fbXfoUFdwzcFa3OcF9lj4J5L9xbTno8aKreuvh/c44HFXdd0tqi7G2VfHzZi7q/IHwTUTNobPLgX489k/jKo7GDGrb9Pw9OCaw777+BtldOQXSZTCL5KoqOW6mi77jJm5mZ1STHsiUpTY5bows1nAAmBLzj2JSAmiluvK/DPwWUCf2S9yEop6zG9mNwBvuvtLLWz77nJdh/cOxgwnIgUIfv3HzHqBu4FrWtne3ZcBywD6zj9V9xJEThAxR/5zgDnAS2a2mcYKvavN7PQ8GxORYgUf+d39ZeC0Iz9nNwDz3D3uHRoi8r6IXa5LRE5ysct1NV8+O7duRKQ0eoefSKJKndgzoWOI35wS/p6grtm14Jo53TuDawB6LHwiy/TK+KixOiz2tncguGJvNW6s2GXPplYOBddc1vNG1FhPfewrwTUrrrgoaqybJq+PqttW6wiu6a+HT2jrtNazoiO/SKIUfpFEKfwiiVL4RRKl8IskSuEXSZTCL5IohV8kUQq/SKIUfpFEKfwiiVL4RRKl8IskytzL+1g9M9sF/GqMi08BToRPA1IfR1MfRzvR+zjb3U9t5ReUGv5jMbMX3H2e+lAf6qOcPnS3XyRRCr9Iok6k8C97vxvIqI+jqY+j/dr0ccI85heRcp1IR34RKZHCL5KoUsNvZgvN7BdmttHMlo5yebeZfTO7fJWZzS6gh1lm9n0z22Bm683s9lG2+YiZ7TOzNdnX5/Luo2mszWb2cjbOC6Ncbmb2b9k+WWtml+Y8/nlNf+caM9tvZneM2Kaw/WFmD5rZTjNb13TeNDNbaWavZd/7xqhdkm3zmpktKaCPe8zslWy/P2FmU8eoPeZ1mEMfXzCzN5v2/3Vj1B4zX+/h7qV8AR3AJmAu0AW8BFwwYpu/BO7LTi8GvllAHzOAS7PTk4BXR+njI8BTJe2XzcApx7j8OuBpwIArgFUFX0dv0XijSCn7A7gKuBRY13TePwJLs9NLgS+OUjcNeD373ped7su5j2uAzuz0F0fro5XrMIc+vgB8poXr7pj5GvlV5pH/cmCju7/u7kPAo8CiEdssAh7KTj8GzDeL+CD9Y3D37e6+OjvdD2wAzsxzjJwtAr7hDc8BU81sRkFjzQc2uftY78LMnbv/CNg94uzm/4OHgI+PUvq7wEp33+3ue4CVwMI8+3D3Z9y9mv34HI1FaQs1xv5oRSv5OkqZ4T8TaF6VYSvvDd2722Q7fR8wvaiGsocVlwCrRrn4Q2b2kpk9bWYXFtUD4MAzZvaimX16lMtb2W95WQw8MsZlZe0PgA+4+3Zo3FjTtDBskzL3C8AtNO6BjeZ412Eebssefjw4xsOg4P1RZvhHO4KPfJ2xlW1yYWYTgW8Bd7j7/hEXr6Zx1/eDwFeAbxfRQ+ZKd78UuBb4KzO7amSro9Tkvk/MrAu4AfifUS4uc3+0qsz/lbuBKvDwGJsc7zps19eAc4DfALYD/zRam6Ocd8z9UWb4twKzmn6eCWwbaxsz6wSmEHcX6JjMbByN4D/s7o+PvNzd97v7gez0CmCcmZ2Sdx/Z79+Wfd8JPEHj7luzVvZbHq4FVrv7jlF6LG1/ZHYceWiTfR9t7bVS9kv2ROL1wB979uB6pBauw7a4+w53r7l7Hfj6GL8/eH+UGf7ngXPNbE52lFkMLB+xzXLgyLO2nwS+N9YOj5U9h/AAsMHdvzzGNqcfea7BzC6nsZ/eybOP7HdPMLNJR07TeIJp3YjNlgN/mj3rfwWw78hd4pzdyBh3+cvaH02a/w+WAE+Oss13gGvMrC+7G3xNdl5uzGwhcCdwg7sfHGObVq7Ddvtofo7nE2P8/lbydbQ8nqEMeCbzOhrPrm8C7s7O+3saOxegh8bdzo3AT4G5BfTw2zTuDq0F1mRf1wG3Ardm29wGrKfxjOlzwIcL2h9zszFeysY7sk+aezHg3myfvQzMK6CPXhphntJ0Xin7g8YNznZgmMbR61M0nud5Fngt+z4t23YecH9T7S3Z/8pG4OYC+thI43H0kf+TI69EnQGsONZ1mHMf/5ld92tpBHrGyD7GytexvvT2XpFE6R1+IolS+EUSpfCLJErhF0mUwi+SKIVfJFEKv0ii/h+FpEOG7FrDbgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_layer_output = K.function([model.layers[0].input, K.learning_phase()],[model.layers[-3].output])\n",
    "layer_output = get_layer_output([X_train[0].reshape(1,512,512,3), 0])[0]\n",
    "plt.imshow(layer_output[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa0d0e40c88>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_layer_output = K.function([model.layers[0].input, K.learning_phase()],[model.layers[-3].output])\n",
    "layer_output = get_layer_output([X_test[0].reshape(1,512,512,3), 0])[0]\n",
    "plt.imshow(layer_output[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-34127f8dda37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_layer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_layer_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test_g\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "get_layer_output = K.function([model.layers[0].input],[model.layers[-3].output])\n",
    "layer_output = get_layer_output([X_test_g[1].reshape(1,512,512,3), 0])[1]\n",
    "plt.imshow(layer_output[0,:,:,0])\n",
    "print(layer_output[0,:,:,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dltdc/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 512, 512, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 518, 518, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 256, 256, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 256, 256, 64) 256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 256, 256, 64) 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 258, 258, 64) 0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 128, 128, 64) 0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 128, 128, 64) 4160        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 128, 128, 64) 256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 128, 128, 64) 0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 128, 128, 64) 36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 128, 128, 64) 256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 128, 128, 64) 0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 128, 128, 256 16640       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 128, 128, 256 16640       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 128, 128, 256 1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 128, 128, 256 1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 128, 128, 256 0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 128, 128, 256 0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 128, 128, 64) 16448       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 128, 128, 64) 256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 128, 128, 64) 0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 128, 128, 64) 36928       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 128, 128, 64) 256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 128, 128, 64) 0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 128, 128, 256 16640       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 128, 128, 256 1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 128, 128, 256 0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128, 128, 256 0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 128, 128, 64) 16448       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 128, 128, 64) 256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 128, 128, 64) 0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 128, 128, 64) 36928       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 128, 128, 64) 256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 128, 128, 64) 0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 128, 128, 256 16640       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 128, 128, 256 1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 128, 128, 256 0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 128, 128, 256 0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 64, 64, 128)  32896       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 64, 64, 128)  0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 64, 64, 128)  0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 64, 64, 512)  131584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 64, 64, 512)  2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 64, 64, 512)  0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 64, 64, 512)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 64, 64, 128)  65664       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 128)  0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 128)  0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 64, 64, 512)  0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 64, 64, 512)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 64, 64, 128)  65664       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 64, 64, 128)  0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 64, 64, 128)  0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 64, 64, 512)  0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 64, 64, 512)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 64, 64, 128)  65664       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 64, 64, 128)  0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 64, 64, 128)  0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 64, 64, 512)  0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 64, 64, 512)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 32, 32, 256)  131328      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 32, 32, 256)  0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 32, 32, 256)  0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 32, 32, 1024) 525312      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 32, 32, 1024) 4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 32, 32, 1024) 0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 32, 32, 1024) 0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 32, 32, 256)  0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 32, 32, 256)  0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 32, 32, 1024) 0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 32, 32, 1024) 0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 32, 32, 256)  0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 32, 32, 256)  0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 32, 32, 1024) 0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 32, 32, 1024) 0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 32, 32, 256)  0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 32, 32, 256)  0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 32, 32, 1024) 0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 32, 32, 1024) 0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 32, 32, 256)  0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 32, 32, 256)  0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 32, 32, 1024) 0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 32, 32, 1024) 0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 32, 32, 256)  0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 32, 32, 256)  0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 32, 32, 1024) 0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 32, 32, 1024) 0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 16, 16, 512)  524800      activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 16, 16, 512)  2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 16, 16, 512)  0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 16, 16, 512)  2359808     activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 16, 16, 512)  2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 16, 16, 512)  0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 16, 16, 2048) 1050624     activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 16, 16, 2048) 2099200     activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, 16, 16, 2048) 8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 16, 16, 2048) 8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 16, 16, 2048) 0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 16, 16, 2048) 0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 16, 16, 512)  1049088     activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 16, 16, 512)  2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 16, 16, 512)  0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 16, 16, 512)  2359808     activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 16, 16, 512)  2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16, 16, 512)  0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 16, 16, 2048) 1050624     activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, 16, 16, 2048) 8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 16, 16, 2048) 0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 16, 16, 2048) 0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 16, 16, 512)  1049088     activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 16, 16, 512)  2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 16, 16, 512)  0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 16, 16, 512)  2359808     activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 16, 16, 512)  2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 16, 16, 512)  0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, 16, 16, 2048) 1050624     activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, 16, 16, 2048) 8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 16, 16, 2048) 0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 16, 16, 2048) 0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 1)            2049        avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 23,589,761\n",
      "Trainable params: 23,536,641\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pic = X_train[1]\n",
    "model = Sequential()\n",
    "#model.add(Input(shape=pic.shape))\n",
    "# model.add(Conv2D(filters=4, kernel_size=(7,7), strides=1, input_shape=pic.shape))\n",
    "Kerasmodel = keras.applications.resnet50.ResNet50(include_top=True\n",
    "                , weights=None#'imagenet'\n",
    "                # , input_tensor=inputs\n",
    "                , input_shape=pic.shape\n",
    "                )\n",
    "# Kerasmodel._layers.pop()\n",
    "# Kerasmodel.layers.pop()\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(1,kernel_initializer='he_normal'))\n",
    "model.add(Activation('linear')) #softmax\n",
    "model = Model( inputs=Kerasmodel.input , outputs=model(Kerasmodel.layers[-2].output))\n",
    "model.compile(loss='mean_squared_error', #mse binary_crossentropy Dice-coefficient loss function vs cross-entropy\n",
    "                optimizer=optimizers.Adam(lr=5e-3),\n",
    "                metrics=['mse'])\n",
    "\n",
    "model.summary()     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## resnet 50 color without pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1053 samples, validate on 264 samples\n",
      "Epoch 1/400\n",
      "1053/1053 [==============================] - 189s 180ms/step - loss: 0.5812 - mean_squared_error: 0.5812 - val_loss: 0.1542 - val_mean_squared_error: 0.1542\n",
      "Epoch 2/400\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0952 - val_mean_squared_error: 0.0952\n",
      "Epoch 3/400\n",
      "1053/1053 [==============================] - 104s 99ms/step - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0143 - val_mean_squared_error: 0.0143\n",
      "Epoch 4/400\n",
      "1053/1053 [==============================] - 99s 94ms/step - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0073 - val_mean_squared_error: 0.0073\n",
      "Epoch 5/400\n",
      "1053/1053 [==============================] - 119s 113ms/step - loss: 0.0119 - mean_squared_error: 0.0119 - val_loss: 0.0078 - val_mean_squared_error: 0.0078\n",
      "Epoch 6/400\n",
      "1053/1053 [==============================] - 128s 122ms/step - loss: 0.0102 - mean_squared_error: 0.0102 - val_loss: 0.0045 - val_mean_squared_error: 0.0045\n",
      "Epoch 7/400\n",
      "1053/1053 [==============================] - 121s 115ms/step - loss: 0.0126 - mean_squared_error: 0.0126 - val_loss: 0.0159 - val_mean_squared_error: 0.0159\n",
      "Epoch 8/400\n",
      "1053/1053 [==============================] - 123s 117ms/step - loss: 0.0123 - mean_squared_error: 0.0123 - val_loss: 0.0277 - val_mean_squared_error: 0.0277\n",
      "Epoch 9/400\n",
      "1053/1053 [==============================] - 109s 103ms/step - loss: 0.0109 - mean_squared_error: 0.0109 - val_loss: 0.0093 - val_mean_squared_error: 0.0093\n",
      "Epoch 10/400\n",
      "1053/1053 [==============================] - 106s 100ms/step - loss: 0.0098 - mean_squared_error: 0.0098 - val_loss: 0.0143 - val_mean_squared_error: 0.0143\n",
      "Epoch 11/400\n",
      "1053/1053 [==============================] - 104s 99ms/step - loss: 0.0062 - mean_squared_error: 0.0062 - val_loss: 0.0365 - val_mean_squared_error: 0.0365\n",
      "Epoch 12/400\n",
      "1053/1053 [==============================] - 104s 99ms/step - loss: 0.0102 - mean_squared_error: 0.0102 - val_loss: 0.1209 - val_mean_squared_error: 0.1209\n",
      "Epoch 13/400\n",
      "1053/1053 [==============================] - 105s 100ms/step - loss: 0.0103 - mean_squared_error: 0.0103 - val_loss: 0.0250 - val_mean_squared_error: 0.0250\n",
      "Epoch 14/400\n",
      "1053/1053 [==============================] - 98s 93ms/step - loss: 0.0088 - mean_squared_error: 0.0088 - val_loss: 0.0176 - val_mean_squared_error: 0.0176\n",
      "Epoch 15/400\n",
      "1053/1053 [==============================] - 92s 88ms/step - loss: 0.0068 - mean_squared_error: 0.0068 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 16/400\n",
      "1053/1053 [==============================] - 91s 87ms/step - loss: 0.0055 - mean_squared_error: 0.0055 - val_loss: 0.0093 - val_mean_squared_error: 0.0093\n",
      "Epoch 17/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.0227 - val_mean_squared_error: 0.0227\n",
      "Epoch 18/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0049 - mean_squared_error: 0.0049 - val_loss: 0.0057 - val_mean_squared_error: 0.0057\n",
      "Epoch 19/400\n",
      "1053/1053 [==============================] - 95s 90ms/step - loss: 0.0065 - mean_squared_error: 0.0065 - val_loss: 0.0316 - val_mean_squared_error: 0.0316\n",
      "Epoch 20/400\n",
      "1053/1053 [==============================] - 105s 100ms/step - loss: 0.0069 - mean_squared_error: 0.0069 - val_loss: 0.0139 - val_mean_squared_error: 0.0139\n",
      "Epoch 21/400\n",
      "1053/1053 [==============================] - 108s 102ms/step - loss: 0.0073 - mean_squared_error: 0.0073 - val_loss: 0.0232 - val_mean_squared_error: 0.0232\n",
      "Epoch 22/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0066 - mean_squared_error: 0.0066 - val_loss: 0.0080 - val_mean_squared_error: 0.0080\n",
      "Epoch 23/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 0.0087 - mean_squared_error: 0.0087 - val_loss: 0.0127 - val_mean_squared_error: 0.0127\n",
      "Epoch 24/400\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 0.0079 - mean_squared_error: 0.0079 - val_loss: 0.1110 - val_mean_squared_error: 0.1110\n",
      "Epoch 25/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0477 - mean_squared_error: 0.0477 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 26/400\n",
      "1053/1053 [==============================] - 94s 89ms/step - loss: 0.0366 - mean_squared_error: 0.0366 - val_loss: 0.0183 - val_mean_squared_error: 0.0183\n",
      "Epoch 27/400\n",
      "1053/1053 [==============================] - 88s 84ms/step - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 28/400\n",
      "1053/1053 [==============================] - 98s 93ms/step - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0255 - val_mean_squared_error: 0.0255\n",
      "Epoch 29/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0171 - mean_squared_error: 0.0171 - val_loss: 0.1954 - val_mean_squared_error: 0.1954\n",
      "Epoch 30/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0170 - mean_squared_error: 0.0170 - val_loss: 0.0557 - val_mean_squared_error: 0.0557\n",
      "Epoch 31/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0130 - mean_squared_error: 0.0130 - val_loss: 0.0127 - val_mean_squared_error: 0.0127\n",
      "Epoch 32/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0142 - mean_squared_error: 0.0142 - val_loss: 0.0160 - val_mean_squared_error: 0.0160\n",
      "Epoch 33/400\n",
      "1053/1053 [==============================] - 99s 94ms/step - loss: 0.0127 - mean_squared_error: 0.0127 - val_loss: 0.0656 - val_mean_squared_error: 0.0656\n",
      "Epoch 34/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0121 - mean_squared_error: 0.0121 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 35/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 0.0103 - mean_squared_error: 0.0103 - val_loss: 0.0096 - val_mean_squared_error: 0.0096\n",
      "Epoch 36/400\n",
      "1053/1053 [==============================] - 102s 97ms/step - loss: 0.0109 - mean_squared_error: 0.0109 - val_loss: 0.0081 - val_mean_squared_error: 0.0081\n",
      "Epoch 37/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0108 - mean_squared_error: 0.0108 - val_loss: 0.0080 - val_mean_squared_error: 0.0080\n",
      "Epoch 38/400\n",
      "1053/1053 [==============================] - 91s 87ms/step - loss: 0.0070 - mean_squared_error: 0.0070 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 39/400\n",
      "1053/1053 [==============================] - 104s 98ms/step - loss: 0.0084 - mean_squared_error: 0.0084 - val_loss: 0.0226 - val_mean_squared_error: 0.0226\n",
      "Epoch 40/400\n",
      "1053/1053 [==============================] - 102s 97ms/step - loss: 0.0064 - mean_squared_error: 0.0064 - val_loss: 0.0241 - val_mean_squared_error: 0.0241\n",
      "Epoch 41/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0068 - mean_squared_error: 0.0068 - val_loss: 0.0124 - val_mean_squared_error: 0.0124\n",
      "Epoch 42/400\n",
      "1053/1053 [==============================] - 87s 83ms/step - loss: 0.0052 - mean_squared_error: 0.0052 - val_loss: 0.0089 - val_mean_squared_error: 0.0089\n",
      "Epoch 43/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0058 - mean_squared_error: 0.0058 - val_loss: 0.0062 - val_mean_squared_error: 0.0062\n",
      "Epoch 44/400\n",
      "1053/1053 [==============================] - 103s 97ms/step - loss: 0.0060 - mean_squared_error: 0.0060 - val_loss: 0.0167 - val_mean_squared_error: 0.0167\n",
      "Epoch 45/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 0.0056 - mean_squared_error: 0.0056 - val_loss: 0.0080 - val_mean_squared_error: 0.0080\n",
      "Epoch 46/400\n",
      "1053/1053 [==============================] - 101s 96ms/step - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0067 - val_mean_squared_error: 0.0067\n",
      "Epoch 47/400\n",
      "1053/1053 [==============================] - 107s 102ms/step - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 48/400\n",
      "1053/1053 [==============================] - 108s 102ms/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 49/400\n",
      "1053/1053 [==============================] - 107s 101ms/step - loss: 0.0047 - mean_squared_error: 0.0047 - val_loss: 0.0094 - val_mean_squared_error: 0.0094\n",
      "Epoch 50/400\n",
      "1053/1053 [==============================] - 108s 103ms/step - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0997 - val_mean_squared_error: 0.0997\n",
      "Epoch 51/400\n",
      "1053/1053 [==============================] - 107s 102ms/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "Epoch 52/400\n",
      "1053/1053 [==============================] - 108s 103ms/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0315 - val_mean_squared_error: 0.0315\n",
      "Epoch 53/400\n",
      "1053/1053 [==============================] - 106s 100ms/step - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0167 - val_mean_squared_error: 0.0167\n",
      "Epoch 54/400\n",
      "1053/1053 [==============================] - 109s 104ms/step - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0078 - val_mean_squared_error: 0.0078\n",
      "Epoch 55/400\n",
      "1053/1053 [==============================] - 108s 103ms/step - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0141 - val_mean_squared_error: 0.0141\n",
      "Epoch 56/400\n",
      "1053/1053 [==============================] - 106s 101ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0055 - val_mean_squared_error: 0.0055\n",
      "Epoch 57/400\n",
      "1053/1053 [==============================] - 110s 105ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0049 - val_mean_squared_error: 0.0049\n",
      "Epoch 58/400\n",
      "1053/1053 [==============================] - 110s 104ms/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0173 - val_mean_squared_error: 0.0173\n",
      "Epoch 59/400\n",
      "1053/1053 [==============================] - 110s 104ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 60/400\n",
      "1053/1053 [==============================] - 109s 104ms/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0111 - val_mean_squared_error: 0.0111\n",
      "Epoch 61/400\n",
      "1053/1053 [==============================] - 107s 101ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0080 - val_mean_squared_error: 0.0080\n",
      "Epoch 62/400\n",
      "1053/1053 [==============================] - 109s 103ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0555 - val_mean_squared_error: 0.0555\n",
      "Epoch 63/400\n",
      "1053/1053 [==============================] - 116s 111ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0060 - val_mean_squared_error: 0.0060\n",
      "Epoch 64/400\n",
      "1053/1053 [==============================] - 110s 104ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0037 - val_mean_squared_error: 0.0037\n",
      "Epoch 65/400\n",
      "1053/1053 [==============================] - 113s 108ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 66/400\n",
      "1053/1053 [==============================] - 118s 112ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0034 - val_mean_squared_error: 0.0034\n",
      "Epoch 67/400\n",
      "1053/1053 [==============================] - 118s 112ms/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 68/400\n",
      "1053/1053 [==============================] - 115s 109ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0053 - val_mean_squared_error: 0.0053\n",
      "Epoch 69/400\n",
      "1053/1053 [==============================] - 114s 108ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0281 - val_mean_squared_error: 0.0281\n",
      "Epoch 70/400\n",
      "1053/1053 [==============================] - 116s 110ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0126 - val_mean_squared_error: 0.0126\n",
      "Epoch 71/400\n",
      "1053/1053 [==============================] - 109s 104ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
      "Epoch 72/400\n",
      "1053/1053 [==============================] - 113s 107ms/step - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0059 - val_mean_squared_error: 0.0059\n",
      "Epoch 73/400\n",
      "1053/1053 [==============================] - 113s 107ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0058 - val_mean_squared_error: 0.0058\n",
      "Epoch 74/400\n",
      "1053/1053 [==============================] - 112s 106ms/step - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0123 - val_mean_squared_error: 0.0123\n",
      "Epoch 75/400\n",
      "1053/1053 [==============================] - 105s 100ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0037 - val_mean_squared_error: 0.0037\n",
      "Epoch 76/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0051 - val_mean_squared_error: 0.0051\n",
      "Epoch 77/400\n",
      "1053/1053 [==============================] - 105s 100ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0195 - val_mean_squared_error: 0.0195\n",
      "Epoch 78/400\n",
      "1053/1053 [==============================] - 112s 107ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 79/400\n",
      "1053/1053 [==============================] - 93s 89ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 80/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 81/400\n",
      "1053/1053 [==============================] - 98s 93ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 82/400\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 83/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0068 - val_mean_squared_error: 0.0068\n",
      "Epoch 84/400\n",
      "1053/1053 [==============================] - 105s 100ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0123 - val_mean_squared_error: 0.0123\n",
      "Epoch 85/400\n",
      "1053/1053 [==============================] - 108s 103ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 86/400\n",
      "1053/1053 [==============================] - 104s 99ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 87/400\n",
      "1053/1053 [==============================] - 107s 102ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 88/400\n",
      "1053/1053 [==============================] - 107s 102ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 89/400\n",
      "1053/1053 [==============================] - 105s 100ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0054 - val_mean_squared_error: 0.0054\n",
      "Epoch 90/400\n",
      "1053/1053 [==============================] - 110s 104ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 91/400\n",
      "1053/1053 [==============================] - 108s 102ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 92/400\n",
      "1053/1053 [==============================] - 110s 104ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0246 - val_mean_squared_error: 0.0246\n",
      "Epoch 93/400\n",
      "1053/1053 [==============================] - 106s 101ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 94/400\n",
      "1053/1053 [==============================] - 106s 101ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
      "Epoch 95/400\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
      "Epoch 96/400\n",
      "1053/1053 [==============================] - 89s 84ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 97/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1053 [==============================] - 96s 91ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 98/400\n",
      "1053/1053 [==============================] - 108s 102ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0037 - val_mean_squared_error: 0.0037\n",
      "Epoch 99/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0041 - val_mean_squared_error: 0.0041\n",
      "Epoch 100/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 101/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 102/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
      "Epoch 103/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0130 - val_mean_squared_error: 0.0130\n",
      "Epoch 104/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 105/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 106/400\n",
      "1053/1053 [==============================] - 95s 90ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
      "Epoch 107/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 108/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0088 - val_mean_squared_error: 0.0088\n",
      "Epoch 109/400\n",
      "1053/1053 [==============================] - 93s 89ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0059 - val_mean_squared_error: 0.0059\n",
      "Epoch 110/400\n",
      "1053/1053 [==============================] - 88s 83ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
      "Epoch 111/400\n",
      "1053/1053 [==============================] - 88s 84ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0036 - val_mean_squared_error: 0.0036\n",
      "Epoch 112/400\n",
      "1053/1053 [==============================] - 88s 83ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 113/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 114/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 115/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 116/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 117/400\n",
      "1053/1053 [==============================] - 95s 90ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
      "Epoch 118/400\n",
      "1053/1053 [==============================] - 95s 90ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 119/400\n",
      "1053/1053 [==============================] - 99s 94ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 120/400\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 121/400\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 122/400\n",
      "1053/1053 [==============================] - 99s 94ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
      "Epoch 123/400\n",
      "1053/1053 [==============================] - 101s 96ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 124/400\n",
      "1053/1053 [==============================] - 94s 89ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 125/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 126/400\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 127/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 128/400\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
      "Epoch 129/400\n",
      "1053/1053 [==============================] - 99s 94ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 130/400\n",
      "1053/1053 [==============================] - 101s 96ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 131/400\n",
      "1053/1053 [==============================] - 101s 96ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0057 - val_mean_squared_error: 0.0057\n",
      "Epoch 132/400\n",
      "1053/1053 [==============================] - 104s 99ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 133/400\n",
      "1053/1053 [==============================] - 104s 99ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 134/400\n",
      "1053/1053 [==============================] - 109s 103ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 135/400\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 136/400\n",
      "1053/1053 [==============================] - 108s 103ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
      "Epoch 137/400\n",
      "1053/1053 [==============================] - 114s 109ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 138/400\n",
      "1053/1053 [==============================] - 112s 107ms/step - loss: 9.9126e-04 - mean_squared_error: 9.9126e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 139/400\n",
      "1053/1053 [==============================] - 107s 102ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 140/400\n",
      "1053/1053 [==============================] - 109s 103ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 141/400\n",
      "1053/1053 [==============================] - 106s 101ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 142/400\n",
      "1053/1053 [==============================] - 107s 101ms/step - loss: 9.3602e-04 - mean_squared_error: 9.3602e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 143/400\n",
      "1053/1053 [==============================] - 108s 103ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 144/400\n",
      "1053/1053 [==============================] - 108s 103ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 145/400\n",
      "1053/1053 [==============================] - 112s 106ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 146/400\n",
      "1053/1053 [==============================] - 109s 104ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 147/400\n",
      "1053/1053 [==============================] - 111s 105ms/step - loss: 9.9839e-04 - mean_squared_error: 9.9839e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 148/400\n",
      "1053/1053 [==============================] - 112s 106ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 149/400\n",
      "1053/1053 [==============================] - 113s 107ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 150/400\n",
      "1053/1053 [==============================] - 114s 108ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 151/400\n",
      "1053/1053 [==============================] - 115s 110ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 152/400\n",
      "1053/1053 [==============================] - 117s 111ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 153/400\n",
      "1053/1053 [==============================] - 114s 108ms/step - loss: 9.3828e-04 - mean_squared_error: 9.3828e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 154/400\n",
      "1053/1053 [==============================] - 106s 100ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 155/400\n",
      "1053/1053 [==============================] - 106s 101ms/step - loss: 9.7060e-04 - mean_squared_error: 9.7060e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 156/400\n",
      "1053/1053 [==============================] - 106s 101ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 157/400\n",
      "1053/1053 [==============================] - 106s 101ms/step - loss: 9.7486e-04 - mean_squared_error: 9.7486e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 158/400\n",
      "1053/1053 [==============================] - 102s 97ms/step - loss: 8.7292e-04 - mean_squared_error: 8.7292e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 159/400\n",
      "1053/1053 [==============================] - 98s 93ms/step - loss: 8.4375e-04 - mean_squared_error: 8.4375e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 160/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 161/400\n",
      "1053/1053 [==============================] - 94s 89ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
      "Epoch 162/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 9.4205e-04 - mean_squared_error: 9.4205e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 163/400\n",
      "1053/1053 [==============================] - 93s 89ms/step - loss: 9.8683e-04 - mean_squared_error: 9.8683e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 164/400\n",
      "1053/1053 [==============================] - 95s 90ms/step - loss: 8.5250e-04 - mean_squared_error: 8.5250e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 165/400\n",
      "1053/1053 [==============================] - 105s 100ms/step - loss: 9.6489e-04 - mean_squared_error: 9.6489e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
      "Epoch 166/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 9.0172e-04 - mean_squared_error: 9.0172e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 167/400\n",
      "1053/1053 [==============================] - 108s 102ms/step - loss: 8.0689e-04 - mean_squared_error: 8.0689e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 168/400\n",
      "1053/1053 [==============================] - 101s 96ms/step - loss: 9.1595e-04 - mean_squared_error: 9.1595e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 169/400\n",
      "1053/1053 [==============================] - 88s 84ms/step - loss: 9.3745e-04 - mean_squared_error: 9.3745e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 170/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 8.7579e-04 - mean_squared_error: 8.7579e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
      "Epoch 171/400\n",
      "1053/1053 [==============================] - 95s 90ms/step - loss: 9.2235e-04 - mean_squared_error: 9.2235e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 172/400\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 8.9644e-04 - mean_squared_error: 8.9644e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
      "Epoch 173/400\n",
      "1053/1053 [==============================] - 107s 102ms/step - loss: 8.7231e-04 - mean_squared_error: 8.7231e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 174/400\n",
      "1053/1053 [==============================] - 101s 96ms/step - loss: 9.0723e-04 - mean_squared_error: 9.0723e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 175/400\n",
      "1053/1053 [==============================] - 95s 90ms/step - loss: 7.5746e-04 - mean_squared_error: 7.5746e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 176/400\n",
      "1053/1053 [==============================] - 95s 90ms/step - loss: 8.6365e-04 - mean_squared_error: 8.6365e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 177/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 9.8225e-04 - mean_squared_error: 9.8225e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 178/400\n",
      "1053/1053 [==============================] - 93s 89ms/step - loss: 7.2277e-04 - mean_squared_error: 7.2277e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 179/400\n",
      "1053/1053 [==============================] - 93s 89ms/step - loss: 7.9999e-04 - mean_squared_error: 7.9999e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 180/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 8.4621e-04 - mean_squared_error: 8.4621e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 181/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 7.5266e-04 - mean_squared_error: 7.5266e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 182/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 8.8202e-04 - mean_squared_error: 8.8202e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 183/400\n",
      "1053/1053 [==============================] - 87s 83ms/step - loss: 9.0599e-04 - mean_squared_error: 9.0599e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 184/400\n",
      "1053/1053 [==============================] - 89s 84ms/step - loss: 7.3814e-04 - mean_squared_error: 7.3814e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 185/400\n",
      "1053/1053 [==============================] - 88s 84ms/step - loss: 6.9830e-04 - mean_squared_error: 6.9830e-04 - val_loss: 0.0670 - val_mean_squared_error: 0.0670\n",
      "Epoch 186/400\n",
      "1053/1053 [==============================] - 88s 84ms/step - loss: 7.6942e-04 - mean_squared_error: 7.6942e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 187/400\n",
      "1053/1053 [==============================] - 88s 84ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 188/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 6.3261e-04 - mean_squared_error: 6.3261e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 189/400\n",
      "1053/1053 [==============================] - 89s 84ms/step - loss: 7.1778e-04 - mean_squared_error: 7.1778e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 190/400\n",
      "1053/1053 [==============================] - 88s 83ms/step - loss: 6.2890e-04 - mean_squared_error: 6.2890e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 191/400\n",
      "1053/1053 [==============================] - 88s 83ms/step - loss: 6.7315e-04 - mean_squared_error: 6.7315e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192/400\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 7.4424e-04 - mean_squared_error: 7.4424e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 193/400\n",
      "1053/1053 [==============================] - 99s 94ms/step - loss: 7.9464e-04 - mean_squared_error: 7.9464e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 194/400\n",
      "1053/1053 [==============================] - 102s 97ms/step - loss: 7.1898e-04 - mean_squared_error: 7.1898e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 195/400\n",
      "1053/1053 [==============================] - 117s 111ms/step - loss: 7.4910e-04 - mean_squared_error: 7.4910e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 196/400\n",
      "1053/1053 [==============================] - 117s 111ms/step - loss: 6.4997e-04 - mean_squared_error: 6.4997e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 197/400\n",
      "1053/1053 [==============================] - 118s 112ms/step - loss: 6.1456e-04 - mean_squared_error: 6.1456e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 198/400\n",
      "1053/1053 [==============================] - 116s 110ms/step - loss: 7.3535e-04 - mean_squared_error: 7.3535e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 199/400\n",
      "1053/1053 [==============================] - 117s 111ms/step - loss: 5.8204e-04 - mean_squared_error: 5.8204e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 200/400\n",
      "1053/1053 [==============================] - 114s 108ms/step - loss: 7.3269e-04 - mean_squared_error: 7.3269e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 201/400\n",
      "1053/1053 [==============================] - 117s 111ms/step - loss: 6.8327e-04 - mean_squared_error: 6.8327e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 202/400\n",
      "1053/1053 [==============================] - 117s 111ms/step - loss: 7.0494e-04 - mean_squared_error: 7.0494e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 203/400\n",
      "1053/1053 [==============================] - 115s 110ms/step - loss: 6.6850e-04 - mean_squared_error: 6.6850e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 204/400\n",
      "1053/1053 [==============================] - 113s 107ms/step - loss: 7.6384e-04 - mean_squared_error: 7.6384e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 205/400\n",
      "1053/1053 [==============================] - 113s 107ms/step - loss: 6.5994e-04 - mean_squared_error: 6.5994e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 206/400\n",
      "1053/1053 [==============================] - 116s 111ms/step - loss: 5.5500e-04 - mean_squared_error: 5.5500e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
      "Epoch 207/400\n",
      "1053/1053 [==============================] - 117s 111ms/step - loss: 7.5719e-04 - mean_squared_error: 7.5719e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 208/400\n",
      "1053/1053 [==============================] - 115s 109ms/step - loss: 6.6011e-04 - mean_squared_error: 6.6011e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 209/400\n",
      "1053/1053 [==============================] - 117s 111ms/step - loss: 6.3979e-04 - mean_squared_error: 6.3979e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 210/400\n",
      "1053/1053 [==============================] - 115s 109ms/step - loss: 5.1253e-04 - mean_squared_error: 5.1253e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 211/400\n",
      "1053/1053 [==============================] - 114s 109ms/step - loss: 5.2341e-04 - mean_squared_error: 5.2341e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 212/400\n",
      "1053/1053 [==============================] - 114s 108ms/step - loss: 5.5131e-04 - mean_squared_error: 5.5131e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 213/400\n",
      "1053/1053 [==============================] - 116s 110ms/step - loss: 5.2166e-04 - mean_squared_error: 5.2166e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 214/400\n",
      "1053/1053 [==============================] - 116s 110ms/step - loss: 4.6052e-04 - mean_squared_error: 4.6052e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 215/400\n",
      "1053/1053 [==============================] - 113s 107ms/step - loss: 4.3261e-04 - mean_squared_error: 4.3261e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 216/400\n",
      "1053/1053 [==============================] - 111s 105ms/step - loss: 5.0764e-04 - mean_squared_error: 5.0764e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 217/400\n",
      "1053/1053 [==============================] - 112s 107ms/step - loss: 4.9979e-04 - mean_squared_error: 4.9979e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 218/400\n",
      "1053/1053 [==============================] - 115s 109ms/step - loss: 4.7590e-04 - mean_squared_error: 4.7590e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 219/400\n",
      "1053/1053 [==============================] - 121s 115ms/step - loss: 5.1247e-04 - mean_squared_error: 5.1247e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 220/400\n",
      "1053/1053 [==============================] - 123s 116ms/step - loss: 5.0679e-04 - mean_squared_error: 5.0679e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 221/400\n",
      "1053/1053 [==============================] - 110s 104ms/step - loss: 4.9458e-04 - mean_squared_error: 4.9458e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 222/400\n",
      "1053/1053 [==============================] - 110s 104ms/step - loss: 5.0724e-04 - mean_squared_error: 5.0724e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 223/400\n",
      "1053/1053 [==============================] - 110s 105ms/step - loss: 4.5261e-04 - mean_squared_error: 4.5261e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 224/400\n",
      "1053/1053 [==============================] - 118s 112ms/step - loss: 5.0495e-04 - mean_squared_error: 5.0495e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 225/400\n",
      "1053/1053 [==============================] - 122s 116ms/step - loss: 6.1739e-04 - mean_squared_error: 6.1739e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 226/400\n",
      "1053/1053 [==============================] - 116s 110ms/step - loss: 5.0719e-04 - mean_squared_error: 5.0719e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 227/400\n",
      "1053/1053 [==============================] - 119s 113ms/step - loss: 4.8974e-04 - mean_squared_error: 4.8974e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 228/400\n",
      "1053/1053 [==============================] - 112s 106ms/step - loss: 4.4367e-04 - mean_squared_error: 4.4367e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 229/400\n",
      "1053/1053 [==============================] - 110s 104ms/step - loss: 4.7105e-04 - mean_squared_error: 4.7105e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 230/400\n",
      "1053/1053 [==============================] - 111s 105ms/step - loss: 4.7909e-04 - mean_squared_error: 4.7909e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 231/400\n",
      "1053/1053 [==============================] - 108s 102ms/step - loss: 4.2244e-04 - mean_squared_error: 4.2244e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 232/400\n",
      "1053/1053 [==============================] - 106s 100ms/step - loss: 5.2469e-04 - mean_squared_error: 5.2469e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
      "Epoch 233/400\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 5.8046e-04 - mean_squared_error: 5.8046e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 234/400\n",
      "1053/1053 [==============================] - 98s 93ms/step - loss: 4.5855e-04 - mean_squared_error: 4.5855e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 235/400\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 3.4668e-04 - mean_squared_error: 3.4668e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 236/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 4.7129e-04 - mean_squared_error: 4.7129e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 237/400\n",
      "1053/1053 [==============================] - 95s 90ms/step - loss: 4.8425e-04 - mean_squared_error: 4.8425e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 238/400\n",
      "1053/1053 [==============================] - 94s 90ms/step - loss: 3.9899e-04 - mean_squared_error: 3.9899e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 239/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 4.4795e-04 - mean_squared_error: 4.4795e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 240/400\n",
      "1053/1053 [==============================] - 110s 105ms/step - loss: 4.6579e-04 - mean_squared_error: 4.6579e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 241/400\n",
      "1053/1053 [==============================] - 102s 97ms/step - loss: 4.1197e-04 - mean_squared_error: 4.1197e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 242/400\n",
      "1053/1053 [==============================] - 99s 94ms/step - loss: 3.8346e-04 - mean_squared_error: 3.8346e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 243/400\n",
      "1053/1053 [==============================] - 116s 110ms/step - loss: 4.3629e-04 - mean_squared_error: 4.3629e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 244/400\n",
      "1053/1053 [==============================] - 115s 109ms/step - loss: 4.6479e-04 - mean_squared_error: 4.6479e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 245/400\n",
      "1053/1053 [==============================] - 122s 116ms/step - loss: 4.2306e-04 - mean_squared_error: 4.2306e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 246/400\n",
      "1053/1053 [==============================] - 124s 118ms/step - loss: 3.1464e-04 - mean_squared_error: 3.1464e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 247/400\n",
      "1053/1053 [==============================] - 121s 115ms/step - loss: 4.5871e-04 - mean_squared_error: 4.5871e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 248/400\n",
      "1053/1053 [==============================] - 119s 113ms/step - loss: 4.3344e-04 - mean_squared_error: 4.3344e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 249/400\n",
      "1053/1053 [==============================] - 121s 115ms/step - loss: 3.7186e-04 - mean_squared_error: 3.7186e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 250/400\n",
      "1053/1053 [==============================] - 120s 114ms/step - loss: 6.1080e-04 - mean_squared_error: 6.1080e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 251/400\n",
      "1053/1053 [==============================] - 120s 114ms/step - loss: 3.6269e-04 - mean_squared_error: 3.6269e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 252/400\n",
      "1053/1053 [==============================] - 120s 114ms/step - loss: 4.1232e-04 - mean_squared_error: 4.1232e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 253/400\n",
      "1053/1053 [==============================] - 123s 117ms/step - loss: 3.1672e-04 - mean_squared_error: 3.1672e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 254/400\n",
      "1053/1053 [==============================] - 120s 114ms/step - loss: 3.4665e-04 - mean_squared_error: 3.4665e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 255/400\n",
      "1053/1053 [==============================] - 115s 109ms/step - loss: 3.9166e-04 - mean_squared_error: 3.9166e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 256/400\n",
      "1053/1053 [==============================] - 117s 111ms/step - loss: 3.0555e-04 - mean_squared_error: 3.0555e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 257/400\n",
      "1053/1053 [==============================] - 117s 111ms/step - loss: 3.9983e-04 - mean_squared_error: 3.9983e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 258/400\n",
      "1053/1053 [==============================] - 119s 113ms/step - loss: 4.0233e-04 - mean_squared_error: 4.0233e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 259/400\n",
      "1053/1053 [==============================] - 121s 115ms/step - loss: 3.9731e-04 - mean_squared_error: 3.9731e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 260/400\n",
      "1053/1053 [==============================] - 124s 118ms/step - loss: 3.6062e-04 - mean_squared_error: 3.6062e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 261/400\n",
      "1053/1053 [==============================] - 121s 115ms/step - loss: 3.6715e-04 - mean_squared_error: 3.6715e-04 - val_loss: 9.6800e-04 - val_mean_squared_error: 9.6800e-04\n",
      "Epoch 262/400\n",
      "1053/1053 [==============================] - 121s 115ms/step - loss: 3.5886e-04 - mean_squared_error: 3.5886e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 263/400\n",
      "1053/1053 [==============================] - 127s 121ms/step - loss: 3.8899e-04 - mean_squared_error: 3.8899e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 264/400\n",
      "1053/1053 [==============================] - 123s 117ms/step - loss: 3.1221e-04 - mean_squared_error: 3.1221e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 265/400\n",
      "1053/1053 [==============================] - 122s 116ms/step - loss: 3.7009e-04 - mean_squared_error: 3.7009e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 266/400\n",
      "1053/1053 [==============================] - 118s 112ms/step - loss: 4.2086e-04 - mean_squared_error: 4.2086e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 267/400\n",
      "1053/1053 [==============================] - 115s 109ms/step - loss: 4.0172e-04 - mean_squared_error: 4.0172e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 268/400\n",
      "1053/1053 [==============================] - 118s 112ms/step - loss: 3.5761e-04 - mean_squared_error: 3.5761e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 269/400\n",
      "1053/1053 [==============================] - 117s 112ms/step - loss: 3.3502e-04 - mean_squared_error: 3.3502e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 270/400\n",
      "1053/1053 [==============================] - 115s 110ms/step - loss: 5.5296e-04 - mean_squared_error: 5.5296e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 271/400\n",
      "1053/1053 [==============================] - 116s 110ms/step - loss: 3.5097e-04 - mean_squared_error: 3.5097e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 272/400\n",
      "1053/1053 [==============================] - 121s 115ms/step - loss: 3.5701e-04 - mean_squared_error: 3.5701e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 273/400\n",
      "1053/1053 [==============================] - 131s 124ms/step - loss: 3.1338e-04 - mean_squared_error: 3.1338e-04 - val_loss: 9.8047e-04 - val_mean_squared_error: 9.8047e-04\n",
      "Epoch 274/400\n",
      "1053/1053 [==============================] - 125s 119ms/step - loss: 4.9294e-04 - mean_squared_error: 4.9294e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 275/400\n",
      "1053/1053 [==============================] - 119s 113ms/step - loss: 3.5436e-04 - mean_squared_error: 3.5436e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 276/400\n",
      "1053/1053 [==============================] - 118s 112ms/step - loss: 3.6106e-04 - mean_squared_error: 3.6106e-04 - val_loss: 9.6301e-04 - val_mean_squared_error: 9.6301e-04\n",
      "Epoch 277/400\n",
      "1053/1053 [==============================] - 123s 117ms/step - loss: 2.9022e-04 - mean_squared_error: 2.9022e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 278/400\n",
      "1053/1053 [==============================] - 117s 111ms/step - loss: 3.0523e-04 - mean_squared_error: 3.0523e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 279/400\n",
      "1053/1053 [==============================] - 112s 106ms/step - loss: 3.3902e-04 - mean_squared_error: 3.3902e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 280/400\n",
      "1053/1053 [==============================] - 118s 112ms/step - loss: 3.6825e-04 - mean_squared_error: 3.6825e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 281/400\n",
      "1053/1053 [==============================] - 125s 119ms/step - loss: 3.4490e-04 - mean_squared_error: 3.4490e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 282/400\n",
      "1053/1053 [==============================] - 122s 115ms/step - loss: 2.7355e-04 - mean_squared_error: 2.7355e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 283/400\n",
      "1053/1053 [==============================] - 121s 114ms/step - loss: 2.9908e-04 - mean_squared_error: 2.9908e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/400\n",
      "1053/1053 [==============================] - 120s 114ms/step - loss: 3.2877e-04 - mean_squared_error: 3.2877e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 285/400\n",
      "1053/1053 [==============================] - 122s 116ms/step - loss: 3.0998e-04 - mean_squared_error: 3.0998e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 286/400\n",
      "1053/1053 [==============================] - 124s 118ms/step - loss: 3.3277e-04 - mean_squared_error: 3.3277e-04 - val_loss: 9.8404e-04 - val_mean_squared_error: 9.8404e-04\n",
      "Epoch 287/400\n",
      "1053/1053 [==============================] - 123s 117ms/step - loss: 2.7599e-04 - mean_squared_error: 2.7599e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 288/400\n",
      "1053/1053 [==============================] - 120s 114ms/step - loss: 2.5233e-04 - mean_squared_error: 2.5233e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 289/400\n",
      "1053/1053 [==============================] - 119s 113ms/step - loss: 3.3773e-04 - mean_squared_error: 3.3773e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 290/400\n",
      "1053/1053 [==============================] - 117s 111ms/step - loss: 3.7782e-04 - mean_squared_error: 3.7782e-04 - val_loss: 9.7517e-04 - val_mean_squared_error: 9.7517e-04\n",
      "Epoch 291/400\n",
      "1053/1053 [==============================] - 114s 108ms/step - loss: 2.8974e-04 - mean_squared_error: 2.8974e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 292/400\n",
      "1053/1053 [==============================] - 116s 110ms/step - loss: 2.8615e-04 - mean_squared_error: 2.8615e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 293/400\n",
      "1053/1053 [==============================] - 117s 111ms/step - loss: 3.9859e-04 - mean_squared_error: 3.9859e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 294/400\n",
      "1053/1053 [==============================] - 115s 109ms/step - loss: 3.9526e-04 - mean_squared_error: 3.9526e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 295/400\n",
      "1053/1053 [==============================] - 113s 108ms/step - loss: 3.3861e-04 - mean_squared_error: 3.3861e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 296/400\n",
      "1053/1053 [==============================] - 115s 109ms/step - loss: 2.0742e-04 - mean_squared_error: 2.0742e-04 - val_loss: 9.6556e-04 - val_mean_squared_error: 9.6556e-04\n",
      "Epoch 297/400\n",
      "1053/1053 [==============================] - 121s 115ms/step - loss: 2.5635e-04 - mean_squared_error: 2.5635e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 298/400\n",
      "1053/1053 [==============================] - 118s 112ms/step - loss: 3.7802e-04 - mean_squared_error: 3.7802e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 299/400\n",
      "1053/1053 [==============================] - 112s 107ms/step - loss: 4.2312e-04 - mean_squared_error: 4.2312e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 300/400\n",
      "1053/1053 [==============================] - 110s 104ms/step - loss: 2.5486e-04 - mean_squared_error: 2.5486e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 301/400\n",
      "1053/1053 [==============================] - 111s 105ms/step - loss: 2.5898e-04 - mean_squared_error: 2.5898e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 302/400\n",
      "1053/1053 [==============================] - 114s 108ms/step - loss: 3.1906e-04 - mean_squared_error: 3.1906e-04 - val_loss: 9.6462e-04 - val_mean_squared_error: 9.6462e-04\n",
      "Epoch 303/400\n",
      "1053/1053 [==============================] - 114s 108ms/step - loss: 3.5426e-04 - mean_squared_error: 3.5426e-04 - val_loss: 9.9981e-04 - val_mean_squared_error: 9.9981e-04\n",
      "Epoch 304/400\n",
      "1053/1053 [==============================] - 117s 112ms/step - loss: 2.4788e-04 - mean_squared_error: 2.4788e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 305/400\n",
      "1053/1053 [==============================] - 116s 110ms/step - loss: 2.8399e-04 - mean_squared_error: 2.8399e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 306/400\n",
      "1053/1053 [==============================] - 120s 114ms/step - loss: 2.9208e-04 - mean_squared_error: 2.9208e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 307/400\n",
      "1053/1053 [==============================] - 118s 112ms/step - loss: 2.5745e-04 - mean_squared_error: 2.5745e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 308/400\n",
      "1053/1053 [==============================] - 118s 112ms/step - loss: 2.8265e-04 - mean_squared_error: 2.8265e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 309/400\n",
      "1053/1053 [==============================] - 116s 110ms/step - loss: 3.6561e-04 - mean_squared_error: 3.6561e-04 - val_loss: 9.9868e-04 - val_mean_squared_error: 9.9868e-04\n",
      "Epoch 310/400\n",
      "1053/1053 [==============================] - 118s 112ms/step - loss: 3.0467e-04 - mean_squared_error: 3.0467e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 311/400\n",
      "1053/1053 [==============================] - 117s 111ms/step - loss: 2.2281e-04 - mean_squared_error: 2.2281e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 312/400\n",
      "1053/1053 [==============================] - 116s 110ms/step - loss: 3.2646e-04 - mean_squared_error: 3.2646e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 313/400\n",
      "1053/1053 [==============================] - 113s 108ms/step - loss: 3.0164e-04 - mean_squared_error: 3.0164e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 314/400\n",
      "1053/1053 [==============================] - 117s 111ms/step - loss: 3.1090e-04 - mean_squared_error: 3.1090e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 315/400\n",
      "1053/1053 [==============================] - 112s 106ms/step - loss: 2.6306e-04 - mean_squared_error: 2.6306e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 316/400\n",
      "1053/1053 [==============================] - 104s 99ms/step - loss: 2.7301e-04 - mean_squared_error: 2.7301e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 317/400\n",
      "1053/1053 [==============================] - 106s 101ms/step - loss: 2.4659e-04 - mean_squared_error: 2.4659e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 318/400\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 2.4296e-04 - mean_squared_error: 2.4296e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 319/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 2.4491e-04 - mean_squared_error: 2.4491e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 320/400\n",
      "1053/1053 [==============================] - 95s 90ms/step - loss: 3.3934e-04 - mean_squared_error: 3.3934e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 321/400\n",
      "1053/1053 [==============================] - 102s 96ms/step - loss: 2.1332e-04 - mean_squared_error: 2.1332e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 322/400\n",
      "1053/1053 [==============================] - 116s 110ms/step - loss: 2.6933e-04 - mean_squared_error: 2.6933e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 323/400\n",
      "1053/1053 [==============================] - 124s 117ms/step - loss: 2.4138e-04 - mean_squared_error: 2.4138e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 324/400\n",
      "1053/1053 [==============================] - 124s 118ms/step - loss: 3.4505e-04 - mean_squared_error: 3.4505e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 325/400\n",
      "1053/1053 [==============================] - 125s 119ms/step - loss: 2.4765e-04 - mean_squared_error: 2.4765e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 326/400\n",
      "1053/1053 [==============================] - 124s 118ms/step - loss: 1.8433e-04 - mean_squared_error: 1.8433e-04 - val_loss: 9.6705e-04 - val_mean_squared_error: 9.6705e-04\n",
      "Epoch 327/400\n",
      "1053/1053 [==============================] - 113s 107ms/step - loss: 2.4086e-04 - mean_squared_error: 2.4086e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 328/400\n",
      "1053/1053 [==============================] - 110s 104ms/step - loss: 2.6572e-04 - mean_squared_error: 2.6572e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 329/400\n",
      "1053/1053 [==============================] - 109s 104ms/step - loss: 2.3849e-04 - mean_squared_error: 2.3849e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 330/400\n",
      "1053/1053 [==============================] - 107s 102ms/step - loss: 2.2675e-04 - mean_squared_error: 2.2675e-04 - val_loss: 9.4761e-04 - val_mean_squared_error: 9.4761e-04\n",
      "Epoch 331/400\n",
      "1053/1053 [==============================] - 110s 105ms/step - loss: 1.8816e-04 - mean_squared_error: 1.8816e-04 - val_loss: 9.9633e-04 - val_mean_squared_error: 9.9633e-04\n",
      "Epoch 332/400\n",
      "1053/1053 [==============================] - 109s 103ms/step - loss: 2.2374e-04 - mean_squared_error: 2.2374e-04 - val_loss: 9.0216e-04 - val_mean_squared_error: 9.0216e-04\n",
      "Epoch 333/400\n",
      "1053/1053 [==============================] - 112s 106ms/step - loss: 3.0496e-04 - mean_squared_error: 3.0496e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 334/400\n",
      "1053/1053 [==============================] - 113s 108ms/step - loss: 2.2879e-04 - mean_squared_error: 2.2879e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 335/400\n",
      "1053/1053 [==============================] - 118s 112ms/step - loss: 2.3798e-04 - mean_squared_error: 2.3798e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 336/400\n",
      "1053/1053 [==============================] - 119s 113ms/step - loss: 2.6670e-04 - mean_squared_error: 2.6670e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 337/400\n",
      "1053/1053 [==============================] - 124s 118ms/step - loss: 2.4760e-04 - mean_squared_error: 2.4760e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 338/400\n",
      "1053/1053 [==============================] - 105s 100ms/step - loss: 2.4885e-04 - mean_squared_error: 2.4885e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 339/400\n",
      "1053/1053 [==============================] - 123s 117ms/step - loss: 2.0496e-04 - mean_squared_error: 2.0496e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 340/400\n",
      "1053/1053 [==============================] - 120s 114ms/step - loss: 2.5659e-04 - mean_squared_error: 2.5659e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 341/400\n",
      "1053/1053 [==============================] - 124s 118ms/step - loss: 2.7613e-04 - mean_squared_error: 2.7613e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 342/400\n",
      "1053/1053 [==============================] - 117s 111ms/step - loss: 2.1735e-04 - mean_squared_error: 2.1735e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 343/400\n",
      "1053/1053 [==============================] - 118s 112ms/step - loss: 2.8803e-04 - mean_squared_error: 2.8803e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 344/400\n",
      "1053/1053 [==============================] - 120s 114ms/step - loss: 2.1553e-04 - mean_squared_error: 2.1553e-04 - val_loss: 9.2687e-04 - val_mean_squared_error: 9.2687e-04\n",
      "Epoch 345/400\n",
      "1053/1053 [==============================] - 123s 117ms/step - loss: 2.6453e-04 - mean_squared_error: 2.6453e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 346/400\n",
      "1053/1053 [==============================] - 120s 114ms/step - loss: 2.1530e-04 - mean_squared_error: 2.1530e-04 - val_loss: 8.5356e-04 - val_mean_squared_error: 8.5356e-04\n",
      "Epoch 347/400\n",
      "1053/1053 [==============================] - 118s 112ms/step - loss: 1.8239e-04 - mean_squared_error: 1.8239e-04 - val_loss: 9.6128e-04 - val_mean_squared_error: 9.6128e-04\n",
      "Epoch 348/400\n",
      "1053/1053 [==============================] - 119s 113ms/step - loss: 1.8757e-04 - mean_squared_error: 1.8757e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 349/400\n",
      "1053/1053 [==============================] - 128s 121ms/step - loss: 2.1589e-04 - mean_squared_error: 2.1589e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 350/400\n",
      "1053/1053 [==============================] - 124s 118ms/step - loss: 2.8335e-04 - mean_squared_error: 2.8335e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 351/400\n",
      "1053/1053 [==============================] - 119s 113ms/step - loss: 2.5745e-04 - mean_squared_error: 2.5745e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 352/400\n",
      "1053/1053 [==============================] - 116s 110ms/step - loss: 2.9013e-04 - mean_squared_error: 2.9013e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 353/400\n",
      "1053/1053 [==============================] - 113s 107ms/step - loss: 1.5890e-04 - mean_squared_error: 1.5890e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 354/400\n",
      "1053/1053 [==============================] - 116s 110ms/step - loss: 2.2443e-04 - mean_squared_error: 2.2443e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 355/400\n",
      "1053/1053 [==============================] - 116s 110ms/step - loss: 1.8518e-04 - mean_squared_error: 1.8518e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 356/400\n",
      "1053/1053 [==============================] - 117s 112ms/step - loss: 1.9799e-04 - mean_squared_error: 1.9799e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 357/400\n",
      "1053/1053 [==============================] - 117s 111ms/step - loss: 1.9971e-04 - mean_squared_error: 1.9971e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 358/400\n",
      "1053/1053 [==============================] - 120s 114ms/step - loss: 2.5426e-04 - mean_squared_error: 2.5426e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 359/400\n",
      "1053/1053 [==============================] - 118s 112ms/step - loss: 2.5294e-04 - mean_squared_error: 2.5294e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 360/400\n",
      "1053/1053 [==============================] - 120s 114ms/step - loss: 2.0659e-04 - mean_squared_error: 2.0659e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 361/400\n",
      "1053/1053 [==============================] - 124s 118ms/step - loss: 2.6625e-04 - mean_squared_error: 2.6625e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 362/400\n",
      "1053/1053 [==============================] - 127s 120ms/step - loss: 2.1232e-04 - mean_squared_error: 2.1232e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 363/400\n",
      "1053/1053 [==============================] - 115s 109ms/step - loss: 1.8422e-04 - mean_squared_error: 1.8422e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 364/400\n",
      "1053/1053 [==============================] - 113s 108ms/step - loss: 1.8247e-04 - mean_squared_error: 1.8247e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 365/400\n",
      "1053/1053 [==============================] - 114s 108ms/step - loss: 2.5552e-04 - mean_squared_error: 2.5552e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 366/400\n",
      "1053/1053 [==============================] - 113s 107ms/step - loss: 2.0283e-04 - mean_squared_error: 2.0283e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 367/400\n",
      "1053/1053 [==============================] - 110s 104ms/step - loss: 1.9075e-04 - mean_squared_error: 1.9075e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
      "Epoch 368/400\n",
      "1053/1053 [==============================] - 112s 106ms/step - loss: 1.8146e-04 - mean_squared_error: 1.8146e-04 - val_loss: 9.5164e-04 - val_mean_squared_error: 9.5164e-04\n",
      "Epoch 369/400\n",
      "1053/1053 [==============================] - 115s 110ms/step - loss: 1.8395e-04 - mean_squared_error: 1.8395e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 370/400\n",
      "1053/1053 [==============================] - 117s 112ms/step - loss: 1.9590e-04 - mean_squared_error: 1.9590e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 371/400\n",
      "1053/1053 [==============================] - 115s 109ms/step - loss: 2.4552e-04 - mean_squared_error: 2.4552e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 372/400\n",
      "1053/1053 [==============================] - 113s 108ms/step - loss: 1.7754e-04 - mean_squared_error: 1.7754e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 373/400\n",
      "1053/1053 [==============================] - 112s 106ms/step - loss: 1.5568e-04 - mean_squared_error: 1.5568e-04 - val_loss: 9.6785e-04 - val_mean_squared_error: 9.6785e-04\n",
      "Epoch 374/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1053 [==============================] - 112s 107ms/step - loss: 1.3876e-04 - mean_squared_error: 1.3876e-04 - val_loss: 9.2521e-04 - val_mean_squared_error: 9.2521e-04\n",
      "Epoch 375/400\n",
      "1053/1053 [==============================] - 111s 106ms/step - loss: 1.7596e-04 - mean_squared_error: 1.7596e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 376/400\n",
      "1053/1053 [==============================] - 109s 104ms/step - loss: 2.6555e-04 - mean_squared_error: 2.6555e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 377/400\n",
      "1053/1053 [==============================] - 115s 109ms/step - loss: 1.9737e-04 - mean_squared_error: 1.9737e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 378/400\n",
      "1053/1053 [==============================] - 106s 101ms/step - loss: 2.1606e-04 - mean_squared_error: 2.1606e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 379/400\n",
      "1053/1053 [==============================] - 98s 93ms/step - loss: 2.1357e-04 - mean_squared_error: 2.1357e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 380/400\n",
      "1053/1053 [==============================] - 99s 94ms/step - loss: 2.5300e-04 - mean_squared_error: 2.5300e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 381/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 1.7585e-04 - mean_squared_error: 1.7585e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 382/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 1.8694e-04 - mean_squared_error: 1.8694e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 383/400\n",
      "1053/1053 [==============================] - 95s 90ms/step - loss: 2.8968e-04 - mean_squared_error: 2.8968e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 384/400\n",
      "1053/1053 [==============================] - 94s 89ms/step - loss: 1.7218e-04 - mean_squared_error: 1.7218e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 385/400\n",
      "1053/1053 [==============================] - 93s 89ms/step - loss: 1.6430e-04 - mean_squared_error: 1.6430e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 386/400\n",
      "1053/1053 [==============================] - 108s 102ms/step - loss: 2.7973e-04 - mean_squared_error: 2.7973e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 387/400\n",
      "1053/1053 [==============================] - 111s 105ms/step - loss: 2.2489e-04 - mean_squared_error: 2.2489e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 388/400\n",
      "1053/1053 [==============================] - 112s 106ms/step - loss: 1.9384e-04 - mean_squared_error: 1.9384e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 389/400\n",
      "1053/1053 [==============================] - 105s 99ms/step - loss: 1.9996e-04 - mean_squared_error: 1.9996e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 390/400\n",
      "1053/1053 [==============================] - 111s 106ms/step - loss: 1.7652e-04 - mean_squared_error: 1.7652e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 391/400\n",
      "1053/1053 [==============================] - 112s 107ms/step - loss: 1.5570e-04 - mean_squared_error: 1.5570e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 392/400\n",
      "1053/1053 [==============================] - 112s 106ms/step - loss: 1.3629e-04 - mean_squared_error: 1.3629e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 393/400\n",
      "1053/1053 [==============================] - 105s 100ms/step - loss: 2.1319e-04 - mean_squared_error: 2.1319e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 394/400\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 1.8415e-04 - mean_squared_error: 1.8415e-04 - val_loss: 9.0829e-04 - val_mean_squared_error: 9.0829e-04\n",
      "Epoch 395/400\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 1.5592e-04 - mean_squared_error: 1.5592e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 396/400\n",
      "1053/1053 [==============================] - 106s 101ms/step - loss: 1.7368e-04 - mean_squared_error: 1.7368e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 397/400\n",
      "1053/1053 [==============================] - 99s 94ms/step - loss: 1.9285e-04 - mean_squared_error: 1.9285e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 398/400\n",
      "1053/1053 [==============================] - 99s 94ms/step - loss: 1.5585e-04 - mean_squared_error: 1.5585e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 399/400\n",
      "1053/1053 [==============================] - 106s 100ms/step - loss: 1.7375e-04 - mean_squared_error: 1.7375e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 400/400\n",
      "1053/1053 [==============================] - 98s 93ms/step - loss: 1.6909e-04 - mean_squared_error: 1.6909e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-7b6b5f3718d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# print(history.history)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0my_pred_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "early_stop = EarlyStopping(monitor='val_mean_squared_error', patience=30)\n",
    "# model.load_weights('./modelWights/regression_model'+model_fn.__name__+'.h5')\n",
    "model_checkpoint = ModelCheckpoint('./modelWights/weights_'+'resnet50_without_pre'+time_str+'.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=nb_epochs, validation_data=(X_val, y_val), callbacks=[model_checkpoint])\n",
    "# print(history.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.602876 -1.714657 -1.725154 -1.905815 ... -1.99617  -1.835018 -1.672766 -1.55472 ]\n",
      " [-1.626547 -1.703639 -1.874007 -2.015578 ... -2.038206 -1.992903 -1.842401 -1.556368]\n",
      " [-2.45059  -2.495802 -2.756412 -2.987875 ... -3.205384 -2.886243 -2.12324  -2.002175]\n",
      " [-2.581464 -2.636156 -3.228589 -3.844085 ... -4.178961 -3.674651 -2.766613 -2.335279]\n",
      " ...\n",
      " [-2.635763 -2.492539 -3.298693 -4.123997 ... -4.648807 -3.982989 -3.119166 -2.588822]\n",
      " [-2.355711 -2.239166 -3.107065 -3.694455 ... -4.223949 -3.626656 -2.724855 -2.437797]\n",
      " [-1.976302 -1.940042 -2.678933 -2.927193 ... -3.009618 -2.561657 -2.244801 -1.837124]\n",
      " [-1.748684 -1.95183  -2.336588 -2.763162 ... -3.112839 -2.657984 -2.096123 -1.754172]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_3rd_layer_output = K.function([model.layers[0].input, K.learning_phase()],\n",
    "                                  [model.layers[-3].output])\n",
    "# output in test mode = 0\n",
    "layer_output = get_3rd_layer_output([X_train[2].reshape(1,512,512,3), 0])[0]\n",
    "imshow(layer_output[0,:,:,0])\n",
    "print(layer_output[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pretrained with imagenet weight tend to have negative activation??\n",
    "- network architacture: ResNet50 have larger absolute activation (e.g. 326) while mobileNet have lower value (e.g. 1e-20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## resnet 50 color without pretrain--results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330/330 [==============================] - 60s 182ms/step\n",
      "(330,)\n",
      "(330,)\n",
      "[  28.603256    71.07702    274.13205    235.63416    444.87213\n",
      "  121.185776   223.74332     55.099144   151.6922     162.6203\n",
      "  237.1217      49.00352    151.48047     20.321041   109.658264\n",
      "   49.619377   154.97488    134.90668    248.74939    198.93352\n",
      "  505.0509     239.85258    284.63034     74.45407     69.19576\n",
      "   69.405495   130.32161     26.75192    136.03673    149.9048\n",
      "  154.9479     125.91618    111.04576     38.923145   115.37618\n",
      "  149.87564    286.33975     31.314335    74.96099    428.89722\n",
      "  250.07385    278.27795    172.63026    247.9583     284.84143\n",
      "  314.19244     39.688393    22.28883    101.418594   140.69962\n",
      "    8.597553    94.09465    152.2221      25.92963    191.65405\n",
      "  173.46999    108.332115   246.38612    229.86555      2.8510242\n",
      "  231.05772    316.58746     43.660717   451.05524    223.35466\n",
      "  270.56265    225.81323    215.14874    263.69553    104.63653\n",
      "  235.94928     54.252686   190.36946    163.11998     66.14015\n",
      "  202.39096    151.91853    137.7165     123.39221     15.393316\n",
      "  128.87839    362.3257      56.845665    60.81222     53.17995\n",
      "   55.411354    55.83682     92.50316     95.78079     89.017914\n",
      "  218.06999     26.370481   180.71106    166.52011    226.22984\n",
      "  223.9408     104.37238    202.39096     67.05632     65.58372\n",
      "  182.82724     94.50992     86.448944    21.762505    18.670261\n",
      "   43.936127   167.16226    186.88995     57.88771     25.355816\n",
      "   23.017794   163.39954    148.00435     55.72851    104.89634\n",
      "  388.652       32.199265   288.36472     33.994606   272.97366\n",
      "  233.8669     152.66537    183.49335    241.08351    233.40051\n",
      "  163.97382    250.66763     57.8008      23.29497    120.55248\n",
      "  247.43826    304.4013     448.08377     53.226486    91.82785\n",
      "  232.53563     75.991005   115.703       13.274282   479.0145\n",
      "   31.14988     71.22266     26.857346   489.3194      25.946453\n",
      "  107.412445   105.907684    85.20969    147.7095     191.1077\n",
      "  287.66727     90.73392    113.29831     28.691635   493.55597\n",
      "   20.546616   152.5328      54.372208    26.439308   146.52928\n",
      "   72.32843   2067.43       108.06395    124.359055    51.63835\n",
      "   37.987225   243.89323    124.77912    231.6956      43.178333\n",
      "   28.087734   121.21997     18.437922   147.03442     92.83185\n",
      "  201.55684    295.66873    109.54043    244.4998      82.008705\n",
      "  459.1539     162.3712     115.15271    124.59966     83.33702\n",
      "  141.22673    169.56311     55.08422    132.6088      40.87603\n",
      "  173.99925     97.63991     22.71399    142.38524    409.671\n",
      "   13.458133   115.37365     85.272255   120.15626     44.37682\n",
      "   89.922455   178.30692    193.56018    106.80439     59.191406\n",
      "   53.626076   133.98734    173.73839     73.92793     66.50671\n",
      "  204.65756    150.79765    158.47821    132.48076    127.3565\n",
      "   89.57566    250.46439    268.0511      60.53433    285.13657\n",
      "  132.0194     150.30678     17.756521   131.82634    299.36392\n",
      "   27.917593    24.630428   122.58998    136.68172     66.37632\n",
      "  221.29233     37.708057   189.13585    209.2315      53.07475\n",
      "  130.42232    183.9745      65.01279    119.2778      17.302021\n",
      "  198.8748      91.85536    133.0171      68.80767    300.98413\n",
      "   81.45136     62.92917    139.49916     22.546858   172.78693\n",
      "  156.2802     182.27113    127.66205     77.99774    348.04385\n",
      "  299.32733    111.50222     63.233547    12.8348465   82.35704\n",
      "   79.99297    142.97285    160.69267    475.44214    206.62111\n",
      "   56.164623    78.95958    281.31165    105.278206   132.89297\n",
      "  112.969505   117.9215     254.31111    253.7032      17.054558\n",
      "  164.54791    190.63559    241.08351    171.32118    153.99844\n",
      "  106.41368    216.7791     256.77274    231.33727     79.22977\n",
      "   32.695004   131.4693      18.168837   293.322      244.56476\n",
      "  170.76122    143.82715     79.44168     19.155561   301.90753\n",
      "  236.31262    121.81826    112.93904    147.19853    248.30606\n",
      "  184.03107    494.75568     49.67212     96.18187    119.79883\n",
      "   87.221214   173.29053    101.79691    445.65497     72.92899\n",
      "   59.556553    30.865223   136.93066     32.83731    243.93332\n",
      "  454.62747    348.4858     145.34494     61.467396   254.75746\n",
      "  251.0843     121.17913    258.3737     283.5874      18.95885\n",
      "  147.4371      52.124924   180.78508    179.62614     87.387794 ]\n",
      "[  6.  54. 275. 240. 450. 108. 225.  58. 150. 162. 250.  80. 162.  64.\n",
      " 100.  46. 174. 128. 250. 184. 500. 300. 285.  68.  88.  30. 120.   8.\n",
      " 128. 154. 144. 126.  80.  52. 112. 158. 300.  18.  66. 450. 265. 290.\n",
      " 168. 255. 295. 300.  32.  24.  96. 132.   6.  92. 186.   8. 180. 196.\n",
      " 108. 230. 225.   8. 215. 290.  54. 450. 250. 270. 230. 200. 225.  80.\n",
      " 245.   4. 190. 168.  44. 225. 182. 122. 120.   4. 124. 350.  56.   2.\n",
      "  12.  84.  84.  78.  84.  82. 225.  18. 176. 174. 198. 225. 100. 225.\n",
      "  46.  60. 188.  88.  70.  10.  24.  48. 172. 186.  52.  10.   2. 188.\n",
      " 156.  48. 116. 400.  30. 275.  40. 275. 215. 156. 190. 225. 245. 172.\n",
      " 235. 102.   6. 122. 265. 295. 450.  46.  86. 196.  84. 112.  10. 500.\n",
      "  20.  58.  34. 500.  14.  82. 104.  92. 154. 188. 285. 100. 120.  34.\n",
      " 500.  16. 132.  46.  12. 136.  22.   1. 100.  90.  58.  38. 270. 112.\n",
      " 215.  32.  26. 124.  16. 144.  90. 182. 300. 134. 215. 102. 450. 128.\n",
      "  94. 138.  72. 146. 158.   0. 112.  40. 178.  98.  32. 136. 400.   4.\n",
      " 112.  28. 106.  28.  74. 200. 192.  96.  58.  32. 140. 172.  72.  40.\n",
      " 200. 156. 162. 142. 134.  76. 255. 240.  58. 280. 124. 128.  28. 136.\n",
      " 275.  26.  16. 128. 134.  64. 225.  32. 186. 220.  52. 154. 184.  54.\n",
      " 114.  18. 220.  94. 130.  60. 295.  74.  60. 164.  10. 198. 138. 200.\n",
      " 128.  74. 350. 285. 102.   2.   4.  92.  86. 154. 190. 500. 186.  42.\n",
      "  62. 275. 110. 124. 116.  98. 275. 265.  28. 182. 198. 225. 170. 154.\n",
      "  86. 235. 300. 240.  84.  36. 150.  14. 300. 250. 164. 160.  78.  22.\n",
      " 300. 230. 112. 118. 118. 250. 178. 500.   0.  90. 120.  80. 186.  90.\n",
      " 450.  74.  64.  24. 140.  36. 215. 450. 350. 136.  68. 290. 220. 116.\n",
      " 300. 290.   8. 120.  48. 188. 164.  86.]\n",
      "mse =  1037.0219513345353\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('./modelWights/weights_resnet50_without_pre1570593544.982001.h5')\n",
    "y_pred_ = model.predict(X_test, batch_size=1, verbose=1)\n",
    "\n",
    "time_str = str(time.time())\n",
    "y_pred_, y_test_ = post_training_data(y_test, y_pred_)\n",
    "predicted_file = 'regress_'+\"resnet50_without_pre\"+\"resnet50\"+time_str+'_pred_color.csv'\n",
    "save_result(\"./result/\"+predicted_file,y_pred_,y_test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## resnet 50 gray scale without pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dltdc/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 1053 samples, validate on 264 samples\n",
      "Epoch 1/250\n",
      "1053/1053 [==============================] - 57s 54ms/step - loss: 0.5666 - mse: 0.5666 - val_loss: 0.1555 - val_mse: 0.1555\n",
      "Epoch 2/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0267 - mse: 0.0267 - val_loss: 0.0192 - val_mse: 0.0192\n",
      "Epoch 3/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.0189 - val_mse: 0.0189\n",
      "Epoch 4/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.1635 - val_mse: 0.1635\n",
      "Epoch 5/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0208 - mse: 0.0208 - val_loss: 0.0153 - val_mse: 0.0153\n",
      "Epoch 6/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 7/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0217 - mse: 0.0217 - val_loss: 0.0818 - val_mse: 0.0818\n",
      "Epoch 8/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0195 - mse: 0.0195 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 9/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0141 - mse: 0.0141 - val_loss: 2.2309 - val_mse: 2.2309\n",
      "Epoch 10/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 11/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0253 - val_mse: 0.0253\n",
      "Epoch 12/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0131 - val_mse: 0.0131\n",
      "Epoch 13/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 14/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 15/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0498 - val_mse: 0.0498\n",
      "Epoch 16/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0286 - val_mse: 0.0286\n",
      "Epoch 17/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 18/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0176 - val_mse: 0.0176\n",
      "Epoch 19/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0147 - val_mse: 0.0147\n",
      "Epoch 20/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0436 - val_mse: 0.0436\n",
      "Epoch 21/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0401 - mse: 0.0401 - val_loss: 1263186067952.4849 - val_mse: 1263186214912.0000\n",
      "Epoch 22/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0299 - mse: 0.0299 - val_loss: 0.0207 - val_mse: 0.0207\n",
      "Epoch 23/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 0.0274 - val_mse: 0.0274\n",
      "Epoch 24/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0178 - mse: 0.0178 - val_loss: 0.0101 - val_mse: 0.0101\n",
      "Epoch 25/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0190 - val_mse: 0.0190\n",
      "Epoch 26/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 27/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0107 - val_mse: 0.0107\n",
      "Epoch 28/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 29/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0133 - val_mse: 0.0133\n",
      "Epoch 30/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 31/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 32/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 33/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 34/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0057 - val_mse: 0.0057\n",
      "Epoch 35/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 36/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 37/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.0132 - val_mse: 0.0132\n",
      "Epoch 38/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0247 - val_mse: 0.0247\n",
      "Epoch 39/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 40/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0875 - val_mse: 0.0875\n",
      "Epoch 41/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0304 - val_mse: 0.0304\n",
      "Epoch 42/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0109 - val_mse: 0.0109\n",
      "Epoch 43/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0064 - val_mse: 0.0064\n",
      "Epoch 44/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 45/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0344 - val_mse: 0.0344\n",
      "Epoch 46/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0255 - val_mse: 0.0255\n",
      "Epoch 47/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 48/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 49/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0210 - val_mse: 0.0210\n",
      "Epoch 50/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 51/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 52/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 53/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 54/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0071 - val_mse: 0.0071\n",
      "Epoch 55/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 56/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 57/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 58/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0056 - val_mse: 0.0056\n",
      "Epoch 59/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 60/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 61/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 62/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 63/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 64/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 65/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0122 - val_mse: 0.0122\n",
      "Epoch 66/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 67/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 68/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 69/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0965 - val_mse: 0.0965\n",
      "Epoch 70/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 71/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0154 - val_mse: 0.0154\n",
      "Epoch 72/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 73/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 74/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 75/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0057 - val_mse: 0.0057\n",
      "Epoch 76/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 77/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 78/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 79/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 80/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 81/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 82/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 83/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0048 - val_mse: 0.0048\n",
      "Epoch 84/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 85/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 86/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0105 - val_mse: 0.0105\n",
      "Epoch 87/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 88/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 89/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 90/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 91/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 92/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 93/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 94/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 95/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 96/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 97/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 98/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 99/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 100/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 101/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 102/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 103/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 104/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 105/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 106/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 107/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 108/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 109/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 110/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 111/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 112/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 113/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 114/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 115/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 116/250\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 117/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 118/250\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 119/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 120/250\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 121/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 122/250\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 123/250\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 124/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 125/250\n",
      "1053/1053 [==============================] - 48s 45ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 126/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 127/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 128/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 129/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 130/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 131/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 132/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 133/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 134/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 135/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 136/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 137/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 138/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 139/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 140/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 141/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 142/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 143/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 144/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 145/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 146/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 147/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 148/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 149/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 150/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 151/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 152/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 9.9879e-04 - mse: 9.9879e-04 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 153/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 154/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 9.7035e-04 - mse: 9.7035e-04 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 155/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 156/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 157/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 158/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 159/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 160/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 9.3942e-04 - mse: 9.3942e-04 - val_loss: 0.0064 - val_mse: 0.0064\n",
      "Epoch 161/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 162/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 9.7259e-04 - mse: 9.7259e-04 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 163/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 164/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 9.5047e-04 - mse: 9.5047e-04 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 165/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 9.5663e-04 - mse: 9.5663e-04 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 166/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 167/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 168/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 169/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 9.8760e-04 - mse: 9.8760e-04 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 170/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 9.4182e-04 - mse: 9.4182e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 171/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 8.5228e-04 - mse: 8.5228e-04 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 172/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 9.5493e-04 - mse: 9.5493e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 173/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 174/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 9.8117e-04 - mse: 9.8117e-04 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 175/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 176/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 177/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 7.9397e-04 - mse: 7.9397e-04 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 178/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 9.4883e-04 - mse: 9.4883e-04 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 179/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 9.1159e-04 - mse: 9.1159e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 180/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 9.2067e-04 - mse: 9.2067e-04 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 181/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 9.7519e-04 - mse: 9.7519e-04 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 182/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 8.4957e-04 - mse: 8.4957e-04 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 183/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 8.8607e-04 - mse: 8.8607e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 184/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 8.6447e-04 - mse: 8.6447e-04 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 185/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 8.8748e-04 - mse: 8.8748e-04 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 186/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 8.7194e-04 - mse: 8.7194e-04 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 187/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 7.9412e-04 - mse: 7.9412e-04 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 188/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 8.5382e-04 - mse: 8.5382e-04 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 189/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 9.0297e-04 - mse: 9.0297e-04 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 190/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 7.4982e-04 - mse: 7.4982e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 191/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 9.4838e-04 - mse: 9.4839e-04 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 192/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 8.5502e-04 - mse: 8.5502e-04 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 193/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 9.5923e-04 - mse: 9.5923e-04 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 194/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 7.4498e-04 - mse: 7.4498e-04 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 195/250\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 196/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 8.3217e-04 - mse: 8.3217e-04 - val_loss: 0.0049 - val_mse: 0.0049\n",
      "Epoch 197/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 8.0053e-04 - mse: 8.0053e-04 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 198/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 9.2382e-04 - mse: 9.2382e-04 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 199/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 8.3249e-04 - mse: 8.3249e-04 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 200/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 6.2239e-04 - mse: 6.2239e-04 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 201/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 6.6648e-04 - mse: 6.6648e-04 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 202/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 7.8846e-04 - mse: 7.8846e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 203/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 8.0808e-04 - mse: 8.0808e-04 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 204/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 7.0374e-04 - mse: 7.0374e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 205/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 6.9072e-04 - mse: 6.9072e-04 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 206/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 7.4266e-04 - mse: 7.4266e-04 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 207/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 8.5519e-04 - mse: 8.5519e-04 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 208/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 7.1695e-04 - mse: 7.1695e-04 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 209/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 210/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 7.9528e-04 - mse: 7.9528e-04 - val_loss: 9.8904e-04 - val_mse: 9.8904e-04\n",
      "Epoch 211/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 9.7782e-04 - mse: 9.7782e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 212/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 7.3803e-04 - mse: 7.3803e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 213/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 7.2353e-04 - mse: 7.2353e-04 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 214/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 5.9459e-04 - mse: 5.9459e-04 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 215/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 7.6556e-04 - mse: 7.6556e-04 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 216/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 6.4682e-04 - mse: 6.4682e-04 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 217/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 6.1689e-04 - mse: 6.1689e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 218/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 7.3181e-04 - mse: 7.3181e-04 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 219/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 6.7925e-04 - mse: 6.7925e-04 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 220/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 8.5568e-04 - mse: 8.5568e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 221/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 8.6583e-04 - mse: 8.6583e-04 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 222/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 7.2156e-04 - mse: 7.2156e-04 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 223/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 6.7269e-04 - mse: 6.7269e-04 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 224/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 6.0115e-04 - mse: 6.0115e-04 - val_loss: 0.0010 - val_mse: 0.0010\n",
      "Epoch 225/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 6.1294e-04 - mse: 6.1294e-04 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 226/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 6.1318e-04 - mse: 6.1318e-04 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 227/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 6.4042e-04 - mse: 6.4042e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 228/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 5.8928e-04 - mse: 5.8928e-04 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 229/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 5.4360e-04 - mse: 5.4360e-04 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 230/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 6.4128e-04 - mse: 6.4128e-04 - val_loss: 0.0015 - val_mse: 0.0015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 231/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 6.6455e-04 - mse: 6.6455e-04 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 232/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 9.5208e-04 - mse: 9.5208e-04 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 233/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 5.7518e-04 - mse: 5.7518e-04 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 234/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 5.9772e-04 - mse: 5.9772e-04 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 235/250\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 7.7513e-04 - mse: 7.7513e-04 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 236/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 6.7691e-04 - mse: 6.7691e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 237/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 6.9056e-04 - mse: 6.9056e-04 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 238/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 5.6714e-04 - mse: 5.6714e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 239/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 6.8818e-04 - mse: 6.8818e-04 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 240/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 6.3735e-04 - mse: 6.3735e-04 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 241/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 6.5327e-04 - mse: 6.5327e-04 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 242/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 6.3932e-04 - mse: 6.3932e-04 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 243/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 4.9982e-04 - mse: 4.9982e-04 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 244/250\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 5.1658e-04 - mse: 5.1658e-04 - val_loss: 9.4480e-04 - val_mse: 9.4480e-04\n",
      "Epoch 245/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 5.0713e-04 - mse: 5.0713e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 246/250\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 5.2315e-04 - mse: 5.2315e-04 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 247/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 7.8593e-04 - mse: 7.8593e-04 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 248/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 6.0379e-04 - mse: 6.0379e-04 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 249/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 6.0249e-04 - mse: 6.0249e-04 - val_loss: 9.3511e-04 - val_mse: 9.3511e-04\n",
      "Epoch 250/250\n",
      "1053/1053 [==============================] - 49s 46ms/step - loss: 5.6532e-04 - mse: 5.6532e-04 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "330/330 [==============================] - 7s 21ms/step\n",
      "(330,)\n",
      "(330,)\n",
      "[   18.32506      31.263664    249.88976     212.11621     422.8016\n",
      "   133.58058     227.01994      76.95268     143.45323     148.02345\n",
      "   243.5157       25.432705    150.61372     139.87436     102.60952\n",
      "    35.255253    151.00642     126.75914     242.70007     182.07335\n",
      "   459.3274      236.89679     267.72498      61.713047     58.881775\n",
      "   110.20673     115.84923      -8.256838    144.13068     143.36356\n",
      "   129.89032     100.72277     109.773544     56.56925     118.4564\n",
      "   160.4214      283.46194      15.557185     51.844994    410.83432\n",
      "   252.04236     268.13388     174.75714     219.87192     284.9514\n",
      "   302.6886       27.12319      14.1587925    92.76132     136.21649\n",
      "    23.519316     90.05837     163.11247      74.37695     159.75235\n",
      "   146.2384       99.46728     240.93414     249.51175      16.027092\n",
      "   202.78987     283.16943      40.78758     416.5625      240.02577\n",
      "   264.2472      235.53172     203.03497     243.97237      70.37791\n",
      "   212.56026      11.559397    161.58685     171.20442      30.04054\n",
      "   161.73149     127.08495     124.78236     118.341606     24.039745\n",
      "    97.558685    341.7821       86.94327       4.904568     16.920038\n",
      "    42.374714     44.67711      62.454105     77.55236      82.82896\n",
      "   226.35117       4.7048926   179.72191     154.17723     202.82773\n",
      "   212.88965     108.924355    161.73149      49.612156     57.23311\n",
      "   167.37952      80.60598      69.60639      14.360547      9.066768\n",
      "    49.88064     126.57693     176.87991      58.45388       5.411789\n",
      "   -11.456206    136.43616     122.288055     57.0162      110.5219\n",
      "   396.1056       17.479397    261.57535      37.030727    267.87662\n",
      "   232.10251     140.96098     145.19817     238.66016     216.37769\n",
      "   120.91153     241.10829      75.45451     -16.455963    120.31709\n",
      "   233.43626     292.68433     432.6087       51.0405       76.87489\n",
      "   218.81335      67.048386     94.39063       4.8623233   484.92642\n",
      "    74.01581      56.82414      29.542446    470.6341       19.524298\n",
      "   116.90081     104.33331      62.319748    109.48591     153.62547\n",
      "   276.36688      74.56313      68.976494     29.880129    446.83923\n",
      "    17.971233    134.60663      28.566822     13.237447    121.19472\n",
      "    36.481983  -1401.6888       70.363014     93.30437      38.04492\n",
      "    39.801456    263.3587      106.62899     206.97424      34.47245\n",
      "    32.209805    102.89225       3.515333    126.16563      83.22367\n",
      "   183.3537      290.24054     112.981544    252.11603      64.85007\n",
      "   417.0793      150.53023     105.416885    132.97653      68.58145\n",
      "   103.52426     132.86221      24.163403    151.13367      25.007748\n",
      "   146.24936     102.89488      38.04823     139.8769      394.01022\n",
      "    19.901209     99.12306      68.30169     103.1931       51.75302\n",
      "    79.956215    177.73453     156.25258     113.66814      44.964134\n",
      "    54.734966    125.30577     176.28586      47.479294    113.24024\n",
      "   207.90517     124.89096     147.56233     110.108215    102.52004\n",
      "    70.20917     202.8911      257.74704      59.542038    276.8893\n",
      "    99.05292     151.88124      32.51525     123.576324    273.72955\n",
      "    17.93746      27.626925    107.834145    128.84036      63.596325\n",
      "   211.83488      33.404766    184.2235      189.20609      49.299473\n",
      "    99.84085     181.11139      46.58483      91.119        17.984972\n",
      "   186.58873      80.83641     123.779724     74.25371     271.32678\n",
      "    51.15082      60.280926    147.42172       5.820759    128.78047\n",
      "   150.64012     209.20842     122.73451      58.01433     344.39496\n",
      "   263.2863       75.00517       5.2214413    98.65476      78.49082\n",
      "    72.28809     134.12325     141.1074      429.19415     185.25533\n",
      "    32.245712     97.00328     293.951       110.25407      89.048805\n",
      "   110.34139      86.261635    262.1389      224.69739      31.360031\n",
      "   149.84367     173.93147     238.66016     158.47923     114.87445\n",
      "   136.01971     207.80338     297.96603     222.84015      70.21357\n",
      "    19.256763    124.93854       5.8904214   311.36197     241.96785\n",
      "   142.79453     146.08824      51.48448      23.439854    293.92618\n",
      "   225.81166      79.60073     102.43499     157.29999     248.05745\n",
      "   188.54408     454.3185       42.433716     87.48557     119.22446\n",
      "    72.71826     178.32803      94.90595     420.37393      50.159737\n",
      "    48.02359      15.34877     113.21612      20.564497    212.11678\n",
      "   416.49783     325.1309      124.71021      50.32598     293.84885\n",
      "   232.99141     124.61305     266.27377     280.5341       22.286774\n",
      "   153.46289      56.4789      140.97952     143.42175      76.11659  ]\n",
      "[  6.  54. 275. 240. 450. 108. 225.  58. 150. 162. 250.  80. 162.  64.\n",
      " 100.  46. 174. 128. 250. 184. 500. 300. 285.  68.  88.  30. 120.   8.\n",
      " 128. 154. 144. 126.  80.  52. 112. 158. 300.  18.  66. 450. 265. 290.\n",
      " 168. 255. 295. 300.  32.  24.  96. 132.   6.  92. 186.   8. 180. 196.\n",
      " 108. 230. 225.   8. 215. 290.  54. 450. 250. 270. 230. 200. 225.  80.\n",
      " 245.   4. 190. 168.  44. 225. 182. 122. 120.   4. 124. 350.  56.   2.\n",
      "  12.  84.  84.  78.  84.  82. 225.  18. 176. 174. 198. 225. 100. 225.\n",
      "  46.  60. 188.  88.  70.  10.  24.  48. 172. 186.  52.  10.   2. 188.\n",
      " 156.  48. 116. 400.  30. 275.  40. 275. 215. 156. 190. 225. 245. 172.\n",
      " 235. 102.   6. 122. 265. 295. 450.  46.  86. 196.  84. 112.  10. 500.\n",
      "  20.  58.  34. 500.  14.  82. 104.  92. 154. 188. 285. 100. 120.  34.\n",
      " 500.  16. 132.  46.  12. 136.  22.   1. 100.  90.  58.  38. 270. 112.\n",
      " 215.  32.  26. 124.  16. 144.  90. 182. 300. 134. 215. 102. 450. 128.\n",
      "  94. 138.  72. 146. 158.   0. 112.  40. 178.  98.  32. 136. 400.   4.\n",
      " 112.  28. 106.  28.  74. 200. 192.  96.  58.  32. 140. 172.  72.  40.\n",
      " 200. 156. 162. 142. 134.  76. 255. 240.  58. 280. 124. 128.  28. 136.\n",
      " 275.  26.  16. 128. 134.  64. 225.  32. 186. 220.  52. 154. 184.  54.\n",
      " 114.  18. 220.  94. 130.  60. 295.  74.  60. 164.  10. 198. 138. 200.\n",
      " 128.  74. 350. 285. 102.   2.   4.  92.  86. 154. 190. 500. 186.  42.\n",
      "  62. 275. 110. 124. 116.  98. 275. 265.  28. 182. 198. 225. 170. 154.\n",
      "  86. 235. 300. 240.  84.  36. 150.  14. 300. 250. 164. 160.  78.  22.\n",
      " 300. 230. 112. 118. 118. 250. 178. 500.   0.  90. 120.  80. 186.  90.\n",
      " 450.  74.  64.  24. 140.  36. 215. 450. 350. 136.  68. 290. 220. 116.\n",
      " 300. 290.   8. 120.  48. 188. 164.  86.]\n",
      "mse =  587.6597843251026\n"
     ]
    }
   ],
   "source": [
    "time_str = str(time.time())\n",
    "early_stop = EarlyStopping(monitor='val_mean_squared_error', patience=30)\n",
    "# model.load_weights('./modelWights/regression_model'+model_fn.__name__+'.h5')\n",
    "model_checkpoint = ModelCheckpoint('./modelWights/weights_'+'resnet50_without_pre'+time_str+'_gray_scale.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train_g, y_train, batch_size=BATCH_SIZE, epochs=250, validation_data=(X_val_g, y_val), callbacks=[model_checkpoint])\n",
    "# print(history.history)\n",
    "\n",
    "y_pred_ = model.predict(X_test_g, batch_size=1, verbose=1)\n",
    "\n",
    "time_str = str(time.time())\n",
    "y_pred_, y_test_ = post_training_data(y_test, y_pred_)\n",
    "predicted_file = 'regress_'+\"resnet50_without_pre\"+\"resnet50\"+time_str+'_without_pretrain_gray_scale.csv'\n",
    "save_result(\"./result/\"+predicted_file,y_pred_,y_test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mobilenet without pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 512, 512, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)    (None, 513, 513, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 256, 256, 32)      864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 256, 256, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (ReLU)            (None, 256, 256, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 256, 256, 32)      288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 256, 256, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (ReLU)        (None, 256, 256, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 256, 256, 64)      2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 256, 256, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (ReLU)        (None, 256, 256, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_pad_2 (ZeroPadding2D)   (None, 257, 257, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 128, 128, 64)      576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 128, 128, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (ReLU)        (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 128, 128, 128)     8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 128, 128, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (ReLU)        (None, 128, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 128, 128, 128)     1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 128, 128, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (ReLU)        (None, 128, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 128, 128, 128)     16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 128, 128, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (ReLU)        (None, 128, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv_pad_4 (ZeroPadding2D)   (None, 129, 129, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 64, 64, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 64, 64, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (ReLU)        (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 64, 64, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 64, 64, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (ReLU)        (None, 64, 64, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 64, 64, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 64, 64, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (ReLU)        (None, 64, 64, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 64, 64, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 64, 64, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (ReLU)        (None, 64, 64, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_6 (ZeroPadding2D)   (None, 65, 65, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 32, 32, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 32, 32, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (ReLU)        (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 32, 32, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (ReLU)        (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 32, 32, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (ReLU)        (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 32, 32, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (ReLU)        (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 32, 32, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (ReLU)        (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 32, 32, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (ReLU)        (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 32, 32, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (ReLU)        (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 32, 32, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (ReLU)        (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 32, 32, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (ReLU)       (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 32, 32, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (ReLU)       (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 32, 32, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (ReLU)       (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 32, 32, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (ReLU)       (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)  (None, 33, 33, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 16, 16, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 16, 16, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (ReLU)       (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 16, 16, 1024)      524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 16, 16, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (ReLU)       (None, 16, 16, 1024)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 16, 16, 1024)      9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 16, 16, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (ReLU)       (None, 16, 16, 1024)      0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 16, 16, 1024)      1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 16, 16, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (ReLU)       (None, 16, 16, 1024)      0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_preds (Conv2D)          (None, 1, 1, 1000)        1025000   \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    (None, 1)                 1001      \n",
      "=================================================================\n",
      "Total params: 4,254,865\n",
      "Trainable params: 4,232,977\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pic = X_train_g[1]\n",
    "model = Sequential()\n",
    "#model.add(Input(shape=pic.shape))\n",
    "# model.add(Conv2D(filters=4, kernel_size=(7,7), strides=1, input_shape=pic.shape))\n",
    "Kerasmodel = keras.applications.mobilenet.MobileNet(include_top=True\n",
    "                , weights=None#'imagenet'\n",
    "                # , input_tensor=inputs\n",
    "                , input_shape=pic.shape\n",
    "                )\n",
    "# Kerasmodel._layers.pop()\n",
    "# Kerasmodel.layers.pop()\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(1,kernel_initializer='he_normal'))\n",
    "model.add(Activation('linear')) #softmax\n",
    "model = Model( inputs=Kerasmodel.input , outputs=model(Kerasmodel.layers[-2].output))\n",
    "model.compile(loss='mean_squared_error', #mse binary_crossentropy Dice-coefficient loss function vs cross-entropy\n",
    "                optimizer=optimizers.Adam(lr=5e-3),\n",
    "                metrics=['mse'])\n",
    "\n",
    "model.summary()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mobilenet gray scale without pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1053 samples, validate on 264 samples\n",
      "Epoch 1/250\n",
      "1053/1053 [==============================] - 32s 31ms/step - loss: 0.0260 - mse: 0.0260 - val_loss: 0.0509 - val_mse: 0.0509\n",
      "Epoch 2/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0142 - mse: 0.0142 - val_loss: 0.0176 - val_mse: 0.0176\n",
      "Epoch 3/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0551 - val_mse: 0.0551\n",
      "Epoch 4/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0256 - val_mse: 0.0256\n",
      "Epoch 5/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0128 - val_mse: 0.0128\n",
      "Epoch 6/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0145 - val_mse: 0.0145\n",
      "Epoch 7/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 8/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 9/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 10/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 11/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 12/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0049 - val_mse: 0.0049\n",
      "Epoch 13/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0193 - val_mse: 0.0193\n",
      "Epoch 14/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 15/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 16/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 17/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 18/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 19/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0067 - mse: 0.0067 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 20/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0154 - val_mse: 0.0154\n",
      "Epoch 21/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 22/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0061 - mse: 0.0061 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 23/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0143 - val_mse: 0.0143\n",
      "Epoch 24/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 25/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0319 - val_mse: 0.0319\n",
      "Epoch 26/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 27/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 28/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 29/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 30/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 31/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 32/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0047 - val_mse: 0.0047\n",
      "Epoch 33/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0426 - val_mse: 0.0426\n",
      "Epoch 34/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0272 - val_mse: 0.0272\n",
      "Epoch 35/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0203 - mse: 0.0203 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 36/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 37/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0053 - val_mse: 0.0053\n",
      "Epoch 38/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 39/250\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0080 - mse: 0.0080 - val_loss: 0.0202 - val_mse: 0.0202\n",
      "Epoch 40/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 41/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 42/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0058 - val_mse: 0.0058\n",
      "Epoch 43/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 44/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 45/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 46/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0049 - val_mse: 0.0049\n",
      "Epoch 47/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 48/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0068 - mse: 0.0068 - val_loss: 0.0148 - val_mse: 0.0148\n",
      "Epoch 49/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0058 - val_mse: 0.0058\n",
      "Epoch 50/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 51/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 52/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0071 - val_mse: 0.0071\n",
      "Epoch 53/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0298 - val_mse: 0.0298\n",
      "Epoch 54/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0054 - val_mse: 0.0054\n",
      "Epoch 55/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0399 - val_mse: 0.0399\n",
      "Epoch 56/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 57/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 58/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0234 - val_mse: 0.0234\n",
      "Epoch 59/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 60/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0389 - val_mse: 0.0389\n",
      "Epoch 61/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0101 - val_mse: 0.0101\n",
      "Epoch 62/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 63/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 64/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0240 - val_mse: 0.0240\n",
      "Epoch 65/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0190 - val_mse: 0.0190\n",
      "Epoch 66/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 67/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 68/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 69/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0226 - val_mse: 0.0226\n",
      "Epoch 70/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 71/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0069 - val_mse: 0.0069\n",
      "Epoch 72/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0285 - val_mse: 0.0285\n",
      "Epoch 73/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0061 - mse: 0.0061 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 74/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0061 - mse: 0.0061 - val_loss: 0.0309 - val_mse: 0.0309\n",
      "Epoch 75/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0068 - val_mse: 0.0068\n",
      "Epoch 76/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 77/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 78/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0102 - val_mse: 0.0102\n",
      "Epoch 79/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0048 - val_mse: 0.0048\n",
      "Epoch 80/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 81/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 82/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0069 - val_mse: 0.0069\n",
      "Epoch 83/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0377 - val_mse: 0.0377\n",
      "Epoch 84/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 85/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0417 - val_mse: 0.0417\n",
      "Epoch 86/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 87/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0362 - val_mse: 0.0362\n",
      "Epoch 88/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 89/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 90/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0047 - val_mse: 0.0047\n",
      "Epoch 91/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0048 - val_mse: 0.0048\n",
      "Epoch 92/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 93/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0048 - val_mse: 0.0048\n",
      "Epoch 94/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0313 - val_mse: 0.0313\n",
      "Epoch 95/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 96/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 97/250\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0265 - val_mse: 0.0265\n",
      "Epoch 98/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0276 - val_mse: 0.0276\n",
      "Epoch 99/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0265 - val_mse: 0.0265\n",
      "Epoch 100/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 101/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 102/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 103/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0249 - val_mse: 0.0249\n",
      "Epoch 104/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 105/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0131 - val_mse: 0.0131\n",
      "Epoch 106/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 107/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0398 - val_mse: 0.0398\n",
      "Epoch 108/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 109/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 110/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0448 - val_mse: 0.0448\n",
      "Epoch 111/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0368 - val_mse: 0.0368\n",
      "Epoch 112/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0367 - val_mse: 0.0367\n",
      "Epoch 113/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 114/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 115/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0214 - val_mse: 0.0214\n",
      "Epoch 116/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0134 - val_mse: 0.0134\n",
      "Epoch 117/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 118/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0292 - val_mse: 0.0292\n",
      "Epoch 119/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 120/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0377 - val_mse: 0.0377\n",
      "Epoch 121/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 122/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0411 - val_mse: 0.0411\n",
      "Epoch 123/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0431 - val_mse: 0.0431\n",
      "Epoch 124/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0399 - val_mse: 0.0399\n",
      "Epoch 125/250\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 126/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0164 - val_mse: 0.0164\n",
      "Epoch 127/250\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0515 - val_mse: 0.0515\n",
      "Epoch 128/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 129/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 130/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0070 - val_mse: 0.0070\n",
      "Epoch 131/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 132/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0502 - val_mse: 0.0502\n",
      "Epoch 133/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 134/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0406 - val_mse: 0.0406\n",
      "Epoch 135/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 136/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0064 - val_mse: 0.0064\n",
      "Epoch 137/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 138/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0054 - val_mse: 0.0054\n",
      "Epoch 139/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 140/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 141/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 142/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 143/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0150 - val_mse: 0.0150\n",
      "Epoch 144/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0416 - val_mse: 0.0416\n",
      "Epoch 145/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0395 - val_mse: 0.0395\n",
      "Epoch 146/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0229 - val_mse: 0.0229\n",
      "Epoch 147/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0210 - val_mse: 0.0210\n",
      "Epoch 148/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0107 - val_mse: 0.0107\n",
      "Epoch 149/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0379 - val_mse: 0.0379\n",
      "Epoch 150/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0295 - val_mse: 0.0295\n",
      "Epoch 151/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0266 - val_mse: 0.0266\n",
      "Epoch 152/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 153/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 154/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0369 - val_mse: 0.0369\n",
      "Epoch 155/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0400 - val_mse: 0.0400\n",
      "Epoch 156/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 157/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 158/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 159/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 160/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 161/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 162/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 163/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 164/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0453 - val_mse: 0.0453\n",
      "Epoch 165/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0378 - val_mse: 0.0378\n",
      "Epoch 166/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0131 - val_mse: 0.0131\n",
      "Epoch 167/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 168/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0118 - val_mse: 0.0118\n",
      "Epoch 169/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0155 - val_mse: 0.0155\n",
      "Epoch 170/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0166 - val_mse: 0.0166\n",
      "Epoch 171/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 172/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 173/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0261 - val_mse: 0.0261\n",
      "Epoch 174/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 175/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0346 - val_mse: 0.0346\n",
      "Epoch 176/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 177/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 178/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 179/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 180/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 181/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0275 - val_mse: 0.0275\n",
      "Epoch 182/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 183/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 184/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0537 - val_mse: 0.0537\n",
      "Epoch 185/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0295 - val_mse: 0.0295\n",
      "Epoch 186/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 187/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 188/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0153 - val_mse: 0.0153\n",
      "Epoch 189/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 190/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0277 - val_mse: 0.0277\n",
      "Epoch 191/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0054 - val_mse: 0.0054\n",
      "Epoch 192/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0128 - val_mse: 0.0128\n",
      "Epoch 193/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 194/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 195/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0308 - val_mse: 0.0308\n",
      "Epoch 196/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0561 - val_mse: 0.0561\n",
      "Epoch 197/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0290 - val_mse: 0.0290\n",
      "Epoch 198/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0220 - val_mse: 0.0220\n",
      "Epoch 199/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 200/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 201/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 202/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0283 - val_mse: 0.0283\n",
      "Epoch 203/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 204/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0229 - val_mse: 0.0229\n",
      "Epoch 205/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 206/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0065 - val_mse: 0.0065\n",
      "Epoch 207/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 208/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0291 - val_mse: 0.0291\n",
      "Epoch 209/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0364 - val_mse: 0.0364\n",
      "Epoch 210/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0238 - val_mse: 0.0238\n",
      "Epoch 211/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0209 - val_mse: 0.0209\n",
      "Epoch 212/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0305 - val_mse: 0.0305\n",
      "Epoch 213/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 214/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 215/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 216/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 217/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0243 - val_mse: 0.0243\n",
      "Epoch 218/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 219/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0300 - val_mse: 0.0300\n",
      "Epoch 220/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0268 - val_mse: 0.0268\n",
      "Epoch 221/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 222/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0130 - val_mse: 0.0130\n",
      "Epoch 223/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 224/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 225/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 226/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 227/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0255 - val_mse: 0.0255\n",
      "Epoch 228/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 229/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0123 - val_mse: 0.0123\n",
      "Epoch 230/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 9.4075e-04 - mse: 9.4075e-04 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 231/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 232/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0362 - val_mse: 0.0362\n",
      "Epoch 233/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 234/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 9.5225e-04 - mse: 9.5225e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 235/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 236/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 9.3847e-04 - mse: 9.3847e-04 - val_loss: 0.0015 - val_mse: 0.0015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 237/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 238/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 239/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 240/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0078 - val_mse: 0.0078\n",
      "Epoch 241/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0200 - val_mse: 0.0200\n",
      "Epoch 242/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 243/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 9.5958e-04 - mse: 9.5958e-04 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 244/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0213 - val_mse: 0.0213\n",
      "Epoch 245/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0227 - val_mse: 0.0227\n",
      "Epoch 246/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 247/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 248/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 249/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 250/250\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "330/330 [==============================] - 5s 14ms/step\n",
      "(330,)\n",
      "(330,)\n",
      "[ 30.7253    42.661892 241.24678  231.01028  394.26904  124.00954\n",
      " 222.0862    52.21328  152.60294  150.16183  267.85486   76.115326\n",
      " 139.31189  116.50229  113.72587   62.194424 151.69379  138.57312\n",
      " 261.0329   192.13974  440.9765   234.8123   275.89404   81.43684\n",
      "  84.397934  84.16914  126.4461    37.146687 169.19875  123.640594\n",
      " 132.0359   118.27445   85.32859   46.108955 112.14641  171.40541\n",
      " 289.39282   47.974525  64.84186  386.15662  243.5833   265.32382\n",
      " 157.7375   230.64624  278.30185  288.8308    42.71079   34.790756\n",
      " 106.28996  132.09073   24.403275  96.9365   163.90178   26.97356\n",
      " 180.16634  149.75458  121.10895  239.94595  237.1924    16.177595\n",
      " 215.05759  283.92404   61.350407 385.5038   240.62276  261.81256\n",
      " 227.33525  212.44672  230.37679   69.262856 222.65222   21.239414\n",
      " 169.10909  180.04053   68.06222  209.26305  182.58612  131.54727\n",
      " 118.574394  20.778284 118.678154 313.59482   60.387314  17.71976\n",
      "  23.619577 128.25545  125.77413   75.79366   93.52512   95.90527\n",
      " 237.7319    45.111084 191.23648  160.44395  226.40111  215.71494\n",
      " 117.291084 209.26305   50.251587  75.95767  160.93755  102.10746\n",
      "  86.61717   36.93612   42.622997  57.45178  153.8143   193.28264\n",
      "  72.78679   29.85254   13.340786 180.9276   109.71065   71.47197\n",
      "  98.36406  341.12955   47.441914 258.12857   63.40983  247.51404\n",
      " 227.50304  164.6058   149.71892  236.47824  238.27101  139.85966\n",
      " 256.27444   65.88351   20.441427 136.00525  236.88794  296.29486\n",
      " 407.0392    67.90716   75.2626   200.38295   71.289406  98.122185\n",
      "  33.21597  435.70038   58.648186  83.55535   33.26452  408.86017\n",
      "  24.45273  119.980835 128.30794   98.33195  141.44426  163.6201\n",
      " 273.12204   71.91381   97.13371   36.74035  455.51633   18.698349\n",
      " 136.2828    92.8294    21.537512 122.31142   38.737534  99.40449\n",
      "  95.49058   98.70154   56.391567  40.679188 243.80751  116.526024\n",
      " 214.0676    57.35693   45.6164   120.26113   27.566628 126.95047\n",
      " 106.19969  154.78856  276.9319   127.49104  241.32515  118.82003\n",
      " 379.29092  140.00937  120.18083  135.68655  108.45986  104.83167\n",
      " 149.91182   22.754864 160.48273   69.66713  152.41637  111.73672\n",
      "  46.25506  140.59781  370.8688    18.113554 129.35841   66.44338\n",
      " 102.50045   40.03459  117.78633  151.26851  177.31235  101.17425\n",
      "  63.763977  79.62507  143.95581  176.33728   94.139336  95.68314\n",
      " 215.967    158.87987  146.68315  145.74179  112.261894  98.062836\n",
      " 192.03502  260.9727    77.340744 266.97833  117.79538  130.83743\n",
      "  86.99147  116.48729  258.5095    47.03424   28.901382 114.28242\n",
      " 138.29306   76.80821  223.04431   48.22194  207.88875  196.0265\n",
      "  69.38282   97.8009   184.89682   55.56672  100.54808   24.481565\n",
      " 181.16801  102.41294  133.43776   84.954735 259.88156   59.435234\n",
      "  64.15294  140.5962    22.368267 131.85936  155.41977  187.0675\n",
      " 129.11852   99.1322   334.9303   269.02454  123.16477   16.63038\n",
      "  26.058256  97.73258   85.39884  133.77031  157.34409  445.27368\n",
      " 202.17943   59.90889  101.20875  275.78168  130.26967  107.18164\n",
      " 125.46572  114.745735 239.40248  235.38702   83.83245  130.97307\n",
      " 145.66637  236.47824  167.26372  142.86945  153.01093  220.47568\n",
      " 232.23099  227.00566  103.23246   42.331844 110.390114  35.789654\n",
      " 291.57626  236.99698  151.09326  148.01192   57.810886  35.710545\n",
      " 267.16284  222.25407   99.188896 120.80713  139.3211   243.71123\n",
      " 168.27979  427.35684   24.697527  90.37093  125.75278   79.58042\n",
      " 178.41768  103.79892  388.63504   39.222538  86.933     25.185555\n",
      " 130.63153   51.40365  223.77278  406.07147  310.73087  135.05923\n",
      "  75.384125 286.36096  235.8531   127.48851  272.5435   265.28012\n",
      "  24.766758 128.07999   69.82678  170.81725  151.94391   91.12385 ]\n",
      "[  6.  54. 275. 240. 450. 108. 225.  58. 150. 162. 250.  80. 162.  64.\n",
      " 100.  46. 174. 128. 250. 184. 500. 300. 285.  68.  88.  30. 120.   8.\n",
      " 128. 154. 144. 126.  80.  52. 112. 158. 300.  18.  66. 450. 265. 290.\n",
      " 168. 255. 295. 300.  32.  24.  96. 132.   6.  92. 186.   8. 180. 196.\n",
      " 108. 230. 225.   8. 215. 290.  54. 450. 250. 270. 230. 200. 225.  80.\n",
      " 245.   4. 190. 168.  44. 225. 182. 122. 120.   4. 124. 350.  56.   2.\n",
      "  12.  84.  84.  78.  84.  82. 225.  18. 176. 174. 198. 225. 100. 225.\n",
      "  46.  60. 188.  88.  70.  10.  24.  48. 172. 186.  52.  10.   2. 188.\n",
      " 156.  48. 116. 400.  30. 275.  40. 275. 215. 156. 190. 225. 245. 172.\n",
      " 235. 102.   6. 122. 265. 295. 450.  46.  86. 196.  84. 112.  10. 500.\n",
      "  20.  58.  34. 500.  14.  82. 104.  92. 154. 188. 285. 100. 120.  34.\n",
      " 500.  16. 132.  46.  12. 136.  22.   1. 100.  90.  58.  38. 270. 112.\n",
      " 215.  32.  26. 124.  16. 144.  90. 182. 300. 134. 215. 102. 450. 128.\n",
      "  94. 138.  72. 146. 158.   0. 112.  40. 178.  98.  32. 136. 400.   4.\n",
      " 112.  28. 106.  28.  74. 200. 192.  96.  58.  32. 140. 172.  72.  40.\n",
      " 200. 156. 162. 142. 134.  76. 255. 240.  58. 280. 124. 128.  28. 136.\n",
      " 275.  26.  16. 128. 134.  64. 225.  32. 186. 220.  52. 154. 184.  54.\n",
      " 114.  18. 220.  94. 130.  60. 295.  74.  60. 164.  10. 198. 138. 200.\n",
      " 128.  74. 350. 285. 102.   2.   4.  92.  86. 154. 190. 500. 186.  42.\n",
      "  62. 275. 110. 124. 116.  98. 275. 265.  28. 182. 198. 225. 170. 154.\n",
      "  86. 235. 300. 240.  84.  36. 150.  14. 300. 250. 164. 160.  78.  22.\n",
      " 300. 230. 112. 118. 118. 250. 178. 500.   0.  90. 120.  80. 186.  90.\n",
      " 450.  74.  64.  24. 140.  36. 215. 450. 350. 136.  68. 290. 220. 116.\n",
      " 300. 290.   8. 120.  48. 188. 164.  86.]\n",
      "mse =  666.454539534543\n"
     ]
    }
   ],
   "source": [
    "time_str = str(time.time())\n",
    "early_stop = EarlyStopping(monitor='val_mean_squared_error', patience=30)\n",
    "# model.load_weights('./modelWights/regression_model'+model_fn.__name__+'.h5')\n",
    "model_checkpoint = ModelCheckpoint('./modelWights/weights_'+'mobileNet_without_pre'+time_str+'_gray_scale.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train_g, y_train, batch_size=BATCH_SIZE, epochs=250, validation_data=(X_val_g, y_val), callbacks=[model_checkpoint])\n",
    "# print(history.history)\n",
    "\n",
    "y_pred_ = model.predict(X_test_g, batch_size=1, verbose=1)\n",
    "\n",
    "\n",
    "y_pred_, y_test_ = post_training_data(y_test, y_pred_)\n",
    "predicted_file = 'regress_'+\"mobileNet_without_pre\"+time_str+'_gray_scale.csv'\n",
    "save_result(\"./result/\"+predicted_file,y_pred_,y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.944667350418255"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test_, y_pred_)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mobilenet color without pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1053 samples, validate on 264 samples\n",
      "Epoch 1/250\n",
      "1053/1053 [==============================] - 29s 27ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 2/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 3/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 4/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0047 - val_mse: 0.0047\n",
      "Epoch 5/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 6/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 7/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 8/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 8.9182e-04 - mse: 8.9182e-04 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 9/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 10/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 11/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 9.3887e-04 - mse: 9.3887e-04 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 12/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 13/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 14/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 9.2375e-04 - mse: 9.2375e-04 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 15/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 9.5200e-04 - mse: 9.5200e-04 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 16/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 17/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 7.1096e-04 - mse: 7.1096e-04 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 18/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 9.9396e-04 - mse: 9.9396e-04 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 19/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 9.1151e-04 - mse: 9.1151e-04 - val_loss: 0.0010 - val_mse: 0.0010\n",
      "Epoch 20/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 21/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 9.3226e-04 - mse: 9.3226e-04 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 22/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 7.9541e-04 - mse: 7.9541e-04 - val_loss: 0.0010 - val_mse: 0.0010\n",
      "Epoch 23/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 8.3506e-04 - mse: 8.3506e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 24/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 9.2024e-04 - mse: 9.2024e-04 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 25/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 26/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 9.6636e-04 - mse: 9.6636e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 27/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 9.0739e-04 - mse: 9.0739e-04 - val_loss: 0.0010 - val_mse: 0.0010\n",
      "Epoch 28/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 8.0973e-04 - mse: 8.0973e-04 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 29/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 7.6751e-04 - mse: 7.6751e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 30/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 8.3791e-04 - mse: 8.3791e-04 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 31/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 8.6689e-04 - mse: 8.6689e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 32/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 33/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 8.4705e-04 - mse: 8.4705e-04 - val_loss: 0.0064 - val_mse: 0.0064\n",
      "Epoch 34/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 8.4929e-04 - mse: 8.4929e-04 - val_loss: 0.0010 - val_mse: 0.0010\n",
      "Epoch 35/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 9.0006e-04 - mse: 9.0006e-04 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 36/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 8.3463e-04 - mse: 8.3463e-04 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 37/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 38/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 39/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 7.2142e-04 - mse: 7.2142e-04 - val_loss: 0.0010 - val_mse: 0.0010\n",
      "Epoch 40/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 41/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 9.5842e-04 - mse: 9.5842e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 42/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 43/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 7.3281e-04 - mse: 7.3281e-04 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 44/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 9.4601e-04 - mse: 9.4601e-04 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 45/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 7.5839e-04 - mse: 7.5839e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 46/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 8.1485e-04 - mse: 8.1485e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 47/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 9.4450e-04 - mse: 9.4450e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 48/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 9.4316e-04 - mse: 9.4316e-04 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 49/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 8.8696e-04 - mse: 8.8696e-04 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 50/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 51/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 7.2328e-04 - mse: 7.2328e-04 - val_loss: 9.4936e-04 - val_mse: 9.4936e-04\n",
      "Epoch 52/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 9.1064e-04 - mse: 9.1064e-04 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 53/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 7.6174e-04 - mse: 7.6174e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 54/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.8522e-04 - mse: 6.8522e-04 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 55/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 7.5462e-04 - mse: 7.5462e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 56/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 7.4585e-04 - mse: 7.4585e-04 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 57/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 9.3048e-04 - mse: 9.3048e-04 - val_loss: 0.0014 - val_mse: 0.0014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 9.7956e-04 - mse: 9.7956e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 59/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 8.8268e-04 - mse: 8.8268e-04 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 60/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 61/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 8.6382e-04 - mse: 8.6382e-04 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 62/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 7.2238e-04 - mse: 7.2238e-04 - val_loss: 9.7504e-04 - val_mse: 9.7504e-04\n",
      "Epoch 63/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.8699e-04 - mse: 6.8699e-04 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 64/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 9.1437e-04 - mse: 9.1437e-04 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 65/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 9.5627e-04 - val_mse: 9.5627e-04\n",
      "Epoch 66/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 8.0304e-04 - mse: 8.0304e-04 - val_loss: 0.0010 - val_mse: 0.0010\n",
      "Epoch 67/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 7.2329e-04 - mse: 7.2329e-04 - val_loss: 9.2703e-04 - val_mse: 9.2703e-04\n",
      "Epoch 68/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 5.8293e-04 - mse: 5.8293e-04 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 69/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 6.3756e-04 - mse: 6.3756e-04 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 70/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 7.6170e-04 - mse: 7.6170e-04 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 71/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 9.2081e-04 - mse: 9.2081e-04 - val_loss: 9.9918e-04 - val_mse: 9.9918e-04\n",
      "Epoch 72/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 73/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 7.5115e-04 - mse: 7.5115e-04 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 74/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 8.0271e-04 - mse: 8.0271e-04 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 75/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 76/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 8.3706e-04 - mse: 8.3706e-04 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 77/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 8.2089e-04 - mse: 8.2089e-04 - val_loss: 9.6706e-04 - val_mse: 9.6706e-04\n",
      "Epoch 78/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 7.0104e-04 - mse: 7.0104e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 79/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.9004e-04 - mse: 6.9004e-04 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 80/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 5.6511e-04 - mse: 5.6511e-04 - val_loss: 0.0010 - val_mse: 0.0010\n",
      "Epoch 81/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 8.0941e-04 - mse: 8.0941e-04 - val_loss: 9.9148e-04 - val_mse: 9.9148e-04\n",
      "Epoch 82/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 7.3711e-04 - mse: 7.3711e-04 - val_loss: 9.7374e-04 - val_mse: 9.7374e-04\n",
      "Epoch 83/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 8.6563e-04 - mse: 8.6563e-04 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 84/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 6.6618e-04 - mse: 6.6618e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 85/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.3752e-04 - mse: 5.3752e-04 - val_loss: 9.1072e-04 - val_mse: 9.1072e-04\n",
      "Epoch 86/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 7.3244e-04 - mse: 7.3244e-04 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 87/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 88/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 7.5525e-04 - mse: 7.5525e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 89/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.6096e-04 - mse: 6.6096e-04 - val_loss: 7.7265e-04 - val_mse: 7.7265e-04\n",
      "Epoch 90/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 7.1724e-04 - mse: 7.1724e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 91/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 9.7486e-04 - mse: 9.7486e-04 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 92/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.6922e-04 - mse: 6.6922e-04 - val_loss: 9.0913e-04 - val_mse: 9.0913e-04\n",
      "Epoch 93/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.2445e-04 - mse: 6.2445e-04 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 94/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 7.1175e-04 - mse: 7.1175e-04 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 95/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 7.4996e-04 - mse: 7.4996e-04 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 96/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.3515e-04 - mse: 6.3515e-04 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 97/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.2480e-04 - mse: 6.2480e-04 - val_loss: 0.0010 - val_mse: 0.0010\n",
      "Epoch 98/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.4501e-04 - mse: 5.4501e-04 - val_loss: 9.8284e-04 - val_mse: 9.8284e-04\n",
      "Epoch 99/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 8.7552e-04 - mse: 8.7552e-04 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 100/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 7.3846e-04 - mse: 7.3846e-04 - val_loss: 9.5462e-04 - val_mse: 9.5462e-04\n",
      "Epoch 101/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.8071e-04 - mse: 6.8071e-04 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 102/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.0256e-04 - mse: 6.0256e-04 - val_loss: 9.6203e-04 - val_mse: 9.6203e-04\n",
      "Epoch 103/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.6418e-04 - mse: 6.6418e-04 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 104/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.2276e-04 - mse: 6.2276e-04 - val_loss: 0.0010 - val_mse: 0.0010\n",
      "Epoch 105/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.0543e-04 - mse: 6.0543e-04 - val_loss: 7.9174e-04 - val_mse: 7.9174e-04\n",
      "Epoch 106/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 4.8438e-04 - mse: 4.8438e-04 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 107/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 7.7302e-04 - mse: 7.7302e-04 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 108/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0010 - val_mse: 0.0010\n",
      "Epoch 109/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 5.9275e-04 - mse: 5.9275e-04 - val_loss: 8.2681e-04 - val_mse: 8.2681e-04\n",
      "Epoch 110/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 5.4737e-04 - mse: 5.4737e-04 - val_loss: 7.6580e-04 - val_mse: 7.6580e-04\n",
      "Epoch 111/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.3548e-04 - mse: 5.3548e-04 - val_loss: 8.2950e-04 - val_mse: 8.2950e-04\n",
      "Epoch 112/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 7.5438e-04 - mse: 7.5438e-04 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 113/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1053 [==============================] - 30s 28ms/step - loss: 6.6675e-04 - mse: 6.6675e-04 - val_loss: 0.0010 - val_mse: 0.0010\n",
      "Epoch 114/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 8.0868e-04 - mse: 8.0868e-04 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 115/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.9320e-04 - mse: 5.9320e-04 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 116/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 9.9815e-04 - mse: 9.9815e-04 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 117/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.8012e-04 - mse: 6.8012e-04 - val_loss: 8.7365e-04 - val_mse: 8.7365e-04\n",
      "Epoch 118/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 7.5447e-04 - mse: 7.5447e-04 - val_loss: 9.2848e-04 - val_mse: 9.2848e-04\n",
      "Epoch 119/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.6715e-04 - mse: 4.6715e-04 - val_loss: 8.6794e-04 - val_mse: 8.6794e-04\n",
      "Epoch 120/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.1508e-04 - mse: 6.1508e-04 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 121/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.7290e-04 - mse: 6.7290e-04 - val_loss: 8.3130e-04 - val_mse: 8.3130e-04\n",
      "Epoch 122/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.0997e-04 - mse: 5.0997e-04 - val_loss: 9.9589e-04 - val_mse: 9.9589e-04\n",
      "Epoch 123/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.1841e-04 - mse: 6.1841e-04 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 124/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 7.4913e-04 - mse: 7.4913e-04 - val_loss: 8.5853e-04 - val_mse: 8.5853e-04\n",
      "Epoch 125/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.6872e-04 - mse: 6.6872e-04 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 126/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.3488e-04 - mse: 6.3488e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 127/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 128/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.1753e-04 - mse: 6.1753e-04 - val_loss: 7.7742e-04 - val_mse: 7.7743e-04\n",
      "Epoch 129/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.0210e-04 - mse: 6.0210e-04 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 130/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.5795e-04 - mse: 4.5795e-04 - val_loss: 0.0010 - val_mse: 0.0010\n",
      "Epoch 131/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 7.1370e-04 - mse: 7.1370e-04 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 132/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.3537e-04 - mse: 6.3537e-04 - val_loss: 8.2069e-04 - val_mse: 8.2069e-04\n",
      "Epoch 133/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.2708e-04 - mse: 6.2708e-04 - val_loss: 0.0010 - val_mse: 0.0010\n",
      "Epoch 134/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.5857e-04 - mse: 6.5857e-04 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 135/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 136/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.5647e-04 - mse: 5.5647e-04 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 137/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.2088e-04 - mse: 6.2088e-04 - val_loss: 8.5929e-04 - val_mse: 8.5929e-04\n",
      "Epoch 138/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.0986e-04 - mse: 5.0986e-04 - val_loss: 9.8645e-04 - val_mse: 9.8645e-04\n",
      "Epoch 139/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.7570e-04 - mse: 5.7570e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 140/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 141/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 5.9880e-04 - mse: 5.9880e-04 - val_loss: 0.0010 - val_mse: 0.0010\n",
      "Epoch 142/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.0034e-04 - mse: 6.0034e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 143/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.0352e-04 - mse: 5.0352e-04 - val_loss: 8.6257e-04 - val_mse: 8.6257e-04\n",
      "Epoch 144/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.2757e-04 - mse: 4.2757e-04 - val_loss: 8.6820e-04 - val_mse: 8.6820e-04\n",
      "Epoch 145/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.6126e-04 - mse: 5.6126e-04 - val_loss: 0.0010 - val_mse: 0.0010\n",
      "Epoch 146/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.9668e-04 - mse: 4.9669e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 147/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 9.8099e-04 - mse: 9.8099e-04 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 148/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.0157e-04 - mse: 6.0157e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 149/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.2247e-04 - mse: 5.2247e-04 - val_loss: 7.8872e-04 - val_mse: 7.8872e-04\n",
      "Epoch 150/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 8.4316e-04 - mse: 8.4316e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 151/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.6288e-04 - mse: 4.6288e-04 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 152/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.0489e-04 - mse: 5.0489e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 153/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.1099e-04 - mse: 5.1099e-04 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 154/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.6115e-04 - mse: 5.6115e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 155/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 156/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.3879e-04 - mse: 6.3879e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 157/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.3165e-04 - mse: 5.3165e-04 - val_loss: 9.2049e-04 - val_mse: 9.2049e-04\n",
      "Epoch 158/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.4301e-04 - mse: 5.4301e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 159/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.9735e-04 - mse: 5.9735e-04 - val_loss: 8.3920e-04 - val_mse: 8.3920e-04\n",
      "Epoch 160/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 4.6614e-04 - mse: 4.6614e-04 - val_loss: 9.3092e-04 - val_mse: 9.3092e-04\n",
      "Epoch 161/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.8467e-04 - mse: 5.8467e-04 - val_loss: 8.3825e-04 - val_mse: 8.3825e-04\n",
      "Epoch 162/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 4.6158e-04 - mse: 4.6158e-04 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 163/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 4.6716e-04 - mse: 4.6716e-04 - val_loss: 9.4995e-04 - val_mse: 9.4995e-04\n",
      "Epoch 164/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 4.7399e-04 - mse: 4.7399e-04 - val_loss: 9.9627e-04 - val_mse: 9.9627e-04\n",
      "Epoch 165/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.3212e-04 - mse: 4.3212e-04 - val_loss: 7.8965e-04 - val_mse: 7.8965e-04\n",
      "Epoch 166/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 9.0675e-04 - mse: 9.0675e-04 - val_loss: 0.0010 - val_mse: 0.0010\n",
      "Epoch 167/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.8516e-04 - mse: 5.8516e-04 - val_loss: 0.0016 - val_mse: 0.0016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.9978e-04 - mse: 4.9978e-04 - val_loss: 6.8460e-04 - val_mse: 6.8460e-04\n",
      "Epoch 169/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 7.0613e-04 - mse: 7.0613e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 170/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.9272e-04 - mse: 5.9272e-04 - val_loss: 9.6746e-04 - val_mse: 9.6746e-04\n",
      "Epoch 171/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.8465e-04 - mse: 5.8465e-04 - val_loss: 9.7613e-04 - val_mse: 9.7613e-04\n",
      "Epoch 172/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.6506e-04 - mse: 5.6506e-04 - val_loss: 8.8494e-04 - val_mse: 8.8494e-04\n",
      "Epoch 173/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.7230e-04 - mse: 4.7230e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 174/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.9721e-04 - mse: 5.9721e-04 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 175/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.0409e-04 - mse: 6.0409e-04 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 176/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 5.3776e-04 - mse: 5.3776e-04 - val_loss: 8.6245e-04 - val_mse: 8.6244e-04\n",
      "Epoch 177/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.1451e-04 - mse: 5.1451e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 178/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.7705e-04 - mse: 5.7705e-04 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 179/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 180/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 7.4201e-04 - mse: 7.4201e-04 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 181/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.3329e-04 - mse: 6.3329e-04 - val_loss: 9.3759e-04 - val_mse: 9.3759e-04\n",
      "Epoch 182/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.5291e-04 - mse: 5.5291e-04 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 183/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 5.1618e-04 - mse: 5.1618e-04 - val_loss: 8.6674e-04 - val_mse: 8.6674e-04\n",
      "Epoch 184/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 3.8484e-04 - mse: 3.8484e-04 - val_loss: 9.1923e-04 - val_mse: 9.1923e-04\n",
      "Epoch 185/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.7584e-04 - mse: 4.7584e-04 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 186/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 7.6452e-04 - mse: 7.6452e-04 - val_loss: 8.9693e-04 - val_mse: 8.9693e-04\n",
      "Epoch 187/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.6144e-04 - mse: 5.6144e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 188/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.0701e-04 - mse: 5.0701e-04 - val_loss: 9.2010e-04 - val_mse: 9.2010e-04\n",
      "Epoch 189/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 7.6911e-04 - mse: 7.6911e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 190/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.4415e-04 - mse: 5.4415e-04 - val_loss: 7.9867e-04 - val_mse: 7.9867e-04\n",
      "Epoch 191/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 4.5837e-04 - mse: 4.5837e-04 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 192/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.2345e-04 - mse: 4.2345e-04 - val_loss: 9.5805e-04 - val_mse: 9.5805e-04\n",
      "Epoch 193/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 4.8320e-04 - mse: 4.8320e-04 - val_loss: 8.6887e-04 - val_mse: 8.6887e-04\n",
      "Epoch 194/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.1870e-04 - mse: 5.1870e-04 - val_loss: 8.8855e-04 - val_mse: 8.8855e-04\n",
      "Epoch 195/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.4094e-04 - mse: 6.4094e-04 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 196/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.2813e-04 - mse: 5.2813e-04 - val_loss: 7.6091e-04 - val_mse: 7.6091e-04\n",
      "Epoch 197/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 3.9583e-04 - mse: 3.9583e-04 - val_loss: 8.1758e-04 - val_mse: 8.1758e-04\n",
      "Epoch 198/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 4.5767e-04 - mse: 4.5767e-04 - val_loss: 8.9130e-04 - val_mse: 8.9130e-04\n",
      "Epoch 199/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.6559e-04 - mse: 4.6559e-04 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 200/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 7.9004e-04 - mse: 7.9004e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 201/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.6734e-04 - mse: 5.6734e-04 - val_loss: 8.4505e-04 - val_mse: 8.4505e-04\n",
      "Epoch 202/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.6537e-04 - mse: 5.6537e-04 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 203/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.7945e-04 - mse: 6.7945e-04 - val_loss: 7.7235e-04 - val_mse: 7.7235e-04\n",
      "Epoch 204/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.6348e-04 - mse: 4.6348e-04 - val_loss: 9.7906e-04 - val_mse: 9.7906e-04\n",
      "Epoch 205/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.2295e-04 - mse: 5.2295e-04 - val_loss: 9.3654e-04 - val_mse: 9.3654e-04\n",
      "Epoch 206/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.4542e-04 - mse: 6.4542e-04 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 207/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.4365e-04 - mse: 5.4365e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 208/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.8840e-04 - mse: 6.8840e-04 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 209/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 5.0493e-04 - mse: 5.0493e-04 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 210/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.4061e-04 - mse: 5.4061e-04 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 211/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 5.4998e-04 - mse: 5.4998e-04 - val_loss: 8.5038e-04 - val_mse: 8.5038e-04\n",
      "Epoch 212/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 3.4879e-04 - mse: 3.4879e-04 - val_loss: 8.3362e-04 - val_mse: 8.3362e-04\n",
      "Epoch 213/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.6466e-04 - mse: 4.6466e-04 - val_loss: 8.6079e-04 - val_mse: 8.6079e-04\n",
      "Epoch 214/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 3.6651e-04 - mse: 3.6651e-04 - val_loss: 8.9816e-04 - val_mse: 8.9816e-04\n",
      "Epoch 215/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.8243e-04 - mse: 4.8243e-04 - val_loss: 7.6619e-04 - val_mse: 7.6619e-04\n",
      "Epoch 216/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.2860e-04 - mse: 4.2860e-04 - val_loss: 7.0262e-04 - val_mse: 7.0262e-04\n",
      "Epoch 217/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.6710e-04 - mse: 4.6710e-04 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 218/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.9565e-04 - mse: 5.9565e-04 - val_loss: 0.0047 - val_mse: 0.0047\n",
      "Epoch 219/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 7.6148e-04 - mse: 7.6148e-04 - val_loss: 8.0406e-04 - val_mse: 8.0406e-04\n",
      "Epoch 220/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.4708e-04 - mse: 4.4708e-04 - val_loss: 7.1096e-04 - val_mse: 7.1096e-04\n",
      "Epoch 221/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.2424e-04 - mse: 5.2424e-04 - val_loss: 8.9496e-04 - val_mse: 8.9496e-04\n",
      "Epoch 222/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.2924e-04 - mse: 6.2924e-04 - val_loss: 9.8397e-04 - val_mse: 9.8397e-04\n",
      "Epoch 223/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 5.7568e-04 - mse: 5.7568e-04 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 224/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.9448e-04 - mse: 4.9448e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 225/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.2554e-04 - mse: 4.2554e-04 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 226/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.6666e-04 - mse: 4.6666e-04 - val_loss: 8.5903e-04 - val_mse: 8.5903e-04\n",
      "Epoch 227/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 228/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 6.8868e-04 - mse: 6.8868e-04 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 229/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.0942e-04 - mse: 4.0942e-04 - val_loss: 0.0010 - val_mse: 0.0010\n",
      "Epoch 230/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.7693e-04 - mse: 4.7693e-04 - val_loss: 9.9226e-04 - val_mse: 9.9226e-04\n",
      "Epoch 231/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 3.4672e-04 - mse: 3.4672e-04 - val_loss: 9.9709e-04 - val_mse: 9.9709e-04\n",
      "Epoch 232/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.2508e-04 - mse: 4.2508e-04 - val_loss: 8.6077e-04 - val_mse: 8.6077e-04\n",
      "Epoch 233/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 4.0140e-04 - mse: 4.0140e-04 - val_loss: 9.5801e-04 - val_mse: 9.5801e-04\n",
      "Epoch 234/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.7609e-04 - mse: 4.7609e-04 - val_loss: 9.7733e-04 - val_mse: 9.7733e-04\n",
      "Epoch 235/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0010 - val_mse: 0.0010\n",
      "Epoch 236/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.5196e-04 - mse: 4.5196e-04 - val_loss: 9.0399e-04 - val_mse: 9.0399e-04\n",
      "Epoch 237/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.8070e-04 - mse: 4.8070e-04 - val_loss: 9.3206e-04 - val_mse: 9.3206e-04\n",
      "Epoch 238/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 3.5053e-04 - mse: 3.5053e-04 - val_loss: 8.7040e-04 - val_mse: 8.7040e-04\n",
      "Epoch 239/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 3.2102e-04 - mse: 3.2102e-04 - val_loss: 7.9069e-04 - val_mse: 7.9069e-04\n",
      "Epoch 240/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 3.7057e-04 - mse: 3.7057e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 241/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.0034e-04 - mse: 4.0034e-04 - val_loss: 7.9147e-04 - val_mse: 7.9147e-04\n",
      "Epoch 242/250\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 4.2091e-04 - mse: 4.2091e-04 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 243/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.7639e-04 - mse: 4.7639e-04 - val_loss: 9.8715e-04 - val_mse: 9.8715e-04\n",
      "Epoch 244/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.3052e-04 - mse: 4.3052e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 245/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 6.8041e-04 - mse: 6.8041e-04 - val_loss: 8.0724e-04 - val_mse: 8.0724e-04\n",
      "Epoch 246/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.6937e-04 - mse: 4.6937e-04 - val_loss: 8.7984e-04 - val_mse: 8.7984e-04\n",
      "Epoch 247/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.2029e-04 - mse: 4.2029e-04 - val_loss: 8.3253e-04 - val_mse: 8.3253e-04\n",
      "Epoch 248/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.8887e-04 - mse: 4.8887e-04 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 249/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.8137e-04 - mse: 4.8137e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 250/250\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 4.4337e-04 - mse: 4.4337e-04 - val_loss: 8.6794e-04 - val_mse: 8.6794e-04\n",
      "330/330 [==============================] - 4s 12ms/step\n",
      "(330,)\n",
      "(330,)\n",
      "[ -2.165079    35.931675   275.8517     247.24489    448.0586\n",
      " 109.36606    228.77425     54.25783    152.05338    153.25034\n",
      " 263.20898     69.75164    132.08571     52.378647    99.04268\n",
      "  46.57513    166.89879    132.152      249.49344    195.67604\n",
      " 507.95914    272.4241     277.67215     60.46927     56.094482\n",
      "  30.716822   119.73239     -0.7239133  124.12724    130.15778\n",
      " 147.2784     114.012375   104.11805     50.247536    91.2518\n",
      " 162.76097    296.44238     12.328386    43.966057   456.5466\n",
      " 272.48746    275.8523     170.24736    239.6921     285.88907\n",
      " 315.51367     24.036572    -9.671598    87.398674   140.40384\n",
      "  -5.0055237   97.77371    183.70865      8.5969715  184.86021\n",
      " 220.39705    107.59364    244.48802    231.22186     -2.9101224\n",
      " 214.14296    292.65845     34.347088   467.05664    260.81842\n",
      " 282.78152    251.8602     201.85551    254.5474      84.21539\n",
      " 238.31424     -6.4736156  192.65013    146.50603     54.72782\n",
      " 233.42284    164.18417    128.22908    115.424706   -13.704106\n",
      " 106.92448    396.376       33.462524    -8.891434     5.4231286\n",
      "  62.312       68.007835    75.52366     83.29142     51.077843\n",
      " 222.77536      8.381203   193.63158    160.95924    201.42267\n",
      " 225.22751     94.143524   233.42284     31.839804    55.16059\n",
      " 196.0942      97.30114     69.93789      3.3514202   16.261206\n",
      "  26.791632   171.58127    184.71387     43.60293     -0.7174015\n",
      "  -8.421868   185.00172    155.27814     29.243364   110.833374\n",
      " 399.39792      5.6743026  273.90164     31.237856   274.86838\n",
      " 260.4915     160.73752    171.90953    236.83563    253.45966\n",
      " 159.07562    264.11176     59.486763   -18.293724   123.17272\n",
      " 263.89813    294.17746    465.17044     53.697086    57.883926\n",
      " 193.24843     47.718784   116.87284     17.30974    510.1908\n",
      "  10.89254     63.824535    11.9437275  511.10733     -3.9874763\n",
      "  84.568016   110.275406    70.64329    132.3457     193.85535\n",
      " 293.2524      57.06759     99.295364    14.899627   506.17545\n",
      "   4.7071576  131.82452     44.693893    12.5371065  135.16472\n",
      "  19.010723   164.40027     85.81121    107.4949      46.337105\n",
      "  40.307312   250.23833    107.045876   212.63707     14.845848\n",
      "  13.607532   107.043625     7.4206142  145.63005     85.54811\n",
      " 193.35199    307.57312    144.41782    256.32193    108.10882\n",
      " 450.31644    153.38544     85.85861    116.24781     67.1013\n",
      " 119.95065    171.48747     -1.3676733  109.60776     49.94005\n",
      " 169.93942    107.900375    -3.869474   127.63208    397.72064\n",
      "  -9.169475   109.683914    49.89205     95.36275     23.523048\n",
      "  85.41771    188.15462    171.46223     99.644424    37.151573\n",
      "  57.089813   143.06358    187.04387     65.65337     50.057755\n",
      " 215.66953    158.92337    167.99272    122.15364    115.69925\n",
      "  73.15029    232.09059    258.53        40.0227     279.83826\n",
      "  96.118004   124.30772     33.347904   124.05521    278.11435\n",
      "  13.744086    -9.548619   132.51599    127.15443     62.215275\n",
      " 239.24286     18.895418   196.66013    217.86182     42.133263\n",
      " 113.84605    182.67657     47.220325   112.31117     12.0794325\n",
      " 215.01573     66.7673     119.02953     48.969643   331.72925\n",
      "  54.44841     47.018505   148.58545     -0.71927905 139.4256\n",
      " 143.91443    191.99413    123.48128     62.897026   359.81357\n",
      " 293.7355     102.9771      -7.5423865   -1.9395351   76.5619\n",
      "  57.456417   134.77132    182.43068    519.06934    199.34503\n",
      "  32.71538     74.085365   291.5988     117.27315     93.10232\n",
      " 110.32095    102.71763    272.33716    259.1169      17.535225\n",
      " 162.00093    183.1264     236.83563    175.88106    130.78557\n",
      " 105.00544    237.64903    301.51685    234.17773     71.62446\n",
      "   5.364716   133.50761      3.3796577  310.91846    256.26834\n",
      " 167.74832    140.37712     50.85443      5.9419723  308.67926\n",
      " 222.4895     107.14312    118.27283    138.27553    261.0132\n",
      " 179.73999    496.0074      -0.64375997  80.956955   112.920525\n",
      "  76.266716   187.85146     84.08866    480.43448     57.022556\n",
      "  64.406975     3.3177586  145.67184     11.129409   244.90611\n",
      " 456.2254     370.6301     130.16736     57.081535   298.33032\n",
      " 232.72488    135.99236    288.8186     282.02414     -1.7517656\n",
      " 159.30862     27.77098    177.44748    180.1911      72.49936   ]\n",
      "[  6.  54. 275. 240. 450. 108. 225.  58. 150. 162. 250.  80. 162.  64.\n",
      " 100.  46. 174. 128. 250. 184. 500. 300. 285.  68.  88.  30. 120.   8.\n",
      " 128. 154. 144. 126.  80.  52. 112. 158. 300.  18.  66. 450. 265. 290.\n",
      " 168. 255. 295. 300.  32.  24.  96. 132.   6.  92. 186.   8. 180. 196.\n",
      " 108. 230. 225.   8. 215. 290.  54. 450. 250. 270. 230. 200. 225.  80.\n",
      " 245.   4. 190. 168.  44. 225. 182. 122. 120.   4. 124. 350.  56.   2.\n",
      "  12.  84.  84.  78.  84.  82. 225.  18. 176. 174. 198. 225. 100. 225.\n",
      "  46.  60. 188.  88.  70.  10.  24.  48. 172. 186.  52.  10.   2. 188.\n",
      " 156.  48. 116. 400.  30. 275.  40. 275. 215. 156. 190. 225. 245. 172.\n",
      " 235. 102.   6. 122. 265. 295. 450.  46.  86. 196.  84. 112.  10. 500.\n",
      "  20.  58.  34. 500.  14.  82. 104.  92. 154. 188. 285. 100. 120.  34.\n",
      " 500.  16. 132.  46.  12. 136.  22.   1. 100.  90.  58.  38. 270. 112.\n",
      " 215.  32.  26. 124.  16. 144.  90. 182. 300. 134. 215. 102. 450. 128.\n",
      "  94. 138.  72. 146. 158.   0. 112.  40. 178.  98.  32. 136. 400.   4.\n",
      " 112.  28. 106.  28.  74. 200. 192.  96.  58.  32. 140. 172.  72.  40.\n",
      " 200. 156. 162. 142. 134.  76. 255. 240.  58. 280. 124. 128.  28. 136.\n",
      " 275.  26.  16. 128. 134.  64. 225.  32. 186. 220.  52. 154. 184.  54.\n",
      " 114.  18. 220.  94. 130.  60. 295.  74.  60. 164.  10. 198. 138. 200.\n",
      " 128.  74. 350. 285. 102.   2.   4.  92.  86. 154. 190. 500. 186.  42.\n",
      "  62. 275. 110. 124. 116.  98. 275. 265.  28. 182. 198. 225. 170. 154.\n",
      "  86. 235. 300. 240.  84.  36. 150.  14. 300. 250. 164. 160.  78.  22.\n",
      " 300. 230. 112. 118. 118. 250. 178. 500.   0.  90. 120.  80. 186.  90.\n",
      " 450.  74.  64.  24. 140.  36. 215. 450. 350. 136.  68. 290. 220. 116.\n",
      " 300. 290.   8. 120.  48. 188. 164.  86.]\n",
      "mse =  299.19144723726714\n"
     ]
    }
   ],
   "source": [
    "time_str = str(time.time())\n",
    "early_stop = EarlyStopping(monitor='val_mean_squared_error', patience=30)\n",
    "# model.load_weights('./modelWights/regression_model'+model_fn.__name__+'.h5')\n",
    "model_checkpoint = ModelCheckpoint('./modelWights/weights_'+'mobileNet_without_pre'+time_str+'_color.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=250, validation_data=(X_val, y_val), callbacks=[model_checkpoint])\n",
    "# print(history.history)\n",
    "\n",
    "y_pred_ = model.predict(X_test, batch_size=1, verbose=1)\n",
    "\n",
    "\n",
    "y_pred_, y_test_ = post_training_data(y_test, y_pred_)\n",
    "predicted_file = 'regress_'+\"mobileNet_without_pre\"+time_str+'_color.csv'\n",
    "save_result(\"./result/\"+predicted_file,y_pred_,y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_batch = np.expand_dims(pic, axis=0)\n",
    "layer_1 = K.function([model.layers[0].input], [model.layers[0].output])\n",
    "f1 = layer_1([pic_batch])[0]\n",
    "#number of your filters\n",
    "for _ in range(4):\n",
    "    show_img = f1[:, :, :, _]\n",
    "    show_img.shape = [506, 506]\n",
    "    print(show_img)\n",
    "    plt.subplot(2, 2, _+1)\n",
    "    plt.imshow(show_img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
