{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import initializers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Conv2DTranspose, Dense, Dropout, Activation, Flatten\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import keras_applications\n",
    "\n",
    "import sklearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "import keras.applications.resnet50\n",
    "from keras.applications.resnet50 import ResNet50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.client.session.Session object at 0x7fd066862d30>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction=0.5\n",
    "\n",
    "# gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "# sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "KTF.set_session(sess)\n",
    "\n",
    "tf.keras.backend.set_session(sess)\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "# IMPORTANT: models have to be loaded AFTER SETTING THE SESSION for keras! \n",
    "# Otherwise, their weights will be unavailable in the threads after the session there has been set\n",
    "set_session(sess)\n",
    "\n",
    "img_height = 512\n",
    "img_width = 512\n",
    "nb_channels = 3 #1 3\n",
    "\n",
    "BATCH_SIZE = 4 #8 16 1\n",
    "RANDOM_STATE = random_state = 42\n",
    "time_str = str(time.time())\n",
    "\n",
    "BIN_SIZE = 20#20\n",
    "MAX_LIM = 200#200\n",
    "MIN_LIM = 20\n",
    "nb_bins = (MAX_LIM - MIN_LIM)/BIN_SIZE\n",
    "\n",
    "IS_SCALE = False\n",
    "nb_epochs = 400\n",
    "print(tf.Session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('WaterQuality/X_train_ratio=0.2.npy')\n",
    "X_test = np.load('WaterQuality/X_test_ratio=0.2.npy')\n",
    "y_train = np.load('WaterQuality/y_train_ratio=0.2.npy')\n",
    "y_test = np.load('WaterQuality/y_test_ratio=0.2.npy')\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=True, random_state=random_state)\n",
    "scaler = preprocessing.MaxAbsScaler()#StandardScaler\n",
    "y_train = scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "y_train = y_train.flatten()\n",
    "\n",
    "joblib.dump(scaler, 'MaxAbsScaler.pkl')\n",
    "\n",
    "# scaler_val = preprocessing.MaxAbsScaler()\n",
    "y_val = scaler.transform(y_val.reshape(-1, 1))\n",
    "y_val = y_val.flatten()\n",
    "\n",
    "\n",
    "y_test = scaler.transform(y_test.reshape(-1, 1))\n",
    "y_test = y_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_g = np.zeros(X_train.shape[:-1])\n",
    "X_val_g = np.zeros(X_val.shape[:-1])\n",
    "X_test_g = np.zeros(X_test.shape[:-1])\n",
    "for i in range(X_train.shape[0]):\n",
    "    X_train_g[i] = cv2.cvtColor(X_train[i], cv2.COLOR_RGB2GRAY)\n",
    "for i in range(X_val.shape[0]):\n",
    "    X_val_g[i] = cv2.cvtColor(X_val[i], cv2.COLOR_RGB2GRAY)\n",
    "for i in range(X_test.shape[0]):\n",
    "    X_test_g[i] = cv2.cvtColor(X_test[i], cv2.COLOR_RGB2GRAY)\n",
    "X_train_g = np.stack([X_train_g]*3, axis=-1)\n",
    "X_val_g = np.stack([X_val_g]*3, axis=-1)\n",
    "X_test_g = np.stack([X_test_g]*3, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_models(train_shape=None, input_shape=None, model_name=\"resnet50\", **kwargs):\n",
    "    \"\"\"\n",
    "    model_name = \"inception_v3\", \"mobilenet\", or \"densenet\" \"resnet50\" \"resnet101\"\n",
    "    \"\"\"\n",
    "\n",
    "    def inner( input_shape=input_shape, model_name=model_name,is_classfication=False, **kwargs):\n",
    "#         nonlocal model\n",
    "#         nonlocal input_shape\n",
    "        if input_shape is None and train_shape is not None:\n",
    "            input_shape = train_shape[1:]\n",
    "        else:\n",
    "            input_shape = (img_width, img_height, nb_channels)\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        if model_name == 'inception_v3':\n",
    "            Kerasmodel = keras.applications.inception_v3.InceptionV3(include_top=False\n",
    "                , weights='imagenet'\n",
    "                # , input_tensor=inputs\n",
    "                , input_shape=input_shape\n",
    "                , pooling=None)\n",
    "        if model_name == 'mobilenet':\n",
    "            Kerasmodel = keras.applications.mobilenet.MobileNet(include_top=False\n",
    "                , weights='imagenet'\n",
    "                # , input_tensor=inputs\n",
    "                , input_shape=input_shape\n",
    "                , pooling=None)\n",
    "        if model_name == 'densenet':\n",
    "            Kerasmodel = keras.applications.densenet.DenseNet121(include_top=False\n",
    "                , weights='imagenet'\n",
    "                # , input_tensor=inputs\n",
    "                , input_shape=input_shape\n",
    "                , pooling=None)\n",
    "        if model_name == 'resnet50':\n",
    "            Kerasmodel = keras.applications.resnet50.ResNet50(include_top=False\n",
    "                , weights='imagenet'\n",
    "                # , input_tensor=inputs\n",
    "                , input_shape=input_shape\n",
    "                , pooling=None)\n",
    "        if model_name == 'resnet101':\n",
    "            Kerasmodel = keras.applications.resnet101.ResNet101(include_top=False\n",
    "                , weights='imagenet'\n",
    "                # , input_tensor=inputs\n",
    "                , input_shape=input_shape\n",
    "                , pooling=None)\n",
    "        model.add(Flatten())\n",
    "        # model.add(Dropout(0.5))\n",
    "#         model.add(Dense(256, kernel_initializer='he_normal', use_bias=True, kernel_regularizer=l2(0.2)))\n",
    "#         model.add(Dropout(0.5))\n",
    "\n",
    "        if is_classfication == False:\n",
    "            print('categorical false')\n",
    "            model.add(Dense(1,kernel_initializer='he_normal'))\n",
    "            model.add(Activation('linear')) #softmax\n",
    "            model = Model( inputs=Kerasmodel.input , outputs=model(Kerasmodel.output) )\n",
    "            # model = Model( inputs=inputs, outputs=result )\n",
    "            model.compile(loss='mean_squared_error', #mse binary_crossentropy Dice-coefficient loss function vs cross-entropy\n",
    "                optimizer=optimizers.Adam(lr=5e-3),\n",
    "                metrics=['mse'])\n",
    "        elif is_classfication == True:\n",
    "            print('categorical true')\n",
    "            model.add(Dense(int(kwargs['nb_classes']), kernel_initializer='he_normal'))\n",
    "            model.add(Activation('sigmoid')) #softmax\n",
    "            model = Model( inputs=Kerasmodel.input , outputs=model(Kerasmodel.output) )\n",
    "\n",
    "            model.compile(loss='binary_crossentropy',\n",
    "                optimizer=optimizers.Adam(lr=5e-3),\n",
    "                metrics=['accuracy'])\n",
    "        else:\n",
    "            raise Exception('No model returned')\n",
    "        print(model.summary())\n",
    "        return model\n",
    "    return inner\n",
    "\n",
    "\n",
    "def run_model(train, label, vali_train, vali_label, test, test_label, model_name=\"resnet50\",  model_fn = VGG16, is_classfication=False, fold=1, nb_classes=1):\n",
    "    model = model_fn( is_classfication=is_classfication, nb_classes=nb_classes)\n",
    "    early_stop = EarlyStopping(monitor='val_mean_squared_error', patience=30)\n",
    "    # model.load_weights('./modelWights/regression_model'+model_fn.__name__+'.h5')\n",
    "    model_checkpoint = ModelCheckpoint('./modelWights/weights_'+model_name+'_fold_'+str(fold)+time_str+'_gray_scale.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "    history = model.fit(train, label, batch_size=BATCH_SIZE, epochs=nb_epochs, validation_data=(vali_train, vali_label), callbacks=[model_checkpoint])\n",
    "    # print(history.history)\n",
    "    import json\n",
    "    with open('./modelWights/history.json', 'w') as f:\n",
    "        json.dump(history.history, f)\n",
    "    y_pred = model.predict(test, batch_size=1, verbose=1)\n",
    "\n",
    "    y_test = test_label\n",
    "    \n",
    "    return y_pred, y_test\n",
    "\n",
    "def post_training_data(y_test, y_pred, is_scaler=True ):\n",
    "    if not is_scaler:\n",
    "        y_test = y_test\n",
    "        y_pred = y_pred\n",
    "    else:\n",
    "        scaler_val = joblib.load('MaxAbsScaler.pkl')\n",
    "        y_test, y_pred = transform_y(y_test, y_pred, scaler_val)\n",
    "\n",
    "    np.save('y_pred.npy', y_pred)\n",
    "    np.save('y_test_transformed.npy', y_test)\n",
    "    print(y_pred.shape)###\n",
    "    print(y_test.shape)###\n",
    "    y_pred = y_pred.flatten()\n",
    "    print(y_pred)###\n",
    "    print(y_test)###\n",
    "    # y_test = y_test.flatten()\n",
    "\n",
    "    # if is_categorical:\n",
    "    #     y_pred = y_pred.reshape(-1,2)\n",
    "    #     y_test = y_test[:,0]\n",
    "    #     y_pred = y_pred[:,0]\n",
    "\n",
    "    ########### margin cut off ###########\n",
    "    y_pred[np.where(y_pred>500)  ] = 500\n",
    "    y_pred[np.where(y_pred<0) ] = 0\n",
    "    return y_pred, y_test\n",
    "\n",
    "def transform_y(y_test, y_pred, scaler_test):\n",
    "    y_test = y_test.reshape(-1, 1)\n",
    "    y_pred = y_pred.reshape(-1, 1)\n",
    "    y_test = scaler_test.inverse_transform(y_test)\n",
    "    y_pred = scaler_test.inverse_transform(y_pred)\n",
    "    y_test = y_test.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "    return y_test, y_pred\n",
    "\n",
    "def save_result(name,y_pred,y_true):\n",
    "    with open(name,\"a\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        y_diff = [(y_pred[i] - y_true[i]) for i in range(len(y_pred))]\n",
    "        y = [y_pred, y_true, y_diff]\n",
    "        mse = 0.0\n",
    "        for num in range(len(y_diff)):\n",
    "            mse += pow(y_diff[num],2) / len(y_diff)\n",
    "        print('mse = ', mse)\n",
    "        writer.writerows(y)\n",
    "        \n",
    "def reinitLayers(model):\n",
    "    session = K.get_session()\n",
    "    for layer in model.layers: \n",
    "        if isinstance(layer, keras.engine.network.Network):\n",
    "            reinitLayers(layer)\n",
    "            continue\n",
    "        print(\"LAYER::\", layer.name)\n",
    "        for v in layer.__dict__:\n",
    "            v_arg = getattr(layer,v)\n",
    "            if hasattr(v_arg,'initializer'):\n",
    "                initializer_method = getattr(v_arg, 'initializer')\n",
    "                initializer_method.run(session=session)\n",
    "                print('reinitializing layer {}.{}'.format(layer.name, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = pretrained_models(model_name=\"resnet50\") #dnn_model VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical false\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 512, 512, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 518, 518, 3)  0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 256, 256, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 256, 256, 64) 256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 256, 256, 64) 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 258, 258, 64) 0           activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 128, 128, 64) 0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 128, 128, 64) 4160        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 128, 128, 64) 256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 128, 128, 64) 0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 128, 128, 64) 36928       activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 128, 128, 64) 256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 128, 128, 64) 0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 128, 128, 256 16640       activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 128, 128, 256 16640       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 128, 128, 256 1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 128, 128, 256 1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 128, 128, 256 0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 128, 128, 256 0           add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 128, 128, 64) 16448       activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 128, 128, 64) 256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 128, 128, 64) 0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 128, 128, 64) 36928       activation_105[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 128, 128, 64) 256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 128, 128, 64) 0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 128, 128, 256 16640       activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 128, 128, 256 1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 128, 128, 256 0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, 128, 128, 256 0           add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 128, 128, 64) 16448       activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 128, 128, 64) 256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 128, 128, 64) 0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 128, 128, 64) 36928       activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 128, 128, 64) 256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 128, 128, 64) 0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 128, 128, 256 16640       activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 128, 128, 256 1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 128, 128, 256 0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 128, 128, 256 0           add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 64, 64, 128)  32896       activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, 64, 64, 128)  0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, 64, 64, 128)  0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_112[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 64, 64, 512)  131584      activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 64, 64, 512)  2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 64, 64, 512)  0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, 64, 64, 512)  0           add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 64, 64, 128)  65664       activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, 64, 64, 128)  0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 64, 64, 128)  0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, 64, 64, 512)  0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 64, 64, 512)  0           add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 64, 64, 128)  65664       activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 64, 64, 128)  0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 64, 64, 128)  0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (None, 64, 64, 512)  0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 64, 64, 512)  0           add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 64, 64, 128)  65664       activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 64, 64, 128)  0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 64, 64, 128)  0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (None, 64, 64, 512)  0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 64, 64, 512)  0           add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 32, 32, 256)  131328      activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 32, 32, 256)  0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 32, 32, 256)  0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_124[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 32, 32, 1024) 525312      activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 32, 32, 1024) 4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (None, 32, 32, 1024) 0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 32, 32, 1024) 0           add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, 32, 32, 256)  0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, 32, 32, 256)  0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_41 (Add)                    (None, 32, 32, 1024) 0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, 32, 32, 1024) 0           add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, 32, 32, 256)  0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, 32, 32, 256)  0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_42 (Add)                    (None, 32, 32, 1024) 0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, 32, 32, 1024) 0           add_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 32, 32, 256)  0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, 32, 32, 256)  0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_133[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_43 (Add)                    (None, 32, 32, 1024) 0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 32, 32, 1024) 0           add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 32, 32, 256)  0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 32, 32, 256)  0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_44 (Add)                    (None, 32, 32, 1024) 0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 32, 32, 1024) 0           add_44[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, 32, 32, 256)  0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 32, 32, 256)  0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_45 (Add)                    (None, 32, 32, 1024) 0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, 32, 32, 1024) 0           add_45[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 16, 16, 512)  524800      activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 16, 16, 512)  2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, 16, 16, 512)  0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 16, 16, 512)  2359808     activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 16, 16, 512)  2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 16, 16, 512)  0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 16, 16, 2048) 1050624     activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 16, 16, 2048) 2099200     activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, 16, 16, 2048) 8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 16, 16, 2048) 8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_46 (Add)                    (None, 16, 16, 2048) 0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 16, 16, 2048) 0           add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 16, 16, 512)  1049088     activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 16, 16, 512)  2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 16, 16, 512)  0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 16, 16, 512)  2359808     activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 16, 16, 512)  2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, 16, 16, 512)  0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 16, 16, 2048) 1050624     activation_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, 16, 16, 2048) 8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_47 (Add)                    (None, 16, 16, 2048) 0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, 16, 16, 2048) 0           add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 16, 16, 512)  1049088     activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 16, 16, 512)  2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, 16, 16, 512)  0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 16, 16, 512)  2359808     activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 16, 16, 512)  2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, 16, 16, 512)  0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, 16, 16, 2048) 1050624     activation_148[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, 16, 16, 2048) 8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_48 (Add)                    (None, 16, 16, 2048) 0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, 16, 16, 2048) 0           add_48[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_4 (Sequential)       (None, 1)            524289      activation_149[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 24,112,001\n",
      "Trainable params: 24,058,881\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1053 samples, validate on 264 samples\n",
      "Epoch 1/400\n",
      "1053/1053 [==============================] - 115s 109ms/step - loss: 803.4075 - mean_squared_error: 803.4075 - val_loss: 92.9796 - val_mean_squared_error: 92.9796\n",
      "Epoch 2/400\n",
      "1053/1053 [==============================] - 88s 84ms/step - loss: 25.6277 - mean_squared_error: 25.6277 - val_loss: 41.8336 - val_mean_squared_error: 41.8336\n",
      "Epoch 3/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 4.2023 - mean_squared_error: 4.2023 - val_loss: 8.8229 - val_mean_squared_error: 8.8229\n",
      "Epoch 4/400\n",
      "1053/1053 [==============================] - 83s 78ms/step - loss: 0.9280 - mean_squared_error: 0.9280 - val_loss: 10.2148 - val_mean_squared_error: 10.2148\n",
      "Epoch 5/400\n",
      "1053/1053 [==============================] - 82s 78ms/step - loss: 1.5310 - mean_squared_error: 1.5310 - val_loss: 11.9949 - val_mean_squared_error: 11.9949\n",
      "Epoch 6/400\n",
      "1053/1053 [==============================] - 81s 77ms/step - loss: 9.8627 - mean_squared_error: 9.8627 - val_loss: 7.0885 - val_mean_squared_error: 7.0885\n",
      "Epoch 7/400\n",
      "1053/1053 [==============================] - 85s 80ms/step - loss: 0.5648 - mean_squared_error: 0.5648 - val_loss: 1.4428 - val_mean_squared_error: 1.4428\n",
      "Epoch 8/400\n",
      "1053/1053 [==============================] - 83s 79ms/step - loss: 0.4405 - mean_squared_error: 0.4405 - val_loss: 3.1514 - val_mean_squared_error: 3.1514\n",
      "Epoch 9/400\n",
      "1053/1053 [==============================] - 86s 81ms/step - loss: 0.8034 - mean_squared_error: 0.8034 - val_loss: 4.5396 - val_mean_squared_error: 4.5396\n",
      "Epoch 10/400\n",
      "1053/1053 [==============================] - 82s 78ms/step - loss: 0.6885 - mean_squared_error: 0.6885 - val_loss: 0.7417 - val_mean_squared_error: 0.7417\n",
      "Epoch 11/400\n",
      "1053/1053 [==============================] - 82s 78ms/step - loss: 0.4505 - mean_squared_error: 0.4505 - val_loss: 0.5055 - val_mean_squared_error: 0.5055\n",
      "Epoch 12/400\n",
      "1053/1053 [==============================] - 83s 79ms/step - loss: 1.0262 - mean_squared_error: 1.0262 - val_loss: 0.3675 - val_mean_squared_error: 0.3675\n",
      "Epoch 13/400\n",
      "1053/1053 [==============================] - 91s 87ms/step - loss: 0.2168 - mean_squared_error: 0.2168 - val_loss: 0.8514 - val_mean_squared_error: 0.8514\n",
      "Epoch 14/400\n",
      "1053/1053 [==============================] - 93s 89ms/step - loss: 1.0820 - mean_squared_error: 1.0820 - val_loss: 6.2130 - val_mean_squared_error: 6.2130\n",
      "Epoch 15/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.6808 - mean_squared_error: 0.6808 - val_loss: 1.7611 - val_mean_squared_error: 1.7611\n",
      "Epoch 16/400\n",
      "1053/1053 [==============================] - 87s 82ms/step - loss: 0.6705 - mean_squared_error: 0.6705 - val_loss: 4.1248 - val_mean_squared_error: 4.1248\n",
      "Epoch 17/400\n",
      "1053/1053 [==============================] - 92s 88ms/step - loss: 0.3708 - mean_squared_error: 0.3708 - val_loss: 0.9217 - val_mean_squared_error: 0.9217\n",
      "Epoch 18/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 0.1690 - mean_squared_error: 0.1690 - val_loss: 0.7531 - val_mean_squared_error: 0.7531\n",
      "Epoch 19/400\n",
      "1053/1053 [==============================] - 89s 84ms/step - loss: 9.0343 - mean_squared_error: 9.0343 - val_loss: 11.3334 - val_mean_squared_error: 11.3334\n",
      "Epoch 20/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 0.0936 - mean_squared_error: 0.0936 - val_loss: 0.0532 - val_mean_squared_error: 0.0532\n",
      "Epoch 21/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0770 - mean_squared_error: 0.0770 - val_loss: 0.0265 - val_mean_squared_error: 0.0265\n",
      "Epoch 22/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0491 - mean_squared_error: 0.0491 - val_loss: 0.0163 - val_mean_squared_error: 0.0163\n",
      "Epoch 23/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0847 - mean_squared_error: 0.0847 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 24/400\n",
      "1053/1053 [==============================] - 88s 84ms/step - loss: 0.0384 - mean_squared_error: 0.0384 - val_loss: 0.1494 - val_mean_squared_error: 0.1494\n",
      "Epoch 25/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0379 - val_mean_squared_error: 0.0379\n",
      "Epoch 26/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0755 - mean_squared_error: 0.0755 - val_loss: 0.0892 - val_mean_squared_error: 0.0892\n",
      "Epoch 27/400\n",
      "1053/1053 [==============================] - 88s 83ms/step - loss: 0.1213 - mean_squared_error: 0.1213 - val_loss: 0.4084 - val_mean_squared_error: 0.4084\n",
      "Epoch 28/400\n",
      "1053/1053 [==============================] - 89s 84ms/step - loss: 0.0627 - mean_squared_error: 0.0627 - val_loss: 0.6347 - val_mean_squared_error: 0.6347\n",
      "Epoch 29/400\n",
      "1053/1053 [==============================] - 89s 84ms/step - loss: 0.2722 - mean_squared_error: 0.2722 - val_loss: 0.2479 - val_mean_squared_error: 0.2479\n",
      "Epoch 30/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 0.0788 - mean_squared_error: 0.0788 - val_loss: 2.7481 - val_mean_squared_error: 2.7481\n",
      "Epoch 31/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 0.0908 - mean_squared_error: 0.0908 - val_loss: 0.6373 - val_mean_squared_error: 0.6373\n",
      "Epoch 32/400\n",
      "1053/1053 [==============================] - 89s 84ms/step - loss: 0.1230 - mean_squared_error: 0.1230 - val_loss: 0.3883 - val_mean_squared_error: 0.3883\n",
      "Epoch 33/400\n",
      "1053/1053 [==============================] - 88s 84ms/step - loss: 0.0754 - mean_squared_error: 0.0754 - val_loss: 0.5229 - val_mean_squared_error: 0.5229\n",
      "Epoch 34/400\n",
      "1053/1053 [==============================] - 88s 84ms/step - loss: 0.0934 - mean_squared_error: 0.0934 - val_loss: 1.0219 - val_mean_squared_error: 1.0219\n",
      "Epoch 35/400\n",
      "1053/1053 [==============================] - 88s 84ms/step - loss: 0.1436 - mean_squared_error: 0.1436 - val_loss: 4.7812 - val_mean_squared_error: 4.7812\n",
      "Epoch 36/400\n",
      "1053/1053 [==============================] - 88s 83ms/step - loss: 0.0854 - mean_squared_error: 0.0854 - val_loss: 0.5922 - val_mean_squared_error: 0.5922\n",
      "Epoch 37/400\n",
      "1053/1053 [==============================] - 89s 84ms/step - loss: 0.1256 - mean_squared_error: 0.1256 - val_loss: 0.2833 - val_mean_squared_error: 0.2833\n",
      "Epoch 38/400\n",
      "1053/1053 [==============================] - 88s 83ms/step - loss: 0.0672 - mean_squared_error: 0.0672 - val_loss: 0.0774 - val_mean_squared_error: 0.0774\n",
      "Epoch 39/400\n",
      "1053/1053 [==============================] - 84s 80ms/step - loss: 0.0613 - mean_squared_error: 0.0613 - val_loss: 0.1328 - val_mean_squared_error: 0.1328\n",
      "Epoch 40/400\n",
      "1053/1053 [==============================] - 86s 81ms/step - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 41/400\n",
      "1053/1053 [==============================] - 83s 79ms/step - loss: 0.0520 - mean_squared_error: 0.0520 - val_loss: 0.1893 - val_mean_squared_error: 0.1893\n",
      "Epoch 42/400\n",
      "1053/1053 [==============================] - 79s 75ms/step - loss: 18498.9408 - mean_squared_error: 18498.9408 - val_loss: 14258414.3277 - val_mean_squared_error: 14258414.3277\n",
      "Epoch 43/400\n",
      "1053/1053 [==============================] - 85s 81ms/step - loss: 94.4694 - mean_squared_error: 94.4694 - val_loss: 305.3613 - val_mean_squared_error: 305.3613\n",
      "Epoch 44/400\n",
      "1053/1053 [==============================] - 86s 82ms/step - loss: 73.7418 - mean_squared_error: 73.7418 - val_loss: 98.2086 - val_mean_squared_error: 98.2086\n",
      "Epoch 45/400\n",
      "1053/1053 [==============================] - 79s 75ms/step - loss: 52.5289 - mean_squared_error: 52.5289 - val_loss: 31.6911 - val_mean_squared_error: 31.6911\n",
      "Epoch 46/400\n",
      "1053/1053 [==============================] - 82s 78ms/step - loss: 12.1883 - mean_squared_error: 12.1883 - val_loss: 85.2798 - val_mean_squared_error: 85.2798\n",
      "Epoch 47/400\n",
      "1053/1053 [==============================] - 85s 81ms/step - loss: 16.1840 - mean_squared_error: 16.1840 - val_loss: 9.5625 - val_mean_squared_error: 9.5625\n",
      "Epoch 48/400\n",
      "1053/1053 [==============================] - 84s 80ms/step - loss: 9.3510 - mean_squared_error: 9.3510 - val_loss: 15.4649 - val_mean_squared_error: 15.4649\n",
      "Epoch 49/400\n",
      "1053/1053 [==============================] - 84s 80ms/step - loss: 10.0661 - mean_squared_error: 10.0661 - val_loss: 8.3307 - val_mean_squared_error: 8.3307\n",
      "Epoch 50/400\n",
      "1053/1053 [==============================] - 86s 81ms/step - loss: 9.8534 - mean_squared_error: 9.8534 - val_loss: 15.7812 - val_mean_squared_error: 15.7812\n",
      "Epoch 51/400\n",
      "1053/1053 [==============================] - 87s 83ms/step - loss: 11.6794 - mean_squared_error: 11.6794 - val_loss: 50.3188 - val_mean_squared_error: 50.3188\n",
      "Epoch 52/400\n",
      "1053/1053 [==============================] - 87s 82ms/step - loss: 13.5174 - mean_squared_error: 13.5174 - val_loss: 18.9040 - val_mean_squared_error: 18.9040\n",
      "Epoch 53/400\n",
      "1053/1053 [==============================] - 87s 82ms/step - loss: 14.2909 - mean_squared_error: 14.2909 - val_loss: 58.6347 - val_mean_squared_error: 58.6347\n",
      "Epoch 54/400\n",
      "1053/1053 [==============================] - 86s 82ms/step - loss: 10.7588 - mean_squared_error: 10.7588 - val_loss: 368.7337 - val_mean_squared_error: 368.7337\n",
      "Epoch 55/400\n",
      "1053/1053 [==============================] - 87s 82ms/step - loss: 23.5454 - mean_squared_error: 23.5454 - val_loss: 35.4642 - val_mean_squared_error: 35.4642\n",
      "Epoch 56/400\n",
      "1053/1053 [==============================] - 87s 82ms/step - loss: 14.2667 - mean_squared_error: 14.2667 - val_loss: 98.7255 - val_mean_squared_error: 98.7255\n",
      "Epoch 57/400\n",
      "1053/1053 [==============================] - 87s 82ms/step - loss: 25.3883 - mean_squared_error: 25.3883 - val_loss: 4.5137 - val_mean_squared_error: 4.5137\n",
      "Epoch 58/400\n",
      "1053/1053 [==============================] - 88s 83ms/step - loss: 6.5556 - mean_squared_error: 6.5556 - val_loss: 5.7897 - val_mean_squared_error: 5.7897\n",
      "Epoch 59/400\n",
      "1053/1053 [==============================] - 86s 82ms/step - loss: 4.7184 - mean_squared_error: 4.7184 - val_loss: 3.2247 - val_mean_squared_error: 3.2247\n",
      "Epoch 60/400\n",
      "1053/1053 [==============================] - 88s 84ms/step - loss: 2.4689 - mean_squared_error: 2.4689 - val_loss: 12.5654 - val_mean_squared_error: 12.5654\n",
      "Epoch 61/400\n",
      "1053/1053 [==============================] - 87s 83ms/step - loss: 4.2347 - mean_squared_error: 4.2347 - val_loss: 7.0622 - val_mean_squared_error: 7.0622\n",
      "Epoch 62/400\n",
      "1053/1053 [==============================] - 88s 84ms/step - loss: 2.1750 - mean_squared_error: 2.1750 - val_loss: 3.8306 - val_mean_squared_error: 3.8306\n",
      "Epoch 63/400\n",
      "1053/1053 [==============================] - 89s 84ms/step - loss: 2.0573 - mean_squared_error: 2.0573 - val_loss: 1.5824 - val_mean_squared_error: 1.5824\n",
      "Epoch 64/400\n",
      "1053/1053 [==============================] - 89s 84ms/step - loss: 0.9230 - mean_squared_error: 0.9230 - val_loss: 1.8979 - val_mean_squared_error: 1.8979\n",
      "Epoch 65/400\n",
      "1053/1053 [==============================] - 88s 83ms/step - loss: 1.3983 - mean_squared_error: 1.3983 - val_loss: 4.9862 - val_mean_squared_error: 4.9862\n",
      "Epoch 66/400\n",
      "1053/1053 [==============================] - 86s 81ms/step - loss: 1.8149 - mean_squared_error: 1.8149 - val_loss: 19.0264 - val_mean_squared_error: 19.0264\n",
      "Epoch 67/400\n",
      "1053/1053 [==============================] - 85s 80ms/step - loss: 1.3891 - mean_squared_error: 1.3891 - val_loss: 1.0457 - val_mean_squared_error: 1.0457\n",
      "Epoch 68/400\n",
      "1053/1053 [==============================] - 84s 80ms/step - loss: 2.0370 - mean_squared_error: 2.0370 - val_loss: 0.2398 - val_mean_squared_error: 0.2398\n",
      "Epoch 69/400\n",
      "1053/1053 [==============================] - 84s 80ms/step - loss: 1.2983 - mean_squared_error: 1.2983 - val_loss: 5.2336 - val_mean_squared_error: 5.2336\n",
      "Epoch 70/400\n",
      "1053/1053 [==============================] - 86s 81ms/step - loss: 0.7987 - mean_squared_error: 0.7987 - val_loss: 4.6812 - val_mean_squared_error: 4.6812\n",
      "Epoch 71/400\n",
      "1053/1053 [==============================] - 85s 80ms/step - loss: 1.2588 - mean_squared_error: 1.2588 - val_loss: 10.6230 - val_mean_squared_error: 10.6230\n",
      "Epoch 72/400\n",
      "1053/1053 [==============================] - 86s 81ms/step - loss: 1.1032 - mean_squared_error: 1.1032 - val_loss: 0.3966 - val_mean_squared_error: 0.3966\n",
      "Epoch 73/400\n",
      "1053/1053 [==============================] - 84s 80ms/step - loss: 0.2111 - mean_squared_error: 0.2111 - val_loss: 0.1407 - val_mean_squared_error: 0.1407\n",
      "Epoch 74/400\n",
      "1053/1053 [==============================] - 85s 81ms/step - loss: 0.3248 - mean_squared_error: 0.3248 - val_loss: 0.3122 - val_mean_squared_error: 0.3122\n",
      "Epoch 75/400\n",
      "1053/1053 [==============================] - 85s 81ms/step - loss: 0.1356 - mean_squared_error: 0.1356 - val_loss: 0.4273 - val_mean_squared_error: 0.4273\n",
      "Epoch 76/400\n",
      "1053/1053 [==============================] - 83s 79ms/step - loss: 0.1218 - mean_squared_error: 0.1218 - val_loss: 0.5162 - val_mean_squared_error: 0.5162\n",
      "Epoch 77/400\n",
      "1053/1053 [==============================] - 89s 84ms/step - loss: 0.2769 - mean_squared_error: 0.2769 - val_loss: 0.1778 - val_mean_squared_error: 0.1778\n",
      "Epoch 78/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.1801 - mean_squared_error: 0.1801 - val_loss: 0.0827 - val_mean_squared_error: 0.0827\n",
      "Epoch 79/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0888 - mean_squared_error: 0.0888 - val_loss: 0.1678 - val_mean_squared_error: 0.1678\n",
      "Epoch 80/400\n",
      "1053/1053 [==============================] - 92s 88ms/step - loss: 0.1512 - mean_squared_error: 0.1512 - val_loss: 0.2134 - val_mean_squared_error: 0.2134\n",
      "Epoch 81/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0830 - mean_squared_error: 0.0830 - val_loss: 0.2896 - val_mean_squared_error: 0.2896\n",
      "Epoch 82/400\n",
      "1053/1053 [==============================] - 94s 89ms/step - loss: 0.1447 - mean_squared_error: 0.1447 - val_loss: 0.1033 - val_mean_squared_error: 0.1033\n",
      "Epoch 83/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0624 - mean_squared_error: 0.0624 - val_loss: 0.0830 - val_mean_squared_error: 0.0830\n",
      "Epoch 84/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.5668 - val_mean_squared_error: 0.5668\n",
      "Epoch 85/400\n",
      "1053/1053 [==============================] - 89s 84ms/step - loss: 0.0480 - mean_squared_error: 0.0480 - val_loss: 0.3084 - val_mean_squared_error: 0.3084\n",
      "Epoch 86/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0459 - mean_squared_error: 0.0459 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 87/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0182 - val_mean_squared_error: 0.0182\n",
      "Epoch 88/400\n",
      "1053/1053 [==============================] - 92s 88ms/step - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.3500 - val_mean_squared_error: 0.3500\n",
      "Epoch 89/400\n",
      "1053/1053 [==============================] - 88s 83ms/step - loss: 0.0371 - mean_squared_error: 0.0371 - val_loss: 0.0264 - val_mean_squared_error: 0.0264\n",
      "Epoch 90/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0274 - val_mean_squared_error: 0.0274\n",
      "Epoch 91/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0166 - mean_squared_error: 0.0166 - val_loss: 0.0204 - val_mean_squared_error: 0.0204\n",
      "Epoch 92/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0169 - val_mean_squared_error: 0.0169\n",
      "Epoch 93/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0164 - mean_squared_error: 0.0164 - val_loss: 0.0341 - val_mean_squared_error: 0.0341\n",
      "Epoch 94/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0121 - mean_squared_error: 0.0121 - val_loss: 0.0114 - val_mean_squared_error: 0.0114\n",
      "Epoch 95/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0115 - mean_squared_error: 0.0115 - val_loss: 0.0142 - val_mean_squared_error: 0.0142\n",
      "Epoch 96/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0106 - mean_squared_error: 0.0106 - val_loss: 0.0194 - val_mean_squared_error: 0.0194\n",
      "Epoch 97/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.1532 - mean_squared_error: 0.1532 - val_loss: 724.7141 - val_mean_squared_error: 724.7141\n",
      "Epoch 98/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 31.2330 - mean_squared_error: 31.2330 - val_loss: 9.1080 - val_mean_squared_error: 9.1080\n",
      "Epoch 99/400\n",
      "1053/1053 [==============================] - 91s 87ms/step - loss: 0.0872 - mean_squared_error: 0.0872 - val_loss: 0.0747 - val_mean_squared_error: 0.0747\n",
      "Epoch 100/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0469 - mean_squared_error: 0.0469 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 101/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0384 - mean_squared_error: 0.0384 - val_loss: 0.0280 - val_mean_squared_error: 0.0280\n",
      "Epoch 102/400\n",
      "1053/1053 [==============================] - 94s 89ms/step - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0212 - val_mean_squared_error: 0.0212\n",
      "Epoch 103/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0195 - val_mean_squared_error: 0.0195\n",
      "Epoch 104/400\n",
      "1053/1053 [==============================] - 93s 89ms/step - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0357 - val_mean_squared_error: 0.0357\n",
      "Epoch 105/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0180 - val_mean_squared_error: 0.0180\n",
      "Epoch 106/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0167 - mean_squared_error: 0.0167 - val_loss: 0.0254 - val_mean_squared_error: 0.0254\n",
      "Epoch 107/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0139 - mean_squared_error: 0.0139 - val_loss: 0.0157 - val_mean_squared_error: 0.0157\n",
      "Epoch 108/400\n",
      "1053/1053 [==============================] - 92s 88ms/step - loss: 0.0153 - mean_squared_error: 0.0153 - val_loss: 0.0127 - val_mean_squared_error: 0.0127\n",
      "Epoch 109/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0154 - mean_squared_error: 0.0154 - val_loss: 0.0097 - val_mean_squared_error: 0.0097\n",
      "Epoch 110/400\n",
      "1053/1053 [==============================] - 92s 88ms/step - loss: 0.0158 - mean_squared_error: 0.0158 - val_loss: 0.0173 - val_mean_squared_error: 0.0173\n",
      "Epoch 111/400\n",
      "1053/1053 [==============================] - 92s 88ms/step - loss: 0.0136 - mean_squared_error: 0.0136 - val_loss: 0.0108 - val_mean_squared_error: 0.0108\n",
      "Epoch 112/400\n",
      "1053/1053 [==============================] - 92s 88ms/step - loss: 0.0137 - mean_squared_error: 0.0137 - val_loss: 0.0180 - val_mean_squared_error: 0.0180\n",
      "Epoch 113/400\n",
      "1053/1053 [==============================] - 91s 87ms/step - loss: 0.0138 - mean_squared_error: 0.0138 - val_loss: 0.0108 - val_mean_squared_error: 0.0108\n",
      "Epoch 114/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0094 - mean_squared_error: 0.0094 - val_loss: 0.0109 - val_mean_squared_error: 0.0109\n",
      "Epoch 115/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0111 - mean_squared_error: 0.0111 - val_loss: 0.0106 - val_mean_squared_error: 0.0106\n",
      "Epoch 116/400\n",
      "1053/1053 [==============================] - 92s 88ms/step - loss: 0.0098 - mean_squared_error: 0.0098 - val_loss: 5.8992 - val_mean_squared_error: 5.8992\n",
      "Epoch 117/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 0.0110 - mean_squared_error: 0.0110 - val_loss: 0.0107 - val_mean_squared_error: 0.0107\n",
      "Epoch 118/400\n",
      "1053/1053 [==============================] - 94s 89ms/step - loss: 0.0102 - mean_squared_error: 0.0102 - val_loss: 0.0108 - val_mean_squared_error: 0.0108\n",
      "Epoch 119/400\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 0.0088 - mean_squared_error: 0.0088 - val_loss: 0.0109 - val_mean_squared_error: 0.0109\n",
      "Epoch 120/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 0.0093 - mean_squared_error: 0.0093 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 121/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0086 - mean_squared_error: 0.0086 - val_loss: 0.0076 - val_mean_squared_error: 0.0076\n",
      "Epoch 122/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0080 - mean_squared_error: 0.0080 - val_loss: 0.0075 - val_mean_squared_error: 0.0075\n",
      "Epoch 123/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0078 - mean_squared_error: 0.0078 - val_loss: 52.7122 - val_mean_squared_error: 52.7122\n",
      "Epoch 124/400\n",
      "1053/1053 [==============================] - 88s 84ms/step - loss: 0.0083 - mean_squared_error: 0.0083 - val_loss: 0.0114 - val_mean_squared_error: 0.0114\n",
      "Epoch 125/400\n",
      "1053/1053 [==============================] - 89s 84ms/step - loss: 0.0073 - mean_squared_error: 0.0073 - val_loss: 0.0062 - val_mean_squared_error: 0.0062\n",
      "Epoch 126/400\n",
      "1053/1053 [==============================] - 94s 89ms/step - loss: 0.0084 - mean_squared_error: 0.0084 - val_loss: 0.0104 - val_mean_squared_error: 0.0104\n",
      "Epoch 127/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 0.0083 - mean_squared_error: 0.0083 - val_loss: 0.0061 - val_mean_squared_error: 0.0061\n",
      "Epoch 128/400\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 0.0068 - mean_squared_error: 0.0068 - val_loss: 0.0085 - val_mean_squared_error: 0.0085\n",
      "Epoch 129/400\n",
      "1053/1053 [==============================] - 94s 89ms/step - loss: 0.0085 - mean_squared_error: 0.0085 - val_loss: 0.0068 - val_mean_squared_error: 0.0068\n",
      "Epoch 130/400\n",
      "1053/1053 [==============================] - 134s 127ms/step - loss: 0.0069 - mean_squared_error: 0.0069 - val_loss: 0.0097 - val_mean_squared_error: 0.0097\n",
      "Epoch 131/400\n",
      "1053/1053 [==============================] - 172s 163ms/step - loss: 0.0078 - mean_squared_error: 0.0078 - val_loss: 0.0068 - val_mean_squared_error: 0.0068\n",
      "Epoch 132/400\n",
      "1053/1053 [==============================] - 172s 163ms/step - loss: 0.0076 - mean_squared_error: 0.0076 - val_loss: 0.0078 - val_mean_squared_error: 0.0078\n",
      "Epoch 133/400\n",
      "1053/1053 [==============================] - 102s 97ms/step - loss: 0.0072 - mean_squared_error: 0.0072 - val_loss: 0.0050 - val_mean_squared_error: 0.0050\n",
      "Epoch 134/400\n",
      "1053/1053 [==============================] - 107s 102ms/step - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.0066 - val_mean_squared_error: 0.0066\n",
      "Epoch 135/400\n",
      "1053/1053 [==============================] - 139s 132ms/step - loss: 0.0075 - mean_squared_error: 0.0075 - val_loss: 0.0063 - val_mean_squared_error: 0.0063\n",
      "Epoch 136/400\n",
      "1053/1053 [==============================] - 140s 133ms/step - loss: 0.0070 - mean_squared_error: 0.0070 - val_loss: 0.0066 - val_mean_squared_error: 0.0066\n",
      "Epoch 137/400\n",
      "1053/1053 [==============================] - 131s 124ms/step - loss: 0.1130 - mean_squared_error: 0.1130 - val_loss: 102993.8488 - val_mean_squared_error: 102993.8488\n",
      "Epoch 138/400\n",
      "1053/1053 [==============================] - 140s 133ms/step - loss: 0.3311 - mean_squared_error: 0.3311 - val_loss: 5.7677 - val_mean_squared_error: 5.7677\n",
      "Epoch 139/400\n",
      "1053/1053 [==============================] - 157s 149ms/step - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0106 - val_mean_squared_error: 0.0106\n",
      "Epoch 140/400\n",
      "1053/1053 [==============================] - 179s 170ms/step - loss: 0.0131 - mean_squared_error: 0.0131 - val_loss: 0.0115 - val_mean_squared_error: 0.0115\n",
      "Epoch 141/400\n",
      "1053/1053 [==============================] - 183s 174ms/step - loss: 0.0117 - mean_squared_error: 0.0117 - val_loss: 0.0101 - val_mean_squared_error: 0.0101\n",
      "Epoch 142/400\n",
      "1053/1053 [==============================] - 186s 177ms/step - loss: 0.0093 - mean_squared_error: 0.0093 - val_loss: 0.0094 - val_mean_squared_error: 0.0094\n",
      "Epoch 143/400\n",
      "1053/1053 [==============================] - 161s 153ms/step - loss: 0.0078 - mean_squared_error: 0.0078 - val_loss: 0.0080 - val_mean_squared_error: 0.0080\n",
      "Epoch 144/400\n",
      "1053/1053 [==============================] - 136s 130ms/step - loss: 0.0082 - mean_squared_error: 0.0082 - val_loss: 0.0085 - val_mean_squared_error: 0.0085\n",
      "Epoch 145/400\n",
      "1053/1053 [==============================] - 251s 239ms/step - loss: 0.0080 - mean_squared_error: 0.0080 - val_loss: 0.0084 - val_mean_squared_error: 0.0084\n",
      "Epoch 146/400\n",
      "1053/1053 [==============================] - 153s 145ms/step - loss: 0.0072 - mean_squared_error: 0.0072 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 147/400\n",
      "1053/1053 [==============================] - 114s 108ms/step - loss: 0.0068 - mean_squared_error: 0.0068 - val_loss: 0.0073 - val_mean_squared_error: 0.0073\n",
      "Epoch 148/400\n",
      "1053/1053 [==============================] - 116s 111ms/step - loss: 0.0069 - mean_squared_error: 0.0069 - val_loss: 0.0093 - val_mean_squared_error: 0.0093\n",
      "Epoch 149/400\n",
      "1053/1053 [==============================] - 112s 106ms/step - loss: 0.0075 - mean_squared_error: 0.0075 - val_loss: 0.0096 - val_mean_squared_error: 0.0096\n",
      "Epoch 150/400\n",
      "1053/1053 [==============================] - 131s 124ms/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.0089 - val_mean_squared_error: 0.0089\n",
      "Epoch 151/400\n",
      "1053/1053 [==============================] - 144s 137ms/step - loss: 0.0062 - mean_squared_error: 0.0062 - val_loss: 0.0067 - val_mean_squared_error: 0.0067\n",
      "Epoch 152/400\n",
      "1053/1053 [==============================] - 122s 116ms/step - loss: 0.0068 - mean_squared_error: 0.0068 - val_loss: 0.0105 - val_mean_squared_error: 0.0105\n",
      "Epoch 153/400\n",
      "1053/1053 [==============================] - 124s 118ms/step - loss: 0.0062 - mean_squared_error: 0.0062 - val_loss: 0.0059 - val_mean_squared_error: 0.0059\n",
      "Epoch 154/400\n",
      "1053/1053 [==============================] - 131s 124ms/step - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.0091 - val_mean_squared_error: 0.0091\n",
      "Epoch 155/400\n",
      "1053/1053 [==============================] - 126s 120ms/step - loss: 0.0059 - mean_squared_error: 0.0059 - val_loss: 0.0070 - val_mean_squared_error: 0.0070\n",
      "Epoch 156/400\n",
      "1053/1053 [==============================] - 149s 141ms/step - loss: 0.0054 - mean_squared_error: 0.0054 - val_loss: 0.0056 - val_mean_squared_error: 0.0056\n",
      "Epoch 157/400\n",
      "1053/1053 [==============================] - 107s 101ms/step - loss: 0.0060 - mean_squared_error: 0.0060 - val_loss: 0.0065 - val_mean_squared_error: 0.0065\n",
      "Epoch 158/400\n",
      "1053/1053 [==============================] - 140s 133ms/step - loss: 0.0055 - mean_squared_error: 0.0055 - val_loss: 0.0042 - val_mean_squared_error: 0.0042\n",
      "Epoch 159/400\n",
      "1053/1053 [==============================] - 133s 127ms/step - loss: 0.0053 - mean_squared_error: 0.0053 - val_loss: 0.0054 - val_mean_squared_error: 0.0054\n",
      "Epoch 160/400\n",
      "1053/1053 [==============================] - 135s 128ms/step - loss: 0.0055 - mean_squared_error: 0.0055 - val_loss: 0.0053 - val_mean_squared_error: 0.0053\n",
      "Epoch 161/400\n",
      "1053/1053 [==============================] - 118s 112ms/step - loss: 0.0072 - mean_squared_error: 0.0072 - val_loss: 0.0053 - val_mean_squared_error: 0.0053\n",
      "Epoch 162/400\n",
      "1053/1053 [==============================] - 158s 150ms/step - loss: 0.0057 - mean_squared_error: 0.0057 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 163/400\n",
      "1053/1053 [==============================] - 122s 115ms/step - loss: 0.0053 - mean_squared_error: 0.0053 - val_loss: 0.0056 - val_mean_squared_error: 0.0056\n",
      "Epoch 164/400\n",
      "1053/1053 [==============================] - 141s 134ms/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.2883 - val_mean_squared_error: 0.2883\n",
      "Epoch 165/400\n",
      "1053/1053 [==============================] - 111s 105ms/step - loss: 0.0053 - mean_squared_error: 0.0053 - val_loss: 0.0082 - val_mean_squared_error: 0.0082\n",
      "Epoch 166/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0065 - mean_squared_error: 0.0065 - val_loss: 0.0051 - val_mean_squared_error: 0.0051\n",
      "Epoch 167/400\n",
      "1053/1053 [==============================] - 107s 102ms/step - loss: 0.0080 - mean_squared_error: 0.0080 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "Epoch 168/400\n",
      "1053/1053 [==============================] - 134s 127ms/step - loss: 0.0675 - mean_squared_error: 0.0675 - val_loss: 0.8396 - val_mean_squared_error: 0.8396\n",
      "Epoch 169/400\n",
      "1053/1053 [==============================] - 123s 117ms/step - loss: 0.0085 - mean_squared_error: 0.0085 - val_loss: 0.0070 - val_mean_squared_error: 0.0070\n",
      "Epoch 170/400\n",
      "1053/1053 [==============================] - 115s 109ms/step - loss: 0.0080 - mean_squared_error: 0.0080 - val_loss: 0.0063 - val_mean_squared_error: 0.0063\n",
      "Epoch 171/400\n",
      "1053/1053 [==============================] - 123s 117ms/step - loss: 0.0073 - mean_squared_error: 0.0073 - val_loss: 0.0063 - val_mean_squared_error: 0.0063\n",
      "Epoch 172/400\n",
      "1053/1053 [==============================] - 201s 190ms/step - loss: 0.0059 - mean_squared_error: 0.0059 - val_loss: 0.0056 - val_mean_squared_error: 0.0056\n",
      "Epoch 173/400\n",
      "1053/1053 [==============================] - 189s 180ms/step - loss: 0.0059 - mean_squared_error: 0.0059 - val_loss: 0.0061 - val_mean_squared_error: 0.0061\n",
      "Epoch 174/400\n",
      "1053/1053 [==============================] - 164s 156ms/step - loss: 0.0070 - mean_squared_error: 0.0070 - val_loss: 0.0276 - val_mean_squared_error: 0.0276\n",
      "Epoch 175/400\n",
      "1053/1053 [==============================] - 138s 131ms/step - loss: 0.0054 - mean_squared_error: 0.0054 - val_loss: 0.0062 - val_mean_squared_error: 0.0062\n",
      "Epoch 176/400\n",
      "1053/1053 [==============================] - 153s 146ms/step - loss: 0.0056 - mean_squared_error: 0.0056 - val_loss: 0.0061 - val_mean_squared_error: 0.0061\n",
      "Epoch 177/400\n",
      "1053/1053 [==============================] - 169s 161ms/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.0049 - val_mean_squared_error: 0.0049\n",
      "Epoch 178/400\n",
      "1053/1053 [==============================] - 147s 140ms/step - loss: 0.0071 - mean_squared_error: 0.0071 - val_loss: 58.4823 - val_mean_squared_error: 58.4823\n",
      "Epoch 179/400\n",
      "1053/1053 [==============================] - 127s 120ms/step - loss: 0.0080 - mean_squared_error: 0.0080 - val_loss: 0.0061 - val_mean_squared_error: 0.0061\n",
      "Epoch 180/400\n",
      "1053/1053 [==============================] - 165s 157ms/step - loss: 0.0058 - mean_squared_error: 0.0058 - val_loss: 0.4267 - val_mean_squared_error: 0.4267\n",
      "Epoch 181/400\n",
      "1053/1053 [==============================] - 138s 131ms/step - loss: 0.0109 - mean_squared_error: 0.0109 - val_loss: 0.0052 - val_mean_squared_error: 0.0052\n",
      "Epoch 182/400\n",
      "1053/1053 [==============================] - 157s 149ms/step - loss: 0.0051 - mean_squared_error: 0.0051 - val_loss: 0.0053 - val_mean_squared_error: 0.0053\n",
      "Epoch 183/400\n",
      "1053/1053 [==============================] - 123s 116ms/step - loss: 0.0050 - mean_squared_error: 0.0050 - val_loss: 0.0052 - val_mean_squared_error: 0.0052\n",
      "Epoch 184/400\n",
      "1053/1053 [==============================] - 158s 150ms/step - loss: 0.0055 - mean_squared_error: 0.0055 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 185/400\n",
      "1053/1053 [==============================] - 143s 135ms/step - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0051 - val_mean_squared_error: 0.0051\n",
      "Epoch 186/400\n",
      "1053/1053 [==============================] - 138s 131ms/step - loss: 0.0051 - mean_squared_error: 0.0051 - val_loss: 0.0058 - val_mean_squared_error: 0.0058\n",
      "Epoch 187/400\n",
      "1053/1053 [==============================] - 128s 122ms/step - loss: 0.0049 - mean_squared_error: 0.0049 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 188/400\n",
      "1053/1053 [==============================] - 150s 142ms/step - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0052 - val_mean_squared_error: 0.0052\n",
      "Epoch 189/400\n",
      "1053/1053 [==============================] - 125s 119ms/step - loss: 0.0053 - mean_squared_error: 0.0053 - val_loss: 0.0060 - val_mean_squared_error: 0.0060\n",
      "Epoch 190/400\n",
      "1053/1053 [==============================] - 164s 155ms/step - loss: 0.0120 - mean_squared_error: 0.0120 - val_loss: 0.2269 - val_mean_squared_error: 0.2269\n",
      "Epoch 191/400\n",
      "1053/1053 [==============================] - 152s 144ms/step - loss: 0.0058 - mean_squared_error: 0.0058 - val_loss: 0.0069 - val_mean_squared_error: 0.0069\n",
      "Epoch 192/400\n",
      "1053/1053 [==============================] - 134s 128ms/step - loss: 0.0047 - mean_squared_error: 0.0047 - val_loss: 0.0052 - val_mean_squared_error: 0.0052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193/400\n",
      "1053/1053 [==============================] - 117s 111ms/step - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0053 - val_mean_squared_error: 0.0053\n",
      "Epoch 194/400\n",
      "1053/1053 [==============================] - 130s 123ms/step - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
      "Epoch 195/400\n",
      "1053/1053 [==============================] - 126s 119ms/step - loss: 0.0047 - mean_squared_error: 0.0047 - val_loss: 0.0061 - val_mean_squared_error: 0.0061\n",
      "Epoch 196/400\n",
      "1053/1053 [==============================] - 120s 114ms/step - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.0042 - val_mean_squared_error: 0.0042\n",
      "Epoch 197/400\n",
      "1053/1053 [==============================] - 142s 134ms/step - loss: 0.0050 - mean_squared_error: 0.0050 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 198/400\n",
      "1053/1053 [==============================] - 151s 143ms/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 199/400\n",
      "1053/1053 [==============================] - 140s 133ms/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 200/400\n",
      "1053/1053 [==============================] - 142s 135ms/step - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0041 - val_mean_squared_error: 0.0041\n",
      "Epoch 201/400\n",
      "1053/1053 [==============================] - 117s 111ms/step - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.7127 - val_mean_squared_error: 0.7127\n",
      "Epoch 202/400\n",
      "1053/1053 [==============================] - 124s 117ms/step - loss: 0.0663 - mean_squared_error: 0.0663 - val_loss: 11.6993 - val_mean_squared_error: 11.6993\n",
      "Epoch 203/400\n",
      "1053/1053 [==============================] - 89s 84ms/step - loss: 0.0126 - mean_squared_error: 0.0126 - val_loss: 0.0090 - val_mean_squared_error: 0.0090\n",
      "Epoch 204/400\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0088 - mean_squared_error: 0.0088 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 205/400\n",
      "1053/1053 [==============================] - 110s 105ms/step - loss: 0.0097 - mean_squared_error: 0.0097 - val_loss: 0.0079 - val_mean_squared_error: 0.0079\n",
      "Epoch 206/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0077 - mean_squared_error: 0.0077 - val_loss: 0.0098 - val_mean_squared_error: 0.0098\n",
      "Epoch 207/400\n",
      "1053/1053 [==============================] - 86s 81ms/step - loss: 0.0076 - mean_squared_error: 0.0076 - val_loss: 0.0079 - val_mean_squared_error: 0.0079\n",
      "Epoch 208/400\n",
      "1053/1053 [==============================] - 86s 82ms/step - loss: 0.0069 - mean_squared_error: 0.0069 - val_loss: 0.0123 - val_mean_squared_error: 0.0123\n",
      "Epoch 209/400\n",
      "1053/1053 [==============================] - 108s 102ms/step - loss: 0.0065 - mean_squared_error: 0.0065 - val_loss: 0.0134 - val_mean_squared_error: 0.0134\n",
      "Epoch 210/400\n",
      "1053/1053 [==============================] - 94s 89ms/step - loss: 0.0068 - mean_squared_error: 0.0068 - val_loss: 0.0569 - val_mean_squared_error: 0.0569\n",
      "Epoch 211/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0081 - mean_squared_error: 0.0081 - val_loss: 0.0126 - val_mean_squared_error: 0.0126\n",
      "Epoch 212/400\n",
      "1053/1053 [==============================] - 107s 102ms/step - loss: 0.0066 - mean_squared_error: 0.0066 - val_loss: 0.0065 - val_mean_squared_error: 0.0065\n",
      "Epoch 213/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0058 - mean_squared_error: 0.0058 - val_loss: 0.0056 - val_mean_squared_error: 0.0056\n",
      "Epoch 214/400\n",
      "1053/1053 [==============================] - 95s 90ms/step - loss: 0.0068 - mean_squared_error: 0.0068 - val_loss: 0.0052 - val_mean_squared_error: 0.0052\n",
      "Epoch 215/400\n",
      "1053/1053 [==============================] - 87s 83ms/step - loss: 0.0055 - mean_squared_error: 0.0055 - val_loss: 0.0099 - val_mean_squared_error: 0.0099\n",
      "Epoch 216/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 0.0057 - mean_squared_error: 0.0057 - val_loss: 0.0061 - val_mean_squared_error: 0.0061\n",
      "Epoch 217/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 0.0055 - mean_squared_error: 0.0055 - val_loss: 0.0067 - val_mean_squared_error: 0.0067\n",
      "Epoch 218/400\n",
      "1053/1053 [==============================] - 87s 83ms/step - loss: 0.0058 - mean_squared_error: 0.0058 - val_loss: 0.0056 - val_mean_squared_error: 0.0056\n",
      "Epoch 219/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 0.0108 - mean_squared_error: 0.0108 - val_loss: 143.1528 - val_mean_squared_error: 143.1528\n",
      "Epoch 220/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 0.0154 - mean_squared_error: 0.0154 - val_loss: 0.0554 - val_mean_squared_error: 0.0554\n",
      "Epoch 221/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0074 - mean_squared_error: 0.0074 - val_loss: 0.0360 - val_mean_squared_error: 0.0360\n",
      "Epoch 222/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0066 - mean_squared_error: 0.0066 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "Epoch 223/400\n",
      "1053/1053 [==============================] - 91s 87ms/step - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.0070 - val_mean_squared_error: 0.0070\n",
      "Epoch 224/400\n",
      "1053/1053 [==============================] - 94s 89ms/step - loss: 0.0070 - mean_squared_error: 0.0070 - val_loss: 0.0057 - val_mean_squared_error: 0.0057\n",
      "Epoch 225/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0057 - mean_squared_error: 0.0057 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 226/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0053 - mean_squared_error: 0.0053 - val_loss: 0.0106 - val_mean_squared_error: 0.0106\n",
      "Epoch 227/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0049 - mean_squared_error: 0.0049 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 228/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0060 - mean_squared_error: 0.0060 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 229/400\n",
      "1053/1053 [==============================] - 88s 84ms/step - loss: 0.0057 - mean_squared_error: 0.0057 - val_loss: 0.0126 - val_mean_squared_error: 0.0126\n",
      "Epoch 230/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 231/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 232/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 233/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 234/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0473 - val_mean_squared_error: 0.0473\n",
      "Epoch 235/400\n",
      "1053/1053 [==============================] - 87s 82ms/step - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 236/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 237/400\n",
      "1053/1053 [==============================] - 88s 84ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 238/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 239/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 240/400\n",
      "1053/1053 [==============================] - 93s 89ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 241/400\n",
      "1053/1053 [==============================] - 95s 91ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 242/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 243/400\n",
      "1053/1053 [==============================] - 95s 91ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 244/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 245/400\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 246/400\n",
      "1053/1053 [==============================] - 97s 93ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 247/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 248/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 249/400\n",
      "1053/1053 [==============================] - 99s 94ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 250/400\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 251/400\n",
      "1053/1053 [==============================] - 85s 81ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 252/400\n",
      "1053/1053 [==============================] - 123s 117ms/step - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 253/400\n",
      "1053/1053 [==============================] - 125s 119ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 254/400\n",
      "1053/1053 [==============================] - 123s 117ms/step - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 255/400\n",
      "1053/1053 [==============================] - 178s 169ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 256/400\n",
      "1053/1053 [==============================] - 125s 118ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 257/400\n",
      "1053/1053 [==============================] - 172s 164ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0475 - val_mean_squared_error: 0.0475\n",
      "Epoch 258/400\n",
      "1053/1053 [==============================] - 171s 163ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 259/400\n",
      "1053/1053 [==============================] - 173s 164ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 260/400\n",
      "1053/1053 [==============================] - 169s 160ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 261/400\n",
      "1053/1053 [==============================] - 157s 149ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 262/400\n",
      "1053/1053 [==============================] - 190s 180ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 263/400\n",
      "1053/1053 [==============================] - 134s 127ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 264/400\n",
      "1053/1053 [==============================] - 135s 128ms/step - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 265/400\n",
      "1053/1053 [==============================] - 191s 182ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 266/400\n",
      "1053/1053 [==============================] - 120s 114ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 267/400\n",
      "1053/1053 [==============================] - 122s 115ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 268/400\n",
      "1053/1053 [==============================] - 121s 115ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 269/400\n",
      "1053/1053 [==============================] - 107s 102ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 270/400\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 271/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "Epoch 272/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "Epoch 273/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 274/400\n",
      "1053/1053 [==============================] - 106s 100ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 275/400\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 276/400\n",
      "1053/1053 [==============================] - 93s 89ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 277/400\n",
      "1053/1053 [==============================] - 95s 90ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 278/400\n",
      "1053/1053 [==============================] - 122s 116ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 279/400\n",
      "1053/1053 [==============================] - 120s 114ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 280/400\n",
      "1053/1053 [==============================] - 136s 129ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 281/400\n",
      "1053/1053 [==============================] - 125s 119ms/step - loss: 0.0443 - mean_squared_error: 0.0443 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 282/400\n",
      "1053/1053 [==============================] - 122s 115ms/step - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 283/400\n",
      "1053/1053 [==============================] - 104s 98ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 284/400\n",
      "1053/1053 [==============================] - 127s 121ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 285/400\n",
      "1053/1053 [==============================] - 105s 100ms/step - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 286/400\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "Epoch 287/400\n",
      "1053/1053 [==============================] - 136s 129ms/step - loss: 0.0445 - mean_squared_error: 0.0445 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 288/400\n",
      "1053/1053 [==============================] - 126s 120ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 289/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1053 [==============================] - 128s 121ms/step - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 290/400\n",
      "1053/1053 [==============================] - 123s 117ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 291/400\n",
      "1053/1053 [==============================] - 115s 109ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 292/400\n",
      "1053/1053 [==============================] - 130s 124ms/step - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 293/400\n",
      "1053/1053 [==============================] - 118s 112ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 294/400\n",
      "1053/1053 [==============================] - 132s 125ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 295/400\n",
      "1053/1053 [==============================] - 120s 114ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 296/400\n",
      "1053/1053 [==============================] - 109s 103ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 297/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 298/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 299/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 300/400\n",
      "1053/1053 [==============================] - 94s 89ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 301/400\n",
      "1053/1053 [==============================] - 94s 89ms/step - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 302/400\n",
      "1053/1053 [==============================] - 91s 87ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 303/400\n",
      "1053/1053 [==============================] - 89s 84ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 304/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 305/400\n",
      "1053/1053 [==============================] - 96s 92ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0474 - val_mean_squared_error: 0.0474\n",
      "Epoch 306/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 307/400\n",
      "1053/1053 [==============================] - 92s 88ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 308/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 309/400\n",
      "1053/1053 [==============================] - 94s 89ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 310/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 311/400\n",
      "1053/1053 [==============================] - 93s 89ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 312/400\n",
      "1053/1053 [==============================] - 91s 87ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 313/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 314/400\n",
      "1053/1053 [==============================] - 93s 89ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 315/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0473 - val_mean_squared_error: 0.0473\n",
      "Epoch 316/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 317/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 318/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 319/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 320/400\n",
      "1053/1053 [==============================] - 88s 84ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 321/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 322/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 323/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 324/400\n",
      "1053/1053 [==============================] - 88s 84ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 325/400\n",
      "1053/1053 [==============================] - 87s 83ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 326/400\n",
      "1053/1053 [==============================] - 89s 84ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 327/400\n",
      "1053/1053 [==============================] - 87s 83ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 328/400\n",
      "1053/1053 [==============================] - 84s 80ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 329/400\n",
      "1053/1053 [==============================] - 85s 81ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 330/400\n",
      "1053/1053 [==============================] - 141s 134ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 331/400\n",
      "1053/1053 [==============================] - 169s 161ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0473 - val_mean_squared_error: 0.0473\n",
      "Epoch 332/400\n",
      "1053/1053 [==============================] - 146s 138ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 333/400\n",
      "1053/1053 [==============================] - 165s 157ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 334/400\n",
      "1053/1053 [==============================] - 162s 154ms/step - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 335/400\n",
      "1053/1053 [==============================] - 165s 157ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 336/400\n",
      "1053/1053 [==============================] - 122s 116ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 337/400\n",
      "1053/1053 [==============================] - 118s 112ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 338/400\n",
      "1053/1053 [==============================] - 162s 153ms/step - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 339/400\n",
      "1053/1053 [==============================] - 124s 118ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0473 - val_mean_squared_error: 0.0473\n",
      "Epoch 340/400\n",
      "1053/1053 [==============================] - 148s 141ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 341/400\n",
      "1053/1053 [==============================] - 137s 130ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 342/400\n",
      "1053/1053 [==============================] - 141s 134ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 343/400\n",
      "1053/1053 [==============================] - 130s 124ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 344/400\n",
      "1053/1053 [==============================] - 99s 94ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 345/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "Epoch 346/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 347/400\n",
      "1053/1053 [==============================] - 95s 90ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 348/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 349/400\n",
      "1053/1053 [==============================] - 94s 90ms/step - loss: 0.0436 - mean_squared_error: 0.0436 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 350/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 351/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 352/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 353/400\n",
      "1053/1053 [==============================] - 92s 88ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 354/400\n",
      "1053/1053 [==============================] - 91s 87ms/step - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 355/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 356/400\n",
      "1053/1053 [==============================] - 89s 85ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 357/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0443 - mean_squared_error: 0.0443 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 358/400\n",
      "1053/1053 [==============================] - 91s 87ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 359/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 360/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 361/400\n",
      "1053/1053 [==============================] - 93s 89ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n",
      "Epoch 362/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 363/400\n",
      "1053/1053 [==============================] - 91s 87ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 364/400\n",
      "1053/1053 [==============================] - 94s 89ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 365/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 366/400\n",
      "1053/1053 [==============================] - 92s 88ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 367/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 368/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 369/400\n",
      "1053/1053 [==============================] - 92s 88ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 370/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 371/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 372/400\n",
      "1053/1053 [==============================] - 91s 87ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 373/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 374/400\n",
      "1053/1053 [==============================] - 94s 89ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 375/400\n",
      "1053/1053 [==============================] - 91s 87ms/step - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 376/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 377/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 378/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0482 - val_mean_squared_error: 0.0482\n",
      "Epoch 379/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0443 - mean_squared_error: 0.0443 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 380/400\n",
      "1053/1053 [==============================] - 91s 87ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 381/400\n",
      "1053/1053 [==============================] - 91s 87ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 382/400\n",
      "1053/1053 [==============================] - 92s 88ms/step - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 383/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 384/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 385/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 386/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 387/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 388/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 389/400\n",
      "1053/1053 [==============================] - 92s 88ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 390/400\n",
      "1053/1053 [==============================] - 91s 87ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 391/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0437 - mean_squared_error: 0.0437 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 392/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 393/400\n",
      "1053/1053 [==============================] - 90s 85ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 394/400\n",
      "1053/1053 [==============================] - 93s 88ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 395/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 396/400\n",
      "1053/1053 [==============================] - 91s 86ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 397/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 398/400\n",
      "1053/1053 [==============================] - 90s 86ms/step - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 399/400\n",
      "1053/1053 [==============================] - 95s 90ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 400/400\n",
      "1053/1053 [==============================] - 92s 87ms/step - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "330/330 [==============================] - 19s 56ms/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'is_scaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5d1455a99975>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"resnet50\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#dnn_model VGG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"resnet50\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_classfication\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpost_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_scaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpredicted_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'regress_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmodel_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"resnet50\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtime_str\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_gray_scale.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mCNN_Regression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./result/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpredicted_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'is_scaler' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred, y_test_ = run_model(X_train_g, y_train, X_val_g, y_val, X_test_g, y_test, model_name=\"resnet50\", model_fn = model_fn,is_classfication=False, nb_classes=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330/330 [==============================] - 38s 114ms/step\n",
      "(330,)\n",
      "(330,)\n",
      "[ 56.97173    39.09628   254.6135    265.0663    524.9647    158.76357\n",
      " 249.61913    65.75686   160.45789   147.94812   222.82823    14.15632\n",
      " 146.38806    55.69239   140.03856    23.388922  112.68035   142.1023\n",
      " 243.96101   155.98552   487.3062    259.1172    275.48492    61.5182\n",
      "  62.932186   58.56553   150.4173     60.131027  121.69925   117.84922\n",
      " 121.12922    92.63883    88.32725    44.95533   133.37387   150.64424\n",
      " 297.5664     30.693964   24.726406  361.60596   256.6332    286.14972\n",
      " 136.44533   231.74408   270.04456   276.81927    44.445755    3.8105621\n",
      "  62.72774   161.20285    35.903336   84.789665  127.295555   20.923145\n",
      " 160.0852    115.94755    81.006294  233.1496    202.4234    -14.231533\n",
      " 229.66307   289.95526    36.227837  436.9232    240.67078   266.18576\n",
      " 259.27576   226.55405   262.921      70.26511   240.27756   -15.753969\n",
      " 175.17888   119.50866    40.634632  189.17473   147.04182   107.22235\n",
      " 111.93927   -18.26769   138.10715   332.8541     70.17914   -10.346174\n",
      "  27.79243   100.07469   100.80367    75.443695   71.763824   80.067764\n",
      " 237.67712     8.665599  198.45572   136.99861   260.0676    268.55728\n",
      "  94.85115   189.17473    93.85043    74.90845   160.36908    78.896576\n",
      "  49.39691    -2.9813573  35.18252    54.685028  123.385574  229.15672\n",
      "  77.87273   -25.145636  -20.258575  185.82008   116.15853    66.32969\n",
      "  73.613434  355.23425    35.769382  262.77365    60.735725  290.04044\n",
      " 242.46678   134.6841    148.2402    222.16037   228.68451   131.4265\n",
      " 301.46332    75.01496   -22.499249  132.60234   238.30983   249.49385\n",
      " 404.72772    42.676254   60.123257  176.6318     80.99797   109.32411\n",
      "  12.419067  498.50235    51.66343    53.214245   35.123787  421.42508\n",
      "  -2.4646297 107.1673    112.14899    84.64153   111.61304   156.79811\n",
      " 262.465      60.686077   91.700035   37.329884  421.28302    58.17435\n",
      " 150.7347     93.646835   36.556393  108.83909    53.890415  260.82385\n",
      "  70.57223   111.36287    37.30306    43.314472  230.90462   109.325066\n",
      " 236.26303    39.26836    91.42462   100.38425    22.530548  148.83441\n",
      " 115.92053   227.12033   260.74194   114.77666   261.0168     41.032417\n",
      " 421.30972   147.88977   105.44954   118.13489    96.82494   109.02225\n",
      " 122.75145   -12.430489  134.28653    32.439545  159.59975   106.728294\n",
      "  30.420319  140.11058   364.3922    -19.193605   88.30911    21.253347\n",
      " 106.37481    54.654743  103.18846   178.50195   149.48904   102.71256\n",
      "  60.23408    62.16182   104.80393   178.5371    106.0615     90.812195\n",
      " 185.79836   163.47078   112.60179   111.99722    76.93234    97.705444\n",
      " 211.92213   267.75516    57.433247  280.7067     80.535675  151.40387\n",
      "  42.661465   94.258575  310.54758    59.69147    29.201672  100.09089\n",
      " 135.01498    68.679245  266.53207    44.512325  229.93538   229.16663\n",
      "  77.33295   108.69445   169.29495    47.475433   84.10004    41.797115\n",
      " 219.19766   107.50848   127.4927    105.537476  326.04257    50.320545\n",
      "  59.326336  136.98387    35.67283   131.52246   157.1631    173.08862\n",
      " 118.38372    52.29273   283.94736   257.3898    107.503815  -17.296017\n",
      "  32.422325   88.37537    72.32146   150.78001   153.48123   493.5658\n",
      " 239.90918    67.758934   96.494156  271.48056   155.9092     95.40089\n",
      " 104.183304  111.60362   264.72763   254.03064    39.413063  124.05318\n",
      " 142.2253    222.16037   119.23465   120.51248   104.813225  210.48882\n",
      " 295.02518   201.15659    78.25232    29.006504  101.43875   -29.559597\n",
      " 300.94116   297.91898   138.09154   141.6755     42.205246   21.45876\n",
      " 292.07736   241.39519   123.963844  130.21252   154.67955   238.14804\n",
      " 201.89244   455.9369     -6.8598313  81.28584   125.29728    40.959194\n",
      " 150.79466    80.98778   404.05444    90.353386   72.151535   15.7682\n",
      " 145.64496    18.042803  202.23781   520.24817   428.63788   108.38302\n",
      "  55.71434   262.8707    235.72046   122.28109   235.80324   271.53403\n",
      "  -8.205593  144.13318    54.642902  152.88696   143.9247     47.96028  ]\n",
      "[  6.  54. 275. 240. 450. 108. 225.  58. 150. 162. 250.  80. 162.  64.\n",
      " 100.  46. 174. 128. 250. 184. 500. 300. 285.  68.  88.  30. 120.   8.\n",
      " 128. 154. 144. 126.  80.  52. 112. 158. 300.  18.  66. 450. 265. 290.\n",
      " 168. 255. 295. 300.  32.  24.  96. 132.   6.  92. 186.   8. 180. 196.\n",
      " 108. 230. 225.   8. 215. 290.  54. 450. 250. 270. 230. 200. 225.  80.\n",
      " 245.   4. 190. 168.  44. 225. 182. 122. 120.   4. 124. 350.  56.   2.\n",
      "  12.  84.  84.  78.  84.  82. 225.  18. 176. 174. 198. 225. 100. 225.\n",
      "  46.  60. 188.  88.  70.  10.  24.  48. 172. 186.  52.  10.   2. 188.\n",
      " 156.  48. 116. 400.  30. 275.  40. 275. 215. 156. 190. 225. 245. 172.\n",
      " 235. 102.   6. 122. 265. 295. 450.  46.  86. 196.  84. 112.  10. 500.\n",
      "  20.  58.  34. 500.  14.  82. 104.  92. 154. 188. 285. 100. 120.  34.\n",
      " 500.  16. 132.  46.  12. 136.  22.   1. 100.  90.  58.  38. 270. 112.\n",
      " 215.  32.  26. 124.  16. 144.  90. 182. 300. 134. 215. 102. 450. 128.\n",
      "  94. 138.  72. 146. 158.   0. 112.  40. 178.  98.  32. 136. 400.   4.\n",
      " 112.  28. 106.  28.  74. 200. 192.  96.  58.  32. 140. 172.  72.  40.\n",
      " 200. 156. 162. 142. 134.  76. 255. 240.  58. 280. 124. 128.  28. 136.\n",
      " 275.  26.  16. 128. 134.  64. 225.  32. 186. 220.  52. 154. 184.  54.\n",
      " 114.  18. 220.  94. 130.  60. 295.  74.  60. 164.  10. 198. 138. 200.\n",
      " 128.  74. 350. 285. 102.   2.   4.  92.  86. 154. 190. 500. 186.  42.\n",
      "  62. 275. 110. 124. 116.  98. 275. 265.  28. 182. 198. 225. 170. 154.\n",
      "  86. 235. 300. 240.  84.  36. 150.  14. 300. 250. 164. 160.  78.  22.\n",
      " 300. 230. 112. 118. 118. 250. 178. 500.   0.  90. 120.  80. 186.  90.\n",
      " 450.  74.  64.  24. 140.  36. 215. 450. 350. 136.  68. 290. 220. 116.\n",
      " 300. 290.   8. 120.  48. 188. 164.  86.]\n",
      "mse =  1048.1315974765887\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('./modelWights/weights_resnet50_fold_11570509297.3577418_gray_scale.h5')\n",
    "y_pred = model.predict(X_test, batch_size=1, verbose=1)\n",
    "y_test_ = y_test\n",
    "time_str = str(time.time())\n",
    "y_pred, y_test_ = post_training_data(y_test_, y_pred)\n",
    "predicted_file = 'regress_'+model_fn.__name__+\"resnet50\"+time_str+'_gray_scale.csv'\n",
    "save_result(\"./result/\"+predicted_file,y_pred,y_test_)\n",
    "# draw_scatter(predicted_file, \"resnet101\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330/330 [==============================] - 26s 78ms/step\n",
      "(330,)\n",
      "(330,)\n",
      "[ 56.97173    39.09628   254.6135    265.0663    524.9647    158.76357\n",
      " 249.61913    65.75686   160.45789   147.94812   222.82823    14.15632\n",
      " 146.38806    55.69239   140.03856    23.388922  112.68035   142.1023\n",
      " 243.96101   155.98552   487.3062    259.1172    275.48492    61.5182\n",
      "  62.932186   58.56553   150.4173     60.131027  121.69925   117.84922\n",
      " 121.12922    92.63883    88.32725    44.95533   133.37387   150.64424\n",
      " 297.5664     30.693964   24.726406  361.60596   256.6332    286.14972\n",
      " 136.44533   231.74408   270.04456   276.81927    44.445755    3.8105621\n",
      "  62.72774   161.20285    35.903336   84.789665  127.295555   20.923145\n",
      " 160.0852    115.94755    81.006294  233.1496    202.4234    -14.231533\n",
      " 229.66307   289.95526    36.227837  436.9232    240.67078   266.18576\n",
      " 259.27576   226.55405   262.921      70.26511   240.27756   -15.753969\n",
      " 175.17888   119.50866    40.634632  189.17473   147.04182   107.22235\n",
      " 111.93927   -18.26769   138.10715   332.8541     70.17914   -10.346174\n",
      "  27.79243   100.07469   100.80367    75.443695   71.763824   80.067764\n",
      " 237.67712     8.665599  198.45572   136.99861   260.0676    268.55728\n",
      "  94.85115   189.17473    93.85043    74.90845   160.36908    78.896576\n",
      "  49.39691    -2.9813573  35.18252    54.685028  123.385574  229.15672\n",
      "  77.87273   -25.145636  -20.258575  185.82008   116.15853    66.32969\n",
      "  73.613434  355.23425    35.769382  262.77365    60.735725  290.04044\n",
      " 242.46678   134.6841    148.2402    222.16037   228.68451   131.4265\n",
      " 301.46332    75.01496   -22.499249  132.60234   238.30983   249.49385\n",
      " 404.72772    42.676254   60.123257  176.6318     80.99797   109.32411\n",
      "  12.419067  498.50235    51.66343    53.214245   35.123787  421.42508\n",
      "  -2.4646297 107.1673    112.14899    84.64153   111.61304   156.79811\n",
      " 262.465      60.686077   91.700035   37.329884  421.28302    58.17435\n",
      " 150.7347     93.646835   36.556393  108.83909    53.890415  260.82385\n",
      "  70.57223   111.36287    37.30306    43.314472  230.90462   109.325066\n",
      " 236.26303    39.26836    91.42462   100.38425    22.530548  148.83441\n",
      " 115.92053   227.12033   260.74194   114.77666   261.0168     41.032417\n",
      " 421.30972   147.88977   105.44954   118.13489    96.82494   109.02225\n",
      " 122.75145   -12.430489  134.28653    32.439545  159.59975   106.728294\n",
      "  30.420319  140.11058   364.3922    -19.193605   88.30911    21.253347\n",
      " 106.37481    54.654743  103.18846   178.50195   149.48904   102.71256\n",
      "  60.23408    62.16182   104.80393   178.5371    106.0615     90.812195\n",
      " 185.79836   163.47078   112.60179   111.99722    76.93234    97.705444\n",
      " 211.92213   267.75516    57.433247  280.7067     80.535675  151.40387\n",
      "  42.661465   94.258575  310.54758    59.69147    29.201672  100.09089\n",
      " 135.01498    68.679245  266.53207    44.512325  229.93538   229.16663\n",
      "  77.33295   108.69445   169.29495    47.475433   84.10004    41.797115\n",
      " 219.19766   107.50848   127.4927    105.537476  326.04257    50.320545\n",
      "  59.326336  136.98387    35.67283   131.52246   157.1631    173.08862\n",
      " 118.38372    52.29273   283.94736   257.3898    107.503815  -17.296017\n",
      "  32.422325   88.37537    72.32146   150.78001   153.48123   493.5658\n",
      " 239.90918    67.758934   96.494156  271.48056   155.9092     95.40089\n",
      " 104.183304  111.60362   264.72763   254.03064    39.413063  124.05318\n",
      " 142.2253    222.16037   119.23465   120.51248   104.813225  210.48882\n",
      " 295.02518   201.15659    78.25232    29.006504  101.43875   -29.559597\n",
      " 300.94116   297.91898   138.09154   141.6755     42.205246   21.45876\n",
      " 292.07736   241.39519   123.963844  130.21252   154.67955   238.14804\n",
      " 201.89244   455.9369     -6.8598313  81.28584   125.29728    40.959194\n",
      " 150.79466    80.98778   404.05444    90.353386   72.151535   15.7682\n",
      " 145.64496    18.042803  202.23781   520.24817   428.63788   108.38302\n",
      "  55.71434   262.8707    235.72046   122.28109   235.80324   271.53403\n",
      "  -8.205593  144.13318    54.642902  152.88696   143.9247     47.96028  ]\n",
      "[  6.  54. 275. 240. 450. 108. 225.  58. 150. 162. 250.  80. 162.  64.\n",
      " 100.  46. 174. 128. 250. 184. 500. 300. 285.  68.  88.  30. 120.   8.\n",
      " 128. 154. 144. 126.  80.  52. 112. 158. 300.  18.  66. 450. 265. 290.\n",
      " 168. 255. 295. 300.  32.  24.  96. 132.   6.  92. 186.   8. 180. 196.\n",
      " 108. 230. 225.   8. 215. 290.  54. 450. 250. 270. 230. 200. 225.  80.\n",
      " 245.   4. 190. 168.  44. 225. 182. 122. 120.   4. 124. 350.  56.   2.\n",
      "  12.  84.  84.  78.  84.  82. 225.  18. 176. 174. 198. 225. 100. 225.\n",
      "  46.  60. 188.  88.  70.  10.  24.  48. 172. 186.  52.  10.   2. 188.\n",
      " 156.  48. 116. 400.  30. 275.  40. 275. 215. 156. 190. 225. 245. 172.\n",
      " 235. 102.   6. 122. 265. 295. 450.  46.  86. 196.  84. 112.  10. 500.\n",
      "  20.  58.  34. 500.  14.  82. 104.  92. 154. 188. 285. 100. 120.  34.\n",
      " 500.  16. 132.  46.  12. 136.  22.   1. 100.  90.  58.  38. 270. 112.\n",
      " 215.  32.  26. 124.  16. 144.  90. 182. 300. 134. 215. 102. 450. 128.\n",
      "  94. 138.  72. 146. 158.   0. 112.  40. 178.  98.  32. 136. 400.   4.\n",
      " 112.  28. 106.  28.  74. 200. 192.  96.  58.  32. 140. 172.  72.  40.\n",
      " 200. 156. 162. 142. 134.  76. 255. 240.  58. 280. 124. 128.  28. 136.\n",
      " 275.  26.  16. 128. 134.  64. 225.  32. 186. 220.  52. 154. 184.  54.\n",
      " 114.  18. 220.  94. 130.  60. 295.  74.  60. 164.  10. 198. 138. 200.\n",
      " 128.  74. 350. 285. 102.   2.   4.  92.  86. 154. 190. 500. 186.  42.\n",
      "  62. 275. 110. 124. 116.  98. 275. 265.  28. 182. 198. 225. 170. 154.\n",
      "  86. 235. 300. 240.  84.  36. 150.  14. 300. 250. 164. 160.  78.  22.\n",
      " 300. 230. 112. 118. 118. 250. 178. 500.   0.  90. 120.  80. 186.  90.\n",
      " 450.  74.  64.  24. 140.  36. 215. 450. 350. 136.  68. 290. 220. 116.\n",
      " 300. 290.   8. 120.  48. 188. 164.  86.]\n",
      "mse =  1048.1315974765887\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_g, batch_size=1, verbose=1)\n",
    "y_test_ = y_test\n",
    "time_str = str(time.time())\n",
    "y_pred, y_test_ = post_training_data(y_test_, y_pred)\n",
    "predicted_file = 'regress_'+model_fn.__name__+\"resnet50\"+time_str+'_gray_scale_pred_color.csv'\n",
    "save_result(\"./result/\"+predicted_file,y_pred,y_test_)\n",
    "# draw_scatter(predicted_file, \"resnet101\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9e83793ef0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "get_layer_output = K.function([model.layers[0].input, K.learning_phase()],[model.layers[-3].output])\n",
    "layer_output = get_layer_output([X_train[2].reshape(1,512,512,3), 0])[0]\n",
    "plt.imshow(layer_output[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-369.41626 -526.9378  -430.79715 ... -362.27216 -358.80005 -254.3877 ]\n",
      " [-519.6171  -739.9861  -589.9607  ... -488.06784 -481.176   -342.8683 ]\n",
      " [-482.4566  -677.2405  -518.43726 ... -421.33786 -411.26694 -290.80206]\n",
      " ...\n",
      " [-469.83298 -654.7402  -492.50415 ... -401.03516 -389.57565 -274.44693]\n",
      " [-465.01306 -644.79114 -479.0014  ... -389.45715 -381.78152 -269.61908]\n",
      " [-310.00427 -432.5793  -319.30478 ... -258.98465 -255.72847 -181.82275]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_3rd_layer_output = K.function([model.layers[0].input, K.learning_phase()],\n",
    "                                  [model.layers[10].output])\n",
    "# output in test mode = 0\n",
    "layer_output = get_3rd_layer_output([X_train[2].reshape(1,512,512,3), 0])[0]\n",
    "imshow(layer_output[0,:,:,0])\n",
    "print(layer_output[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa0d0edff28>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD4CAYAAAAjDTByAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAASUklEQVR4nO3dfYxc1XnH8e8zu95dr1/XBoLBBtsIgQCRQg0loYI0jqmhCCdtqhqV1oVUEW1poWoUjJCSqH81JU1fEhTkAC1pKUQlECxkGhzy1kjBARxj7JiATRxjbGyD39Zrr3dn5ukfc43Gy64958y9Fzvn95FWOztznz3P3tnf3Hk7c8zdEZH0VN7vBkTk/aHwiyRK4RdJlMIvkiiFXyRRnWUO1jV1vI8/fXJwXd0tuGZG177gGoBJlXpwTZ24V0xiX2eJqot8VadG+L4HqEbUHa6PixrLLPxvm1QZjhor9mi5L+Jv84h9+Pabh+nfPdxSYanhH3/6ZK5c9kfBdYeq4TvuztlPB9cA/M74weCawx73jzTs4Tc0AMMR8a9Fhr8/4oYXYFdtfHDNpqHTosbqslpwzUd7t0aN1WsdUXVPDcwIrqlF3NR87vfXtbyt7vaLJErhF0lUW+E3s4Vm9gsz22hmS/NqSkSKFx1+M+sA7gWuBS4AbjSzC/JqTESK1c6R/3Jgo7u/7u5DwKPAonzaEpGitRP+M4E3mn7emp13FDP7tJm9YGYvDO091MZwIpKndsI/2mtA73k9yd2Xufs8d5/XNTX85R8RKUY74d8KzGr6eSawrb12RKQs7YT/eeBcM5tjZl3AYmB5Pm2JSNGi3+Hn7lUzuw34DtABPOju63PrTEQK1dbbe919BbAip15EpER6h59Iokqd2FN3Y2C4K7jurb3hMwHXnT7r+BuN4vyutcE1m6sTo8Z6tv/CqLpntp0fXLP3QG/UWId3xNVN2Bw+AWb8rrjJR9WIFr+6aEfUWI9e8I2ouphJOrWYSVUBu1BHfpFEKfwiiVL4RRKl8IskSuEXSZTCL5IohV8kUQq/SKIUfpFEKfwiiVL4RRKl8IskqtSJPd2VKnMnvRNct3N/+MSZ/lpPcA3AK0N9wTW3Pn1L1FinvBh32ztxW/gKQRMPVqPG6uzfH1VX2XsguMYHD0eNZT3dwTVv1WdGjfXUWedF1V02/pdRdaF6ApYh05FfJFEKv0iiFH6RRLWzYs8sM/u+mW0ws/VmdnuejYlIsdp5wq8K/J27rzazScCLZrbS3X+eU28iUqDoI7+7b3f31dnpfmADo6zYIyInplwe85vZbOASYNUol727XNfg3sE8hhORHLQdfjObCHwLuMPd3/OicPNyXT1T4157F5H8tRV+MxtHI/gPu/vj+bQkImVo59l+Ax4ANrj7l/NrSUTK0M6R/0rgT4CPmtma7Ou6nPoSkYK1s1bfjxl9mW4ROQnoHX4iiSp1Vl+n1ZnWNRBc19sdPottnNWCawDu+dXC4Jo5346bMde9ZU9UnR0Kn/3mw+H7EIDDcTPt6kPh4/lw3H60jvBj2Kk/DV8CDuBLaxZE1a248t7gmrM7w5e2Gx9wX1xHfpFEKfwiiVL4RRKl8IskSuEXSZTCL5IohV8kUQq/SKIUfpFEKfwiiVL4RRKl8IskqtSJPR1WZ0rnoeC6CV1DwTVTOg8G1wC8M9AbXHPa/vD+AKwaN/kI9/CaekRNG3VeC//bvBo3+chr4cewji3bo8aa8sO45bqevPji4Jo7+l4NrgmZY68jv0iiFH6RRCn8IonK46O7O8zsZ2b2VB4NiUg58jjy305jtR4ROYm0+7n9M4HfA+7Ppx0RKUu7R/5/AT4L1HPoRURK1M6iHdcDO939xeNs9+5afQN74l4PF5H8tbtoxw1mthl4lMbiHf81cqPmtfom9IV/GqmIFKOdJbrvcveZ7j4bWAx8z91vyq0zESmUXucXSVQu7+139x8AP8jjd4lIOXTkF0lUqbP6KlZnYsdgcF1PZ/hsr+kdB4JrAKr1iNvDauQrnTGz8wDqEeP5r/GrsRF/W31/3P/H9LVxs0Xv+274Ml8LFv08uOZQwP+UjvwiiVL4RRKl8IskSuEXSZTCL5IohV8kUQq/SKIUfpFEKfwiiVL4RRKl8IskSuEXSZTCL5Kocmf14fRY+Ay9ro7wdd8mdYSvCQhQj5jVZxHr0jUKQ1ZWa1KJuM22uNt5j555GFEXO1aE2HUBOzdui6qb8X9zg2seufq3gmt2155teVsd+UUSpfCLJErhF0lUuyv2TDWzx8zsFTPbYGYfyqsxESlWu0/4/Svwv+7+STPrAnpz6ElEShAdfjObDFwF/BmAuw8BWpJH5CTRzt3+ucAu4N+zJbrvN7MJIzdqXq7rwJ64l1dEJH/thL8TuBT4mrtfAgwAS0du1Lxc18S+cW0MJyJ5aif8W4Gt7r4q+/kxGjcGInISaGetvreAN8zsvOys+UD4B42LyPui3Wf7/xp4OHum/3Xg5vZbEpEytBV+d18DzMupFxEpUakTewwYZ+GTYLoq1eCamAlEUOrckviJPbF1UUPFjVXmbowSe0UPx72aPXFL+DJfL7xzVnDNQLWr5W319l6RRCn8IolS+EUSpfCLJErhF0mUwi+SKIVfJFEKv0iiFH6RRCn8IolS+EUSpfCLJErhF0lUqbP6wOmwenBVxcJnYHWUOK/MOzri6jrj6mw4fJYjlfJmAkaPFztbMWaGXvSMyrjjZa03PGpTusKXnAvJl478IolS+EUSpfCLJKrd5br+1szWm9k6M3vEzHryakxEihUdfjM7E/gbYJ67XwR0AIvzakxEitXu3f5OYLyZddJYp29b+y2JSBna+dz+N4EvAVuA7cA+d39m5HZarkvkxNTO3f4+YBEwBzgDmGBmN43cTst1iZyY2rnb/zHgl+6+y92HgceBD+fTlogUrZ3wbwGuMLNea3y4+3xgQz5tiUjR2nnMv4rG4pyrgZez37Usp75EpGDtLtf1eeDzOfUiIiXSO/xEElXqrD7HGKyHP+NfrYffRtWIm7XV0xX+cmS9Z3zUWJWhyN1fDV/v0OrhsykhfsZizBp/HjljDiL+tsixrKc7qu7tC8P/R/7w1PXBNT/rbH0moI78IolS+EUSpfCLJErhF0mUwi+SKIVfJFEKv0iiFH6RRCn8IolS+EUSpfCLJErhF0lUqRN76lQ4WA+fGDFYC58MNBAxDsCk7qHgmlrvpKixKoNxu78SM7GnFl4DQFfcR6/FjGcxy24BxPxtkROWamdMj6rbe9nh4JoFva8G19xbGWx5Wx35RRKl8IskSuEXSdRxw29mD5rZTjNb13TeNDNbaWavZd/7im1TRPLWypH/P4CFI85bCjzr7ucCz2Y/i8hJ5Ljhd/cfAbtHnL0IeCg7/RDw8Zz7EpGCxT7m/4C7bwfIvp821oZHLde1O/xlNBEpRuFP+B21XNe0rqKHE5EWxYZ/h5nNAMi+78yvJREpQ2z4lwNLstNLgCfzaUdEytLKS32PAD8BzjOzrWb2KeAfgAVm9hqwIPtZRE4ix31zubvfOMZF83PuRURKpHf4iSSq3Fl9bhyshz/jf7gW3mZ/PW4Jrcndrc+KOmLP5Ljd2HEwri526a2osSqRx4eIukolbok1HwpfYs064/b9rg9Ojqq7dd53g2umRcw87AxYJk1HfpFEKfwiiVL4RRKl8IskSuEXSZTCL5IohV8kUQq/SKIUfpFEKfwiiVL4RRKl8IskqtSJPTUq9Nd6gusODocvGRUzDsCV0zYF1/z32XOjxsLieqyNC5+0dGh63O185G6kd0f45KOprw5EjdWxdVd4UeSEpb3nRZUxr/f1uMIC6cgvkiiFXyRRCr9IomKX67rHzF4xs7Vm9oSZTS22TRHJW+xyXSuBi9z9YuBV4K6c+xKRgkUt1+Xuz7h7NfvxOWBmAb2JSIHyeMx/C/D0WBc2L9d1cM/hHIYTkTy0FX4zuxuoAg+PtU3zcl29fd3tDCciOYp+k4+ZLQGuB+a7u+fXkoiUISr8ZrYQuBO42t0P5tuSiJQhdrmurwKTgJVmtsbM7iu4TxHJWexyXQ8U0IuIlEjv8BNJVKmz+qr1CruHJgTXHRgMf5Wgvx43He0Tk9cE1wzeHD7rEODHb58TVdfXHf40y8WT34waa8a4vVF1b1cnBdfc95OPRI111vKzgmu634l72bnvorej6gbq4f/DW6sHgmuGAp5615FfJFEKv0iiFH6RRCn8IolS+EUSpfCLJErhF0mUwi+SKIVfJFEKv0iiFH6RRCn8IolS+EUSVe5afW7sGw6fbXfoUFdwzcFa3OcF9lj4J5L9xbTno8aKreuvh/c44HFXdd0tqi7G2VfHzZi7q/IHwTUTNobPLgX489k/jKo7GDGrb9Pw9OCaw777+BtldOQXSZTCL5KoqOW6mi77jJm5mZ1STHsiUpTY5bows1nAAmBLzj2JSAmiluvK/DPwWUCf2S9yEop6zG9mNwBvuvtLLWz77nJdh/cOxgwnIgUIfv3HzHqBu4FrWtne3ZcBywD6zj9V9xJEThAxR/5zgDnAS2a2mcYKvavN7PQ8GxORYgUf+d39ZeC0Iz9nNwDz3D3uHRoi8r6IXa5LRE5ysct1NV8+O7duRKQ0eoefSKJKndgzoWOI35wS/p6grtm14Jo53TuDawB6LHwiy/TK+KixOiz2tncguGJvNW6s2GXPplYOBddc1vNG1FhPfewrwTUrrrgoaqybJq+PqttW6wiu6a+HT2jrtNazoiO/SKIUfpFEKfwiiVL4RRKl8IskSuEXSZTCL5IohV8kUQq/SKIUfpFEKfwiiVL4RRKl8IskytzL+1g9M9sF/GqMi08BToRPA1IfR1MfRzvR+zjb3U9t5ReUGv5jMbMX3H2e+lAf6qOcPnS3XyRRCr9Iok6k8C97vxvIqI+jqY+j/dr0ccI85heRcp1IR34RKZHCL5KoUsNvZgvN7BdmttHMlo5yebeZfTO7fJWZzS6gh1lm9n0z22Bm683s9lG2+YiZ7TOzNdnX5/Luo2mszWb2cjbOC6Ncbmb2b9k+WWtml+Y8/nlNf+caM9tvZneM2Kaw/WFmD5rZTjNb13TeNDNbaWavZd/7xqhdkm3zmpktKaCPe8zslWy/P2FmU8eoPeZ1mEMfXzCzN5v2/3Vj1B4zX+/h7qV8AR3AJmAu0AW8BFwwYpu/BO7LTi8GvllAHzOAS7PTk4BXR+njI8BTJe2XzcApx7j8OuBpwIArgFUFX0dv0XijSCn7A7gKuBRY13TePwJLs9NLgS+OUjcNeD373ped7su5j2uAzuz0F0fro5XrMIc+vgB8poXr7pj5GvlV5pH/cmCju7/u7kPAo8CiEdssAh7KTj8GzDeL+CD9Y3D37e6+OjvdD2wAzsxzjJwtAr7hDc8BU81sRkFjzQc2uftY78LMnbv/CNg94uzm/4OHgI+PUvq7wEp33+3ue4CVwMI8+3D3Z9y9mv34HI1FaQs1xv5oRSv5OkqZ4T8TaF6VYSvvDd2722Q7fR8wvaiGsocVlwCrRrn4Q2b2kpk9bWYXFtUD4MAzZvaimX16lMtb2W95WQw8MsZlZe0PgA+4+3Zo3FjTtDBskzL3C8AtNO6BjeZ412Eebssefjw4xsOg4P1RZvhHO4KPfJ2xlW1yYWYTgW8Bd7j7/hEXr6Zx1/eDwFeAbxfRQ+ZKd78UuBb4KzO7amSro9Tkvk/MrAu4AfifUS4uc3+0qsz/lbuBKvDwGJsc7zps19eAc4DfALYD/zRam6Ocd8z9UWb4twKzmn6eCWwbaxsz6wSmEHcX6JjMbByN4D/s7o+PvNzd97v7gez0CmCcmZ2Sdx/Z79+Wfd8JPEHj7luzVvZbHq4FVrv7jlF6LG1/ZHYceWiTfR9t7bVS9kv2ROL1wB979uB6pBauw7a4+w53r7l7Hfj6GL8/eH+UGf7ngXPNbE52lFkMLB+xzXLgyLO2nwS+N9YOj5U9h/AAsMHdvzzGNqcfea7BzC6nsZ/eybOP7HdPMLNJR07TeIJp3YjNlgN/mj3rfwWw78hd4pzdyBh3+cvaH02a/w+WAE+Oss13gGvMrC+7G3xNdl5uzGwhcCdwg7sfHGObVq7Ddvtofo7nE2P8/lbydbQ8nqEMeCbzOhrPrm8C7s7O+3saOxegh8bdzo3AT4G5BfTw2zTuDq0F1mRf1wG3Ardm29wGrKfxjOlzwIcL2h9zszFeysY7sk+aezHg3myfvQzMK6CPXhphntJ0Xin7g8YNznZgmMbR61M0nud5Fngt+z4t23YecH9T7S3Z/8pG4OYC+thI43H0kf+TI69EnQGsONZ1mHMf/5ld92tpBHrGyD7GytexvvT2XpFE6R1+IolS+EUSpfCLJErhF0mUwi+SKIVfJFEKv0ii/h+FpEOG7FrDbgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_layer_output = K.function([model.layers[0].input, K.learning_phase()],[model.layers[-3].output])\n",
    "layer_output = get_layer_output([X_train[0].reshape(1,512,512,3), 0])[0]\n",
    "plt.imshow(layer_output[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa0d0e40c88>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_layer_output = K.function([model.layers[0].input, K.learning_phase()],[model.layers[-3].output])\n",
    "layer_output = get_layer_output([X_test[0].reshape(1,512,512,3), 0])[0]\n",
    "plt.imshow(layer_output[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-34127f8dda37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_layer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_layer_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test_g\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "get_layer_output = K.function([model.layers[0].input],[model.layers[-3].output])\n",
    "layer_output = get_layer_output([X_test_g[1].reshape(1,512,512,3), 0])[1]\n",
    "plt.imshow(layer_output[0,:,:,0])\n",
    "print(layer_output[0,:,:,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dltdc/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 512, 512, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 518, 518, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 256, 256, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 256, 256, 64) 256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 256, 256, 64) 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 258, 258, 64) 0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 128, 128, 64) 0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 128, 128, 64) 4160        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 128, 128, 64) 256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 128, 128, 64) 0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 128, 128, 64) 36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 128, 128, 64) 256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 128, 128, 64) 0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 128, 128, 256 16640       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 128, 128, 256 16640       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 128, 128, 256 1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 128, 128, 256 1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 128, 128, 256 0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 128, 128, 256 0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 128, 128, 64) 16448       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 128, 128, 64) 256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 128, 128, 64) 0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 128, 128, 64) 36928       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 128, 128, 64) 256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 128, 128, 64) 0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 128, 128, 256 16640       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 128, 128, 256 1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 128, 128, 256 0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128, 128, 256 0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 128, 128, 64) 16448       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 128, 128, 64) 256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 128, 128, 64) 0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 128, 128, 64) 36928       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 128, 128, 64) 256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 128, 128, 64) 0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 128, 128, 256 16640       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 128, 128, 256 1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 128, 128, 256 0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 128, 128, 256 0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 64, 64, 128)  32896       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 64, 64, 128)  0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 64, 64, 128)  0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 64, 64, 512)  131584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 64, 64, 512)  2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 64, 64, 512)  0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 64, 64, 512)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 64, 64, 128)  65664       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 128)  0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 128)  0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 64, 64, 512)  0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 64, 64, 512)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 64, 64, 128)  65664       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 64, 64, 128)  0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 64, 64, 128)  0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 64, 64, 512)  0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 64, 64, 512)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 64, 64, 128)  65664       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 64, 64, 128)  0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 64, 64, 128)  0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 64, 64, 512)  0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 64, 64, 512)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 32, 32, 256)  131328      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 32, 32, 256)  0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 32, 32, 256)  0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 32, 32, 1024) 525312      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 32, 32, 1024) 4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 32, 32, 1024) 0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 32, 32, 1024) 0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 32, 32, 256)  0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 32, 32, 256)  0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 32, 32, 1024) 0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 32, 32, 1024) 0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 32, 32, 256)  0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 32, 32, 256)  0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 32, 32, 1024) 0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 32, 32, 1024) 0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 32, 32, 256)  0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 32, 32, 256)  0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 32, 32, 1024) 0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 32, 32, 1024) 0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 32, 32, 256)  0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 32, 32, 256)  0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 32, 32, 1024) 0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 32, 32, 1024) 0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 32, 32, 256)  0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 32, 32, 256)  0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 32, 32, 1024) 0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 32, 32, 1024) 0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 16, 16, 512)  524800      activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 16, 16, 512)  2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 16, 16, 512)  0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 16, 16, 512)  2359808     activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 16, 16, 512)  2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 16, 16, 512)  0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 16, 16, 2048) 1050624     activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 16, 16, 2048) 2099200     activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, 16, 16, 2048) 8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 16, 16, 2048) 8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 16, 16, 2048) 0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 16, 16, 2048) 0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 16, 16, 512)  1049088     activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 16, 16, 512)  2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 16, 16, 512)  0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 16, 16, 512)  2359808     activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 16, 16, 512)  2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16, 16, 512)  0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 16, 16, 2048) 1050624     activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, 16, 16, 2048) 8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 16, 16, 2048) 0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 16, 16, 2048) 0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 16, 16, 512)  1049088     activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 16, 16, 512)  2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 16, 16, 512)  0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 16, 16, 512)  2359808     activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 16, 16, 512)  2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 16, 16, 512)  0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, 16, 16, 2048) 1050624     activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, 16, 16, 2048) 8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 16, 16, 2048) 0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 16, 16, 2048) 0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 1)            2049        avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 23,589,761\n",
      "Trainable params: 23,536,641\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pic = X_train[1]\n",
    "model = Sequential()\n",
    "#model.add(Input(shape=pic.shape))\n",
    "# model.add(Conv2D(filters=4, kernel_size=(7,7), strides=1, input_shape=pic.shape))\n",
    "Kerasmodel = keras.applications.resnet50.ResNet50(include_top=True\n",
    "                , weights=None#'imagenet'\n",
    "                # , input_tensor=inputs\n",
    "                , input_shape=pic.shape\n",
    "                )\n",
    "# Kerasmodel._layers.pop()\n",
    "# Kerasmodel.layers.pop()\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(1,kernel_initializer='he_normal'))\n",
    "model.add(Activation('linear')) #softmax\n",
    "model = Model( inputs=Kerasmodel.input , outputs=model(Kerasmodel.layers[-2].output))\n",
    "model.compile(loss='mean_squared_error', #mse binary_crossentropy Dice-coefficient loss function vs cross-entropy\n",
    "                optimizer=optimizers.Adam(lr=5e-3),\n",
    "                metrics=['mse'])\n",
    "\n",
    "model.summary()     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## resnet 50 color without pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1570924433.358989\n",
      "LAYER:: input_1\n",
      "LAYER:: conv1_pad\n",
      "LAYER:: conv1\n",
      "reinitializing layer conv1.kernel\n",
      "reinitializing layer conv1.bias\n",
      "LAYER:: bn_conv1\n",
      "reinitializing layer bn_conv1.gamma\n",
      "reinitializing layer bn_conv1.beta\n",
      "reinitializing layer bn_conv1.moving_mean\n",
      "reinitializing layer bn_conv1.moving_variance\n",
      "LAYER:: activation_1\n",
      "LAYER:: pool1_pad\n",
      "LAYER:: max_pooling2d_1\n",
      "LAYER:: res2a_branch2a\n",
      "reinitializing layer res2a_branch2a.kernel\n",
      "reinitializing layer res2a_branch2a.bias\n",
      "LAYER:: bn2a_branch2a\n",
      "reinitializing layer bn2a_branch2a.gamma\n",
      "reinitializing layer bn2a_branch2a.beta\n",
      "reinitializing layer bn2a_branch2a.moving_mean\n",
      "reinitializing layer bn2a_branch2a.moving_variance\n",
      "LAYER:: activation_2\n",
      "LAYER:: res2a_branch2b\n",
      "reinitializing layer res2a_branch2b.kernel\n",
      "reinitializing layer res2a_branch2b.bias\n",
      "LAYER:: bn2a_branch2b\n",
      "reinitializing layer bn2a_branch2b.gamma\n",
      "reinitializing layer bn2a_branch2b.beta\n",
      "reinitializing layer bn2a_branch2b.moving_mean\n",
      "reinitializing layer bn2a_branch2b.moving_variance\n",
      "LAYER:: activation_3\n",
      "LAYER:: res2a_branch2c\n",
      "reinitializing layer res2a_branch2c.kernel\n",
      "reinitializing layer res2a_branch2c.bias\n",
      "LAYER:: res2a_branch1\n",
      "reinitializing layer res2a_branch1.kernel\n",
      "reinitializing layer res2a_branch1.bias\n",
      "LAYER:: bn2a_branch2c\n",
      "reinitializing layer bn2a_branch2c.gamma\n",
      "reinitializing layer bn2a_branch2c.beta\n",
      "reinitializing layer bn2a_branch2c.moving_mean\n",
      "reinitializing layer bn2a_branch2c.moving_variance\n",
      "LAYER:: bn2a_branch1\n",
      "reinitializing layer bn2a_branch1.gamma\n",
      "reinitializing layer bn2a_branch1.beta\n",
      "reinitializing layer bn2a_branch1.moving_mean\n",
      "reinitializing layer bn2a_branch1.moving_variance\n",
      "LAYER:: add_1\n",
      "LAYER:: activation_4\n",
      "LAYER:: res2b_branch2a\n",
      "reinitializing layer res2b_branch2a.kernel\n",
      "reinitializing layer res2b_branch2a.bias\n",
      "LAYER:: bn2b_branch2a\n",
      "reinitializing layer bn2b_branch2a.gamma\n",
      "reinitializing layer bn2b_branch2a.beta\n",
      "reinitializing layer bn2b_branch2a.moving_mean\n",
      "reinitializing layer bn2b_branch2a.moving_variance\n",
      "LAYER:: activation_5\n",
      "LAYER:: res2b_branch2b\n",
      "reinitializing layer res2b_branch2b.kernel\n",
      "reinitializing layer res2b_branch2b.bias\n",
      "LAYER:: bn2b_branch2b\n",
      "reinitializing layer bn2b_branch2b.gamma\n",
      "reinitializing layer bn2b_branch2b.beta\n",
      "reinitializing layer bn2b_branch2b.moving_mean\n",
      "reinitializing layer bn2b_branch2b.moving_variance\n",
      "LAYER:: activation_6\n",
      "LAYER:: res2b_branch2c\n",
      "reinitializing layer res2b_branch2c.kernel\n",
      "reinitializing layer res2b_branch2c.bias\n",
      "LAYER:: bn2b_branch2c\n",
      "reinitializing layer bn2b_branch2c.gamma\n",
      "reinitializing layer bn2b_branch2c.beta\n",
      "reinitializing layer bn2b_branch2c.moving_mean\n",
      "reinitializing layer bn2b_branch2c.moving_variance\n",
      "LAYER:: add_2\n",
      "LAYER:: activation_7\n",
      "LAYER:: res2c_branch2a\n",
      "reinitializing layer res2c_branch2a.kernel\n",
      "reinitializing layer res2c_branch2a.bias\n",
      "LAYER:: bn2c_branch2a\n",
      "reinitializing layer bn2c_branch2a.gamma\n",
      "reinitializing layer bn2c_branch2a.beta\n",
      "reinitializing layer bn2c_branch2a.moving_mean\n",
      "reinitializing layer bn2c_branch2a.moving_variance\n",
      "LAYER:: activation_8\n",
      "LAYER:: res2c_branch2b\n",
      "reinitializing layer res2c_branch2b.kernel\n",
      "reinitializing layer res2c_branch2b.bias\n",
      "LAYER:: bn2c_branch2b\n",
      "reinitializing layer bn2c_branch2b.gamma\n",
      "reinitializing layer bn2c_branch2b.beta\n",
      "reinitializing layer bn2c_branch2b.moving_mean\n",
      "reinitializing layer bn2c_branch2b.moving_variance\n",
      "LAYER:: activation_9\n",
      "LAYER:: res2c_branch2c\n",
      "reinitializing layer res2c_branch2c.kernel\n",
      "reinitializing layer res2c_branch2c.bias\n",
      "LAYER:: bn2c_branch2c\n",
      "reinitializing layer bn2c_branch2c.gamma\n",
      "reinitializing layer bn2c_branch2c.beta\n",
      "reinitializing layer bn2c_branch2c.moving_mean\n",
      "reinitializing layer bn2c_branch2c.moving_variance\n",
      "LAYER:: add_3\n",
      "LAYER:: activation_10\n",
      "LAYER:: res3a_branch2a\n",
      "reinitializing layer res3a_branch2a.kernel\n",
      "reinitializing layer res3a_branch2a.bias\n",
      "LAYER:: bn3a_branch2a\n",
      "reinitializing layer bn3a_branch2a.gamma\n",
      "reinitializing layer bn3a_branch2a.beta\n",
      "reinitializing layer bn3a_branch2a.moving_mean\n",
      "reinitializing layer bn3a_branch2a.moving_variance\n",
      "LAYER:: activation_11\n",
      "LAYER:: res3a_branch2b\n",
      "reinitializing layer res3a_branch2b.kernel\n",
      "reinitializing layer res3a_branch2b.bias\n",
      "LAYER:: bn3a_branch2b\n",
      "reinitializing layer bn3a_branch2b.gamma\n",
      "reinitializing layer bn3a_branch2b.beta\n",
      "reinitializing layer bn3a_branch2b.moving_mean\n",
      "reinitializing layer bn3a_branch2b.moving_variance\n",
      "LAYER:: activation_12\n",
      "LAYER:: res3a_branch2c\n",
      "reinitializing layer res3a_branch2c.kernel\n",
      "reinitializing layer res3a_branch2c.bias\n",
      "LAYER:: res3a_branch1\n",
      "reinitializing layer res3a_branch1.kernel\n",
      "reinitializing layer res3a_branch1.bias\n",
      "LAYER:: bn3a_branch2c\n",
      "reinitializing layer bn3a_branch2c.gamma\n",
      "reinitializing layer bn3a_branch2c.beta\n",
      "reinitializing layer bn3a_branch2c.moving_mean\n",
      "reinitializing layer bn3a_branch2c.moving_variance\n",
      "LAYER:: bn3a_branch1\n",
      "reinitializing layer bn3a_branch1.gamma\n",
      "reinitializing layer bn3a_branch1.beta\n",
      "reinitializing layer bn3a_branch1.moving_mean\n",
      "reinitializing layer bn3a_branch1.moving_variance\n",
      "LAYER:: add_4\n",
      "LAYER:: activation_13\n",
      "LAYER:: res3b_branch2a\n",
      "reinitializing layer res3b_branch2a.kernel\n",
      "reinitializing layer res3b_branch2a.bias\n",
      "LAYER:: bn3b_branch2a\n",
      "reinitializing layer bn3b_branch2a.gamma\n",
      "reinitializing layer bn3b_branch2a.beta\n",
      "reinitializing layer bn3b_branch2a.moving_mean\n",
      "reinitializing layer bn3b_branch2a.moving_variance\n",
      "LAYER:: activation_14\n",
      "LAYER:: res3b_branch2b\n",
      "reinitializing layer res3b_branch2b.kernel\n",
      "reinitializing layer res3b_branch2b.bias\n",
      "LAYER:: bn3b_branch2b\n",
      "reinitializing layer bn3b_branch2b.gamma\n",
      "reinitializing layer bn3b_branch2b.beta\n",
      "reinitializing layer bn3b_branch2b.moving_mean\n",
      "reinitializing layer bn3b_branch2b.moving_variance\n",
      "LAYER:: activation_15\n",
      "LAYER:: res3b_branch2c\n",
      "reinitializing layer res3b_branch2c.kernel\n",
      "reinitializing layer res3b_branch2c.bias\n",
      "LAYER:: bn3b_branch2c\n",
      "reinitializing layer bn3b_branch2c.gamma\n",
      "reinitializing layer bn3b_branch2c.beta\n",
      "reinitializing layer bn3b_branch2c.moving_mean\n",
      "reinitializing layer bn3b_branch2c.moving_variance\n",
      "LAYER:: add_5\n",
      "LAYER:: activation_16\n",
      "LAYER:: res3c_branch2a\n",
      "reinitializing layer res3c_branch2a.kernel\n",
      "reinitializing layer res3c_branch2a.bias\n",
      "LAYER:: bn3c_branch2a\n",
      "reinitializing layer bn3c_branch2a.gamma\n",
      "reinitializing layer bn3c_branch2a.beta\n",
      "reinitializing layer bn3c_branch2a.moving_mean\n",
      "reinitializing layer bn3c_branch2a.moving_variance\n",
      "LAYER:: activation_17\n",
      "LAYER:: res3c_branch2b\n",
      "reinitializing layer res3c_branch2b.kernel\n",
      "reinitializing layer res3c_branch2b.bias\n",
      "LAYER:: bn3c_branch2b\n",
      "reinitializing layer bn3c_branch2b.gamma\n",
      "reinitializing layer bn3c_branch2b.beta\n",
      "reinitializing layer bn3c_branch2b.moving_mean\n",
      "reinitializing layer bn3c_branch2b.moving_variance\n",
      "LAYER:: activation_18\n",
      "LAYER:: res3c_branch2c\n",
      "reinitializing layer res3c_branch2c.kernel\n",
      "reinitializing layer res3c_branch2c.bias\n",
      "LAYER:: bn3c_branch2c\n",
      "reinitializing layer bn3c_branch2c.gamma\n",
      "reinitializing layer bn3c_branch2c.beta\n",
      "reinitializing layer bn3c_branch2c.moving_mean\n",
      "reinitializing layer bn3c_branch2c.moving_variance\n",
      "LAYER:: add_6\n",
      "LAYER:: activation_19\n",
      "LAYER:: res3d_branch2a\n",
      "reinitializing layer res3d_branch2a.kernel\n",
      "reinitializing layer res3d_branch2a.bias\n",
      "LAYER:: bn3d_branch2a\n",
      "reinitializing layer bn3d_branch2a.gamma\n",
      "reinitializing layer bn3d_branch2a.beta\n",
      "reinitializing layer bn3d_branch2a.moving_mean\n",
      "reinitializing layer bn3d_branch2a.moving_variance\n",
      "LAYER:: activation_20\n",
      "LAYER:: res3d_branch2b\n",
      "reinitializing layer res3d_branch2b.kernel\n",
      "reinitializing layer res3d_branch2b.bias\n",
      "LAYER:: bn3d_branch2b\n",
      "reinitializing layer bn3d_branch2b.gamma\n",
      "reinitializing layer bn3d_branch2b.beta\n",
      "reinitializing layer bn3d_branch2b.moving_mean\n",
      "reinitializing layer bn3d_branch2b.moving_variance\n",
      "LAYER:: activation_21\n",
      "LAYER:: res3d_branch2c\n",
      "reinitializing layer res3d_branch2c.kernel\n",
      "reinitializing layer res3d_branch2c.bias\n",
      "LAYER:: bn3d_branch2c\n",
      "reinitializing layer bn3d_branch2c.gamma\n",
      "reinitializing layer bn3d_branch2c.beta\n",
      "reinitializing layer bn3d_branch2c.moving_mean\n",
      "reinitializing layer bn3d_branch2c.moving_variance\n",
      "LAYER:: add_7\n",
      "LAYER:: activation_22\n",
      "LAYER:: res4a_branch2a\n",
      "reinitializing layer res4a_branch2a.kernel\n",
      "reinitializing layer res4a_branch2a.bias\n",
      "LAYER:: bn4a_branch2a\n",
      "reinitializing layer bn4a_branch2a.gamma\n",
      "reinitializing layer bn4a_branch2a.beta\n",
      "reinitializing layer bn4a_branch2a.moving_mean\n",
      "reinitializing layer bn4a_branch2a.moving_variance\n",
      "LAYER:: activation_23\n",
      "LAYER:: res4a_branch2b\n",
      "reinitializing layer res4a_branch2b.kernel\n",
      "reinitializing layer res4a_branch2b.bias\n",
      "LAYER:: bn4a_branch2b\n",
      "reinitializing layer bn4a_branch2b.gamma\n",
      "reinitializing layer bn4a_branch2b.beta\n",
      "reinitializing layer bn4a_branch2b.moving_mean\n",
      "reinitializing layer bn4a_branch2b.moving_variance\n",
      "LAYER:: activation_24\n",
      "LAYER:: res4a_branch2c\n",
      "reinitializing layer res4a_branch2c.kernel\n",
      "reinitializing layer res4a_branch2c.bias\n",
      "LAYER:: res4a_branch1\n",
      "reinitializing layer res4a_branch1.kernel\n",
      "reinitializing layer res4a_branch1.bias\n",
      "LAYER:: bn4a_branch2c\n",
      "reinitializing layer bn4a_branch2c.gamma\n",
      "reinitializing layer bn4a_branch2c.beta\n",
      "reinitializing layer bn4a_branch2c.moving_mean\n",
      "reinitializing layer bn4a_branch2c.moving_variance\n",
      "LAYER:: bn4a_branch1\n",
      "reinitializing layer bn4a_branch1.gamma\n",
      "reinitializing layer bn4a_branch1.beta\n",
      "reinitializing layer bn4a_branch1.moving_mean\n",
      "reinitializing layer bn4a_branch1.moving_variance\n",
      "LAYER:: add_8\n",
      "LAYER:: activation_25\n",
      "LAYER:: res4b_branch2a\n",
      "reinitializing layer res4b_branch2a.kernel\n",
      "reinitializing layer res4b_branch2a.bias\n",
      "LAYER:: bn4b_branch2a\n",
      "reinitializing layer bn4b_branch2a.gamma\n",
      "reinitializing layer bn4b_branch2a.beta\n",
      "reinitializing layer bn4b_branch2a.moving_mean\n",
      "reinitializing layer bn4b_branch2a.moving_variance\n",
      "LAYER:: activation_26\n",
      "LAYER:: res4b_branch2b\n",
      "reinitializing layer res4b_branch2b.kernel\n",
      "reinitializing layer res4b_branch2b.bias\n",
      "LAYER:: bn4b_branch2b\n",
      "reinitializing layer bn4b_branch2b.gamma\n",
      "reinitializing layer bn4b_branch2b.beta\n",
      "reinitializing layer bn4b_branch2b.moving_mean\n",
      "reinitializing layer bn4b_branch2b.moving_variance\n",
      "LAYER:: activation_27\n",
      "LAYER:: res4b_branch2c\n",
      "reinitializing layer res4b_branch2c.kernel\n",
      "reinitializing layer res4b_branch2c.bias\n",
      "LAYER:: bn4b_branch2c\n",
      "reinitializing layer bn4b_branch2c.gamma\n",
      "reinitializing layer bn4b_branch2c.beta\n",
      "reinitializing layer bn4b_branch2c.moving_mean\n",
      "reinitializing layer bn4b_branch2c.moving_variance\n",
      "LAYER:: add_9\n",
      "LAYER:: activation_28\n",
      "LAYER:: res4c_branch2a\n",
      "reinitializing layer res4c_branch2a.kernel\n",
      "reinitializing layer res4c_branch2a.bias\n",
      "LAYER:: bn4c_branch2a\n",
      "reinitializing layer bn4c_branch2a.gamma\n",
      "reinitializing layer bn4c_branch2a.beta\n",
      "reinitializing layer bn4c_branch2a.moving_mean\n",
      "reinitializing layer bn4c_branch2a.moving_variance\n",
      "LAYER:: activation_29\n",
      "LAYER:: res4c_branch2b\n",
      "reinitializing layer res4c_branch2b.kernel\n",
      "reinitializing layer res4c_branch2b.bias\n",
      "LAYER:: bn4c_branch2b\n",
      "reinitializing layer bn4c_branch2b.gamma\n",
      "reinitializing layer bn4c_branch2b.beta\n",
      "reinitializing layer bn4c_branch2b.moving_mean\n",
      "reinitializing layer bn4c_branch2b.moving_variance\n",
      "LAYER:: activation_30\n",
      "LAYER:: res4c_branch2c\n",
      "reinitializing layer res4c_branch2c.kernel\n",
      "reinitializing layer res4c_branch2c.bias\n",
      "LAYER:: bn4c_branch2c\n",
      "reinitializing layer bn4c_branch2c.gamma\n",
      "reinitializing layer bn4c_branch2c.beta\n",
      "reinitializing layer bn4c_branch2c.moving_mean\n",
      "reinitializing layer bn4c_branch2c.moving_variance\n",
      "LAYER:: add_10\n",
      "LAYER:: activation_31\n",
      "LAYER:: res4d_branch2a\n",
      "reinitializing layer res4d_branch2a.kernel\n",
      "reinitializing layer res4d_branch2a.bias\n",
      "LAYER:: bn4d_branch2a\n",
      "reinitializing layer bn4d_branch2a.gamma\n",
      "reinitializing layer bn4d_branch2a.beta\n",
      "reinitializing layer bn4d_branch2a.moving_mean\n",
      "reinitializing layer bn4d_branch2a.moving_variance\n",
      "LAYER:: activation_32\n",
      "LAYER:: res4d_branch2b\n",
      "reinitializing layer res4d_branch2b.kernel\n",
      "reinitializing layer res4d_branch2b.bias\n",
      "LAYER:: bn4d_branch2b\n",
      "reinitializing layer bn4d_branch2b.gamma\n",
      "reinitializing layer bn4d_branch2b.beta\n",
      "reinitializing layer bn4d_branch2b.moving_mean\n",
      "reinitializing layer bn4d_branch2b.moving_variance\n",
      "LAYER:: activation_33\n",
      "LAYER:: res4d_branch2c\n",
      "reinitializing layer res4d_branch2c.kernel\n",
      "reinitializing layer res4d_branch2c.bias\n",
      "LAYER:: bn4d_branch2c\n",
      "reinitializing layer bn4d_branch2c.gamma\n",
      "reinitializing layer bn4d_branch2c.beta\n",
      "reinitializing layer bn4d_branch2c.moving_mean\n",
      "reinitializing layer bn4d_branch2c.moving_variance\n",
      "LAYER:: add_11\n",
      "LAYER:: activation_34\n",
      "LAYER:: res4e_branch2a\n",
      "reinitializing layer res4e_branch2a.kernel\n",
      "reinitializing layer res4e_branch2a.bias\n",
      "LAYER:: bn4e_branch2a\n",
      "reinitializing layer bn4e_branch2a.gamma\n",
      "reinitializing layer bn4e_branch2a.beta\n",
      "reinitializing layer bn4e_branch2a.moving_mean\n",
      "reinitializing layer bn4e_branch2a.moving_variance\n",
      "LAYER:: activation_35\n",
      "LAYER:: res4e_branch2b\n",
      "reinitializing layer res4e_branch2b.kernel\n",
      "reinitializing layer res4e_branch2b.bias\n",
      "LAYER:: bn4e_branch2b\n",
      "reinitializing layer bn4e_branch2b.gamma\n",
      "reinitializing layer bn4e_branch2b.beta\n",
      "reinitializing layer bn4e_branch2b.moving_mean\n",
      "reinitializing layer bn4e_branch2b.moving_variance\n",
      "LAYER:: activation_36\n",
      "LAYER:: res4e_branch2c\n",
      "reinitializing layer res4e_branch2c.kernel\n",
      "reinitializing layer res4e_branch2c.bias\n",
      "LAYER:: bn4e_branch2c\n",
      "reinitializing layer bn4e_branch2c.gamma\n",
      "reinitializing layer bn4e_branch2c.beta\n",
      "reinitializing layer bn4e_branch2c.moving_mean\n",
      "reinitializing layer bn4e_branch2c.moving_variance\n",
      "LAYER:: add_12\n",
      "LAYER:: activation_37\n",
      "LAYER:: res4f_branch2a\n",
      "reinitializing layer res4f_branch2a.kernel\n",
      "reinitializing layer res4f_branch2a.bias\n",
      "LAYER:: bn4f_branch2a\n",
      "reinitializing layer bn4f_branch2a.gamma\n",
      "reinitializing layer bn4f_branch2a.beta\n",
      "reinitializing layer bn4f_branch2a.moving_mean\n",
      "reinitializing layer bn4f_branch2a.moving_variance\n",
      "LAYER:: activation_38\n",
      "LAYER:: res4f_branch2b\n",
      "reinitializing layer res4f_branch2b.kernel\n",
      "reinitializing layer res4f_branch2b.bias\n",
      "LAYER:: bn4f_branch2b\n",
      "reinitializing layer bn4f_branch2b.gamma\n",
      "reinitializing layer bn4f_branch2b.beta\n",
      "reinitializing layer bn4f_branch2b.moving_mean\n",
      "reinitializing layer bn4f_branch2b.moving_variance\n",
      "LAYER:: activation_39\n",
      "LAYER:: res4f_branch2c\n",
      "reinitializing layer res4f_branch2c.kernel\n",
      "reinitializing layer res4f_branch2c.bias\n",
      "LAYER:: bn4f_branch2c\n",
      "reinitializing layer bn4f_branch2c.gamma\n",
      "reinitializing layer bn4f_branch2c.beta\n",
      "reinitializing layer bn4f_branch2c.moving_mean\n",
      "reinitializing layer bn4f_branch2c.moving_variance\n",
      "LAYER:: add_13\n",
      "LAYER:: activation_40\n",
      "LAYER:: res5a_branch2a\n",
      "reinitializing layer res5a_branch2a.kernel\n",
      "reinitializing layer res5a_branch2a.bias\n",
      "LAYER:: bn5a_branch2a\n",
      "reinitializing layer bn5a_branch2a.gamma\n",
      "reinitializing layer bn5a_branch2a.beta\n",
      "reinitializing layer bn5a_branch2a.moving_mean\n",
      "reinitializing layer bn5a_branch2a.moving_variance\n",
      "LAYER:: activation_41\n",
      "LAYER:: res5a_branch2b\n",
      "reinitializing layer res5a_branch2b.kernel\n",
      "reinitializing layer res5a_branch2b.bias\n",
      "LAYER:: bn5a_branch2b\n",
      "reinitializing layer bn5a_branch2b.gamma\n",
      "reinitializing layer bn5a_branch2b.beta\n",
      "reinitializing layer bn5a_branch2b.moving_mean\n",
      "reinitializing layer bn5a_branch2b.moving_variance\n",
      "LAYER:: activation_42\n",
      "LAYER:: res5a_branch2c\n",
      "reinitializing layer res5a_branch2c.kernel\n",
      "reinitializing layer res5a_branch2c.bias\n",
      "LAYER:: res5a_branch1\n",
      "reinitializing layer res5a_branch1.kernel\n",
      "reinitializing layer res5a_branch1.bias\n",
      "LAYER:: bn5a_branch2c\n",
      "reinitializing layer bn5a_branch2c.gamma\n",
      "reinitializing layer bn5a_branch2c.beta\n",
      "reinitializing layer bn5a_branch2c.moving_mean\n",
      "reinitializing layer bn5a_branch2c.moving_variance\n",
      "LAYER:: bn5a_branch1\n",
      "reinitializing layer bn5a_branch1.gamma\n",
      "reinitializing layer bn5a_branch1.beta\n",
      "reinitializing layer bn5a_branch1.moving_mean\n",
      "reinitializing layer bn5a_branch1.moving_variance\n",
      "LAYER:: add_14\n",
      "LAYER:: activation_43\n",
      "LAYER:: res5b_branch2a\n",
      "reinitializing layer res5b_branch2a.kernel\n",
      "reinitializing layer res5b_branch2a.bias\n",
      "LAYER:: bn5b_branch2a\n",
      "reinitializing layer bn5b_branch2a.gamma\n",
      "reinitializing layer bn5b_branch2a.beta\n",
      "reinitializing layer bn5b_branch2a.moving_mean\n",
      "reinitializing layer bn5b_branch2a.moving_variance\n",
      "LAYER:: activation_44\n",
      "LAYER:: res5b_branch2b\n",
      "reinitializing layer res5b_branch2b.kernel\n",
      "reinitializing layer res5b_branch2b.bias\n",
      "LAYER:: bn5b_branch2b\n",
      "reinitializing layer bn5b_branch2b.gamma\n",
      "reinitializing layer bn5b_branch2b.beta\n",
      "reinitializing layer bn5b_branch2b.moving_mean\n",
      "reinitializing layer bn5b_branch2b.moving_variance\n",
      "LAYER:: activation_45\n",
      "LAYER:: res5b_branch2c\n",
      "reinitializing layer res5b_branch2c.kernel\n",
      "reinitializing layer res5b_branch2c.bias\n",
      "LAYER:: bn5b_branch2c\n",
      "reinitializing layer bn5b_branch2c.gamma\n",
      "reinitializing layer bn5b_branch2c.beta\n",
      "reinitializing layer bn5b_branch2c.moving_mean\n",
      "reinitializing layer bn5b_branch2c.moving_variance\n",
      "LAYER:: add_15\n",
      "LAYER:: activation_46\n",
      "LAYER:: res5c_branch2a\n",
      "reinitializing layer res5c_branch2a.kernel\n",
      "reinitializing layer res5c_branch2a.bias\n",
      "LAYER:: bn5c_branch2a\n",
      "reinitializing layer bn5c_branch2a.gamma\n",
      "reinitializing layer bn5c_branch2a.beta\n",
      "reinitializing layer bn5c_branch2a.moving_mean\n",
      "reinitializing layer bn5c_branch2a.moving_variance\n",
      "LAYER:: activation_47\n",
      "LAYER:: res5c_branch2b\n",
      "reinitializing layer res5c_branch2b.kernel\n",
      "reinitializing layer res5c_branch2b.bias\n",
      "LAYER:: bn5c_branch2b\n",
      "reinitializing layer bn5c_branch2b.gamma\n",
      "reinitializing layer bn5c_branch2b.beta\n",
      "reinitializing layer bn5c_branch2b.moving_mean\n",
      "reinitializing layer bn5c_branch2b.moving_variance\n",
      "LAYER:: activation_48\n",
      "LAYER:: res5c_branch2c\n",
      "reinitializing layer res5c_branch2c.kernel\n",
      "reinitializing layer res5c_branch2c.bias\n",
      "LAYER:: bn5c_branch2c\n",
      "reinitializing layer bn5c_branch2c.gamma\n",
      "reinitializing layer bn5c_branch2c.beta\n",
      "reinitializing layer bn5c_branch2c.moving_mean\n",
      "reinitializing layer bn5c_branch2c.moving_variance\n",
      "LAYER:: add_16\n",
      "LAYER:: activation_49\n",
      "LAYER:: avg_pool\n",
      "LAYER:: dense_1\n",
      "reinitializing layer dense_1.kernel\n",
      "reinitializing layer dense_1.bias\n",
      "LAYER:: activation_50\n",
      "Train on 1053 samples, validate on 264 samples\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1053 [==============================] - 104s 99ms/step - loss: 0.6636 - mse: 0.6636 - val_loss: 0.7552 - val_mse: 0.7552\n",
      "Epoch 2/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0360 - mse: 0.0360 - val_loss: 2.6218 - val_mse: 2.6218\n",
      "Epoch 3/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0372 - mse: 0.0372 - val_loss: 0.0844 - val_mse: 0.0844\n",
      "Epoch 4/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 0.7891 - val_mse: 0.7891\n",
      "Epoch 5/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.0375 - val_mse: 0.0375\n",
      "Epoch 6/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0211 - mse: 0.0211 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 7/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0190 - mse: 0.0190 - val_loss: 0.7257 - val_mse: 0.7257\n",
      "Epoch 8/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.2396 - val_mse: 0.2396\n",
      "Epoch 9/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0214 - val_mse: 0.0214\n",
      "Epoch 10/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.0122 - val_mse: 0.0122\n",
      "Epoch 11/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0314 - mse: 0.0314 - val_loss: 0.0304 - val_mse: 0.0304\n",
      "Epoch 12/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0190 - val_mse: 0.0190\n",
      "Epoch 13/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0970 - val_mse: 0.0970\n",
      "Epoch 14/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0201 - mse: 0.0201 - val_loss: 0.5800 - val_mse: 0.5800\n",
      "Epoch 15/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0228 - val_mse: 0.0228\n",
      "Epoch 16/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0080 - mse: 0.0080 - val_loss: 0.2608 - val_mse: 0.2608\n",
      "Epoch 17/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0337 - mse: 0.0337 - val_loss: 0.0193 - val_mse: 0.0193\n",
      "Epoch 18/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0220 - val_mse: 0.0220\n",
      "Epoch 19/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 20/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0068 - mse: 0.0068 - val_loss: 0.0376 - val_mse: 0.0376\n",
      "Epoch 21/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0068 - mse: 0.0068 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 22/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0175 - val_mse: 0.0175\n",
      "Epoch 23/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0196 - val_mse: 0.0196\n",
      "Epoch 24/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.0064 - val_mse: 0.0064\n",
      "Epoch 25/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0424 - val_mse: 0.0424\n",
      "Epoch 26/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 27/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.7784 - val_mse: 0.7784\n",
      "Epoch 28/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0332 - mse: 0.0332 - val_loss: 994023.4446 - val_mse: 994023.6250\n",
      "Epoch 29/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0374 - mse: 0.0374 - val_loss: 0.0498 - val_mse: 0.0498\n",
      "Epoch 30/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0210 - mse: 0.0210 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 31/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0309 - val_mse: 0.0309\n",
      "Epoch 32/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 33/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0207 - val_mse: 0.0207\n",
      "Epoch 34/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 35/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0121 - val_mse: 0.0121\n",
      "Epoch 36/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 37/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0145 - val_mse: 0.0145\n",
      "Epoch 38/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 39/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0194 - val_mse: 0.0194\n",
      "Epoch 40/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0181 - val_mse: 0.0181\n",
      "Epoch 41/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 42/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0134 - val_mse: 0.0134\n",
      "Epoch 43/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0213 - val_mse: 0.0213\n",
      "Epoch 44/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 45/200\n",
      "1053/1053 [==============================] - 102s 96ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 46/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0126 - val_mse: 0.0126\n",
      "Epoch 47/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0196 - val_mse: 0.0196\n",
      "Epoch 48/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0258 - val_mse: 0.0258\n",
      "Epoch 49/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0053 - val_mse: 0.0053\n",
      "Epoch 50/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 51/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 52/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 53/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 54/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0184 - val_mse: 0.0184\n",
      "Epoch 55/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0212 - val_mse: 0.0212\n",
      "Epoch 56/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0058 - val_mse: 0.0058\n",
      "Epoch 57/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 58/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 59/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 61/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0203 - val_mse: 0.0203\n",
      "Epoch 62/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 63/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0149 - val_mse: 0.0149\n",
      "Epoch 64/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 65/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 66/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 67/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 68/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 69/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 70/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 71/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 72/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 73/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 74/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 75/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 76/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 77/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 78/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 79/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 80/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0058 - val_mse: 0.0058\n",
      "Epoch 81/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 82/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 83/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0049 - val_mse: 0.0049\n",
      "Epoch 84/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 85/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 86/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 87/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 88/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0048 - val_mse: 0.0048\n",
      "Epoch 89/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 90/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 91/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 92/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 93/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 94/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 95/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 96/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 97/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 98/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 99/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 100/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 101/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 102/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 103/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 104/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0053 - val_mse: 0.0053\n",
      "Epoch 105/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 106/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 107/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 108/200\n",
      "1053/1053 [==============================] - 103s 97ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 109/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 110/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 111/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 112/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 113/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 114/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 115/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 116/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 117/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 118/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0019 - val_mse: 0.0019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 120/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 121/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 122/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 123/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 124/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 125/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 126/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 127/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 128/200\n",
      "1053/1053 [==============================] - 103s 97ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 129/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 130/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 131/200\n",
      "1053/1053 [==============================] - 103s 97ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 132/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 133/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 134/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0010 - val_mse: 0.0010\n",
      "Epoch 135/200\n",
      "1053/1053 [==============================] - 103s 97ms/step - loss: 9.3782e-04 - mse: 9.3782e-04 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 136/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 137/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 138/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 139/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 140/200\n",
      "1053/1053 [==============================] - 103s 97ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 141/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 142/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 143/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 144/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 9.3471e-04 - mse: 9.3471e-04 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 145/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 146/200\n",
      "1053/1053 [==============================] - 98s 93ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 9.7748e-04 - val_mse: 9.7748e-04\n",
      "Epoch 147/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 9.9051e-04 - mse: 9.9051e-04 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 148/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 149/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 150/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 151/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 152/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 153/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 154/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 155/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 8.9676e-04 - mse: 8.9676e-04 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 156/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 157/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 8.4820e-04 - mse: 8.4820e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 158/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 159/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 9.4552e-04 - mse: 9.4552e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 160/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 9.0798e-04 - mse: 9.0798e-04 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 161/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 9.8679e-04 - mse: 9.8679e-04 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 162/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 163/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 164/200\n",
      "1053/1053 [==============================] - 103s 97ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 165/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 9.4621e-04 - mse: 9.4621e-04 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 166/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 9.2092e-04 - mse: 9.2092e-04 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 167/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 9.8401e-04 - mse: 9.8401e-04 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 168/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 7.3674e-04 - mse: 7.3674e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 169/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 8.4671e-04 - mse: 8.4671e-04 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 170/200\n",
      "1053/1053 [==============================] - 103s 97ms/step - loss: 8.0632e-04 - mse: 8.0632e-04 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 171/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 8.2042e-04 - mse: 8.2042e-04 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 172/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 9.7382e-04 - mse: 9.7382e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 173/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 9.4096e-04 - mse: 9.4096e-04 - val_loss: 0.0010 - val_mse: 0.0010\n",
      "Epoch 174/200\n",
      "1053/1053 [==============================] - 103s 97ms/step - loss: 9.1970e-04 - mse: 9.1970e-04 - val_loss: 9.7238e-04 - val_mse: 9.7238e-04\n",
      "Epoch 175/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 7.6595e-04 - mse: 7.6595e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 176/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 9.8466e-04 - mse: 9.8466e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 177/200\n",
      "1053/1053 [==============================] - 103s 97ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 178/200\n",
      "1053/1053 [==============================] - 103s 97ms/step - loss: 8.7366e-04 - mse: 8.7366e-04 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 179/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 8.7994e-04 - mse: 8.7994e-04 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 180/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 8.1248e-04 - mse: 8.1248e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 181/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 8.9512e-04 - mse: 8.9512e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 182/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 8.0916e-04 - mse: 8.0916e-04 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 183/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 8.0095e-04 - mse: 8.0095e-04 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 184/200\n",
      "1053/1053 [==============================] - 103s 97ms/step - loss: 8.2046e-04 - mse: 8.2046e-04 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 185/200\n",
      "1053/1053 [==============================] - 103s 97ms/step - loss: 8.0326e-04 - mse: 8.0326e-04 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 186/200\n",
      "1053/1053 [==============================] - 103s 97ms/step - loss: 7.6591e-04 - mse: 7.6591e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 187/200\n",
      "1053/1053 [==============================] - 103s 97ms/step - loss: 7.6103e-04 - mse: 7.6103e-04 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 188/200\n",
      "1053/1053 [==============================] - 103s 97ms/step - loss: 8.8351e-04 - mse: 8.8351e-04 - val_loss: 8.0885e-04 - val_mse: 8.0885e-04\n",
      "Epoch 189/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 8.2655e-04 - mse: 8.2655e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 190/200\n",
      "1053/1053 [==============================] - 103s 97ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0010 - val_mse: 0.0010\n",
      "Epoch 191/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 9.3731e-04 - mse: 9.3731e-04 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 192/200\n",
      "1053/1053 [==============================] - 103s 97ms/step - loss: 7.5458e-04 - mse: 7.5458e-04 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 193/200\n",
      "1053/1053 [==============================] - 103s 97ms/step - loss: 8.7526e-04 - mse: 8.7526e-04 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 194/200\n",
      "1053/1053 [==============================] - 103s 97ms/step - loss: 9.4111e-04 - mse: 9.4111e-04 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 195/200\n",
      "1053/1053 [==============================] - 103s 97ms/step - loss: 7.5524e-04 - mse: 7.5524e-04 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 196/200\n",
      "1053/1053 [==============================] - 102s 97ms/step - loss: 8.4532e-04 - mse: 8.4532e-04 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 197/200\n",
      "1053/1053 [==============================] - 103s 97ms/step - loss: 7.1228e-04 - mse: 7.1228e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 198/200\n",
      "1053/1053 [==============================] - 103s 98ms/step - loss: 7.2256e-04 - mse: 7.2256e-04 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 199/200\n",
      "1053/1053 [==============================] - 103s 97ms/step - loss: 7.8430e-04 - mse: 7.8430e-04 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 200/200\n",
      "1053/1053 [==============================] - 103s 97ms/step - loss: 6.5086e-04 - mse: 6.5086e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "330/330 [==============================] - 33s 99ms/step\n",
      "(330,)\n",
      "(330,)\n",
      "[ 22.489239  67.91104  256.78787  236.62128  451.9091   118.91492\n",
      " 234.30113   64.40487  146.2431   169.91113  228.21834   50.625492\n",
      " 143.29112   55.51424  102.627945  45.242092 157.24425  136.31494\n",
      " 253.12257  192.07828  474.15265  238.59024  274.5148    87.737785\n",
      "  75.28092   28.3516   116.318436  42.730003 131.1719   131.65048\n",
      " 147.11913  121.50902  104.73445   52.745544 117.21199  162.05942\n",
      " 302.28543   34.233326  53.458637 392.77722  252.83679  276.54016\n",
      " 162.70186  228.88667  287.6304   290.7949    39.22081   29.132645\n",
      " 109.44618  132.59642   13.133023  86.21058  140.34328   17.757896\n",
      " 190.76486  189.65031  111.956436 236.81084  213.75294   18.882685\n",
      " 219.18796  290.6403    44.52001  406.9001   227.64493  248.05977\n",
      " 233.7871   205.35431  254.84726   92.755394 232.66364   74.79708\n",
      " 170.55383  150.45483   63.56788  195.36171  155.0922   127.88266\n",
      " 110.62971   12.766037 135.83934  350.0877    53.835594 103.35247\n",
      "  19.334093  81.641266  84.05627   91.070015  57.642433 110.32284\n",
      " 218.2708    16.692352 177.92531  162.05284  232.71817  215.32643\n",
      " 103.1913   195.36171   53.851837  79.90677  175.6159    89.28775\n",
      "  88.82457   19.111637  19.645092  46.910084 163.69823  191.14983\n",
      "  53.749325  16.796972  20.492874 158.96767  139.60266   58.93808\n",
      " 116.51504  365.4134    26.834051 257.89453   43.7791   260.8343\n",
      " 253.11098  159.78627  143.20128  224.54167  223.89418  144.80458\n",
      " 268.0406    72.14993   28.715342 123.83694  229.85478  253.86629\n",
      " 442.04013   62.25528   80.19776  198.22377   69.51368  118.134125\n",
      "  18.392242 468.64102   28.460608  73.87998   30.298414 458.19592\n",
      "  28.529022 112.19314  101.04916  105.31715  138.32002  164.92986\n",
      " 261.53662   80.75345   98.0358    31.22268  463.38458   16.30627\n",
      " 137.88556   58.157063  21.593441 136.52594   57.32917   79.087326\n",
      "  97.41336  109.02518   63.284657  43.463432 249.56473  109.50978\n",
      " 219.65594   44.793007  30.041224 131.6635    24.972486 136.9773\n",
      "  89.72563  181.39342  275.98395   94.79857  256.2037    90.45802\n",
      " 420.08832  147.13814  114.88384  128.56369   81.69081  131.57867\n",
      " 162.45982   91.85675  133.04846   40.29632  170.96408   97.68273\n",
      "  25.352936 147.35066  395.70554   11.168867 120.34558   56.35202\n",
      " 103.01508   41.53318   94.08495  177.94896  183.97437  112.213326\n",
      "  57.46176   47.780205 135.89342  172.12904   70.323975  42.258156\n",
      " 201.59908  141.12508  164.07631  133.7506   126.27208   75.02467\n",
      " 224.05338  252.74226   62.65928  258.81537  130.58873  149.8256\n",
      "  10.640361 143.88116  258.1806    23.41406   24.984789 132.53674\n",
      " 129.87944   72.68499  239.5953    29.270918 198.24232  197.06833\n",
      "  70.922104 123.467064 175.25961   62.198597 105.31798   39.774197\n",
      " 195.80049   74.758286 121.14589   67.819435 275.89374   98.82198\n",
      "  72.92244  147.7876    13.524167 155.81876  147.36366  196.0222\n",
      " 129.58826   61.156086 318.07407  262.7467    99.51587  102.04364\n",
      "  10.534346 100.3402    77.75676  138.35475  144.19522  471.5925\n",
      " 206.1795    34.439163  79.00995  267.76657  113.78776  125.011696\n",
      " 112.156555 111.606804 232.03627  257.6392     8.563377 163.0445\n",
      " 180.33391  224.54167  166.22897  141.34737  111.24542  210.31316\n",
      " 260.73285  228.61244   76.36627   29.587229 136.21883   17.3323\n",
      " 278.87338  243.41324  155.30006  143.64372   76.070694  24.598473\n",
      " 297.10788  224.37476  112.440125 105.84526  144.13823  238.10696\n",
      " 153.97812  473.2845    82.47693  110.08974  120.91964   48.716484\n",
      " 161.23802  102.37844  425.57248   76.98308   56.18967   30.544024\n",
      " 145.59325   34.68576  224.28488  425.57663  336.70074  148.38834\n",
      "  54.7759   271.12375  243.51913  126.54516  268.86093  266.1485\n",
      "  12.997795 133.67618   54.646553 168.3838   156.50099   82.210045]\n",
      "[  6.  54. 275. 240. 450. 108. 225.  58. 150. 162. 250.  80. 162.  64.\n",
      " 100.  46. 174. 128. 250. 184. 500. 300. 285.  68.  88.  30. 120.   8.\n",
      " 128. 154. 144. 126.  80.  52. 112. 158. 300.  18.  66. 450. 265. 290.\n",
      " 168. 255. 295. 300.  32.  24.  96. 132.   6.  92. 186.   8. 180. 196.\n",
      " 108. 230. 225.   8. 215. 290.  54. 450. 250. 270. 230. 200. 225.  80.\n",
      " 245.   4. 190. 168.  44. 225. 182. 122. 120.   4. 124. 350.  56.   2.\n",
      "  12.  84.  84.  78.  84.  82. 225.  18. 176. 174. 198. 225. 100. 225.\n",
      "  46.  60. 188.  88.  70.  10.  24.  48. 172. 186.  52.  10.   2. 188.\n",
      " 156.  48. 116. 400.  30. 275.  40. 275. 215. 156. 190. 225. 245. 172.\n",
      " 235. 102.   6. 122. 265. 295. 450.  46.  86. 196.  84. 112.  10. 500.\n",
      "  20.  58.  34. 500.  14.  82. 104.  92. 154. 188. 285. 100. 120.  34.\n",
      " 500.  16. 132.  46.  12. 136.  22.   1. 100.  90.  58.  38. 270. 112.\n",
      " 215.  32.  26. 124.  16. 144.  90. 182. 300. 134. 215. 102. 450. 128.\n",
      "  94. 138.  72. 146. 158.   0. 112.  40. 178.  98.  32. 136. 400.   4.\n",
      " 112.  28. 106.  28.  74. 200. 192.  96.  58.  32. 140. 172.  72.  40.\n",
      " 200. 156. 162. 142. 134.  76. 255. 240.  58. 280. 124. 128.  28. 136.\n",
      " 275.  26.  16. 128. 134.  64. 225.  32. 186. 220.  52. 154. 184.  54.\n",
      " 114.  18. 220.  94. 130.  60. 295.  74.  60. 164.  10. 198. 138. 200.\n",
      " 128.  74. 350. 285. 102.   2.   4.  92.  86. 154. 190. 500. 186.  42.\n",
      "  62. 275. 110. 124. 116.  98. 275. 265.  28. 182. 198. 225. 170. 154.\n",
      "  86. 235. 300. 240.  84.  36. 150.  14. 300. 250. 164. 160.  78.  22.\n",
      " 300. 230. 112. 118. 118. 250. 178. 500.   0.  90. 120.  80. 186.  90.\n",
      " 450.  74.  64.  24. 140.  36. 215. 450. 350. 136.  68. 290. 220. 116.\n",
      " 300. 290.   8. 120.  48. 188. 164.  86.]\n",
      "mse =  441.2405456629224\n"
     ]
    }
   ],
   "source": [
    "time_str = str(time.time())\n",
    "print(time_str)\n",
    "reinitLayers(model)\n",
    "early_stop = EarlyStopping(monitor='val_mean_squared_error', patience=30)\n",
    "# model.load_weights('./modelWights/regression_model'+model_fn.__name__+'.h5')\n",
    "model_checkpoint = ModelCheckpoint('./modelWights/weights_'+'resnet50_without_pre'+time_str+'_color.h5', monitor='val_loss', save_best_only=True)\n",
    "tensorboard = keras.callbacks.tensorboard_v1.TensorBoard(log_dir=\"~/data/projects_logs/water_logs/resnet50_without_pre_color_{}\".format(time_str), histogram_freq=2,write_grads=True)\n",
    "history = model.fit(X_train, y_train, batch_size=4, epochs=200, validation_data=(X_val, y_val), callbacks=[model_checkpoint])\n",
    "# print(history.history)\n",
    "\n",
    "y_pred_ = model.predict(X_test, batch_size=1, verbose=1)\n",
    "y_pred_, y_test_ = post_training_data(y_test, y_pred_)\n",
    "predicted_file = 'regress_'+\"resnet50_without_pre\"+\"resnet50\"+time_str+'_color.csv'\n",
    "save_result(\"./result/\"+predicted_file,y_pred_,y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 512, 512, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 518, 518, 3)  0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 256, 256, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 256, 256, 64) 256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 256, 256, 64) 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 258, 258, 64) 0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 128, 128, 64) 0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 128, 128, 64) 4160        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 128, 128, 64) 256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 128, 128, 64) 0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 128, 128, 64) 36928       activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 128, 128, 64) 256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 128, 128, 64) 0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 128, 128, 256 16640       activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 128, 128, 256 16640       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 128, 128, 256 1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 128, 128, 256 1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 128, 128, 256 0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 128, 128, 256 0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 128, 128, 64) 16448       activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 128, 128, 64) 256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 128, 128, 64) 0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 128, 128, 64) 36928       activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 128, 128, 64) 256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 128, 128, 64) 0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 128, 128, 256 16640       activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 128, 128, 256 1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 128, 128, 256 0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 128, 128, 256 0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 128, 128, 64) 16448       activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 128, 128, 64) 256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 128, 128, 64) 0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 128, 128, 64) 36928       activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 128, 128, 64) 256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 128, 128, 64) 0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 128, 128, 256 16640       activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 128, 128, 256 1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 128, 128, 256 0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 128, 128, 256 0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 64, 64, 128)  32896       activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 64, 64, 128)  0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 64, 64, 128)  0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 64, 64, 512)  131584      activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 64, 64, 512)  2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 64, 64, 512)  0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 64, 64, 512)  0           add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 64, 64, 128)  65664       activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 64, 64, 128)  0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 64, 64, 128)  0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 64, 64, 512)  0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 64, 64, 512)  0           add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 64, 64, 128)  65664       activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 64, 64, 128)  0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 64, 64, 128)  0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 64, 64, 512)  0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 64, 64, 512)  0           add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 64, 64, 128)  65664       activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 64, 64, 128)  0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 64, 64, 128)  0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 64, 64, 512)  0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 64, 64, 512)  0           add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 32, 32, 256)  131328      activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 32, 32, 256)  0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 32, 32, 256)  0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 32, 32, 1024) 525312      activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 32, 32, 1024) 4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 32, 32, 1024) 0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 32, 32, 1024) 0           add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 32, 32, 256)  0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 32, 32, 256)  0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 32, 32, 1024) 0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 32, 32, 1024) 0           add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 32, 32, 256)  0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 32, 32, 256)  0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 32, 32, 1024) 0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 32, 32, 1024) 0           add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 32, 32, 256)  0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 32, 32, 256)  0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 32, 32, 1024) 0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 32, 32, 1024) 0           add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 32, 32, 256)  0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 32, 32, 256)  0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 32, 32, 1024) 0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 32, 32, 1024) 0           add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 32, 32, 256)  0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 32, 32, 256)  0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 32, 32, 1024) 0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 32, 32, 1024) 0           add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 16, 16, 512)  524800      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 16, 16, 512)  2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 16, 16, 512)  0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 16, 16, 512)  2359808     activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 16, 16, 512)  2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 16, 16, 512)  0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 16, 16, 2048) 1050624     activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 16, 16, 2048) 2099200     activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, 16, 16, 2048) 8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 16, 16, 2048) 8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 16, 16, 2048) 0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 16, 16, 2048) 0           add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 16, 16, 512)  1049088     activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 16, 16, 512)  2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 16, 16, 512)  0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 16, 16, 512)  2359808     activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 16, 16, 512)  2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 16, 16, 512)  0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 16, 16, 2048) 1050624     activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, 16, 16, 2048) 8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 16, 16, 2048) 0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 16, 16, 2048) 0           add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 16, 16, 512)  1049088     activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 16, 16, 512)  2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 16, 16, 512)  0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 16, 16, 512)  2359808     activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 16, 16, 512)  2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 16, 16, 512)  0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, 16, 16, 2048) 1050624     activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, 16, 16, 2048) 8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 16, 16, 2048) 0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 16, 16, 2048) 0           add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 1)            2049        avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 23,589,761\n",
      "Trainable params: 23,536,641\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n",
      "1570948324.0025954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER:: input_2\n",
      "LAYER:: conv1_pad\n",
      "LAYER:: conv1\n",
      "reinitializing layer conv1.kernel\n",
      "reinitializing layer conv1.bias\n",
      "LAYER:: bn_conv1\n",
      "reinitializing layer bn_conv1.gamma\n",
      "reinitializing layer bn_conv1.beta\n",
      "reinitializing layer bn_conv1.moving_mean\n",
      "reinitializing layer bn_conv1.moving_variance\n",
      "LAYER:: activation_51\n",
      "LAYER:: pool1_pad\n",
      "LAYER:: max_pooling2d_2\n",
      "LAYER:: res2a_branch2a\n",
      "reinitializing layer res2a_branch2a.kernel\n",
      "reinitializing layer res2a_branch2a.bias\n",
      "LAYER:: bn2a_branch2a\n",
      "reinitializing layer bn2a_branch2a.gamma\n",
      "reinitializing layer bn2a_branch2a.beta\n",
      "reinitializing layer bn2a_branch2a.moving_mean\n",
      "reinitializing layer bn2a_branch2a.moving_variance\n",
      "LAYER:: activation_52\n",
      "LAYER:: res2a_branch2b\n",
      "reinitializing layer res2a_branch2b.kernel\n",
      "reinitializing layer res2a_branch2b.bias\n",
      "LAYER:: bn2a_branch2b\n",
      "reinitializing layer bn2a_branch2b.gamma\n",
      "reinitializing layer bn2a_branch2b.beta\n",
      "reinitializing layer bn2a_branch2b.moving_mean\n",
      "reinitializing layer bn2a_branch2b.moving_variance\n",
      "LAYER:: activation_53\n",
      "LAYER:: res2a_branch2c\n",
      "reinitializing layer res2a_branch2c.kernel\n",
      "reinitializing layer res2a_branch2c.bias\n",
      "LAYER:: res2a_branch1\n",
      "reinitializing layer res2a_branch1.kernel\n",
      "reinitializing layer res2a_branch1.bias\n",
      "LAYER:: bn2a_branch2c\n",
      "reinitializing layer bn2a_branch2c.gamma\n",
      "reinitializing layer bn2a_branch2c.beta\n",
      "reinitializing layer bn2a_branch2c.moving_mean\n",
      "reinitializing layer bn2a_branch2c.moving_variance\n",
      "LAYER:: bn2a_branch1\n",
      "reinitializing layer bn2a_branch1.gamma\n",
      "reinitializing layer bn2a_branch1.beta\n",
      "reinitializing layer bn2a_branch1.moving_mean\n",
      "reinitializing layer bn2a_branch1.moving_variance\n",
      "LAYER:: add_17\n",
      "LAYER:: activation_54\n",
      "LAYER:: res2b_branch2a\n",
      "reinitializing layer res2b_branch2a.kernel\n",
      "reinitializing layer res2b_branch2a.bias\n",
      "LAYER:: bn2b_branch2a\n",
      "reinitializing layer bn2b_branch2a.gamma\n",
      "reinitializing layer bn2b_branch2a.beta\n",
      "reinitializing layer bn2b_branch2a.moving_mean\n",
      "reinitializing layer bn2b_branch2a.moving_variance\n",
      "LAYER:: activation_55\n",
      "LAYER:: res2b_branch2b\n",
      "reinitializing layer res2b_branch2b.kernel\n",
      "reinitializing layer res2b_branch2b.bias\n",
      "LAYER:: bn2b_branch2b\n",
      "reinitializing layer bn2b_branch2b.gamma\n",
      "reinitializing layer bn2b_branch2b.beta\n",
      "reinitializing layer bn2b_branch2b.moving_mean\n",
      "reinitializing layer bn2b_branch2b.moving_variance\n",
      "LAYER:: activation_56\n",
      "LAYER:: res2b_branch2c\n",
      "reinitializing layer res2b_branch2c.kernel\n",
      "reinitializing layer res2b_branch2c.bias\n",
      "LAYER:: bn2b_branch2c\n",
      "reinitializing layer bn2b_branch2c.gamma\n",
      "reinitializing layer bn2b_branch2c.beta\n",
      "reinitializing layer bn2b_branch2c.moving_mean\n",
      "reinitializing layer bn2b_branch2c.moving_variance\n",
      "LAYER:: add_18\n",
      "LAYER:: activation_57\n",
      "LAYER:: res2c_branch2a\n",
      "reinitializing layer res2c_branch2a.kernel\n",
      "reinitializing layer res2c_branch2a.bias\n",
      "LAYER:: bn2c_branch2a\n",
      "reinitializing layer bn2c_branch2a.gamma\n",
      "reinitializing layer bn2c_branch2a.beta\n",
      "reinitializing layer bn2c_branch2a.moving_mean\n",
      "reinitializing layer bn2c_branch2a.moving_variance\n",
      "LAYER:: activation_58\n",
      "LAYER:: res2c_branch2b\n",
      "reinitializing layer res2c_branch2b.kernel\n",
      "reinitializing layer res2c_branch2b.bias\n",
      "LAYER:: bn2c_branch2b\n",
      "reinitializing layer bn2c_branch2b.gamma\n",
      "reinitializing layer bn2c_branch2b.beta\n",
      "reinitializing layer bn2c_branch2b.moving_mean\n",
      "reinitializing layer bn2c_branch2b.moving_variance\n",
      "LAYER:: activation_59\n",
      "LAYER:: res2c_branch2c\n",
      "reinitializing layer res2c_branch2c.kernel\n",
      "reinitializing layer res2c_branch2c.bias\n",
      "LAYER:: bn2c_branch2c\n",
      "reinitializing layer bn2c_branch2c.gamma\n",
      "reinitializing layer bn2c_branch2c.beta\n",
      "reinitializing layer bn2c_branch2c.moving_mean\n",
      "reinitializing layer bn2c_branch2c.moving_variance\n",
      "LAYER:: add_19\n",
      "LAYER:: activation_60\n",
      "LAYER:: res3a_branch2a\n",
      "reinitializing layer res3a_branch2a.kernel\n",
      "reinitializing layer res3a_branch2a.bias\n",
      "LAYER:: bn3a_branch2a\n",
      "reinitializing layer bn3a_branch2a.gamma\n",
      "reinitializing layer bn3a_branch2a.beta\n",
      "reinitializing layer bn3a_branch2a.moving_mean\n",
      "reinitializing layer bn3a_branch2a.moving_variance\n",
      "LAYER:: activation_61\n",
      "LAYER:: res3a_branch2b\n",
      "reinitializing layer res3a_branch2b.kernel\n",
      "reinitializing layer res3a_branch2b.bias\n",
      "LAYER:: bn3a_branch2b\n",
      "reinitializing layer bn3a_branch2b.gamma\n",
      "reinitializing layer bn3a_branch2b.beta\n",
      "reinitializing layer bn3a_branch2b.moving_mean\n",
      "reinitializing layer bn3a_branch2b.moving_variance\n",
      "LAYER:: activation_62\n",
      "LAYER:: res3a_branch2c\n",
      "reinitializing layer res3a_branch2c.kernel\n",
      "reinitializing layer res3a_branch2c.bias\n",
      "LAYER:: res3a_branch1\n",
      "reinitializing layer res3a_branch1.kernel\n",
      "reinitializing layer res3a_branch1.bias\n",
      "LAYER:: bn3a_branch2c\n",
      "reinitializing layer bn3a_branch2c.gamma\n",
      "reinitializing layer bn3a_branch2c.beta\n",
      "reinitializing layer bn3a_branch2c.moving_mean\n",
      "reinitializing layer bn3a_branch2c.moving_variance\n",
      "LAYER:: bn3a_branch1\n",
      "reinitializing layer bn3a_branch1.gamma\n",
      "reinitializing layer bn3a_branch1.beta\n",
      "reinitializing layer bn3a_branch1.moving_mean\n",
      "reinitializing layer bn3a_branch1.moving_variance\n",
      "LAYER:: add_20\n",
      "LAYER:: activation_63\n",
      "LAYER:: res3b_branch2a\n",
      "reinitializing layer res3b_branch2a.kernel\n",
      "reinitializing layer res3b_branch2a.bias\n",
      "LAYER:: bn3b_branch2a\n",
      "reinitializing layer bn3b_branch2a.gamma\n",
      "reinitializing layer bn3b_branch2a.beta\n",
      "reinitializing layer bn3b_branch2a.moving_mean\n",
      "reinitializing layer bn3b_branch2a.moving_variance\n",
      "LAYER:: activation_64\n",
      "LAYER:: res3b_branch2b\n",
      "reinitializing layer res3b_branch2b.kernel\n",
      "reinitializing layer res3b_branch2b.bias\n",
      "LAYER:: bn3b_branch2b\n",
      "reinitializing layer bn3b_branch2b.gamma\n",
      "reinitializing layer bn3b_branch2b.beta\n",
      "reinitializing layer bn3b_branch2b.moving_mean\n",
      "reinitializing layer bn3b_branch2b.moving_variance\n",
      "LAYER:: activation_65\n",
      "LAYER:: res3b_branch2c\n",
      "reinitializing layer res3b_branch2c.kernel\n",
      "reinitializing layer res3b_branch2c.bias\n",
      "LAYER:: bn3b_branch2c\n",
      "reinitializing layer bn3b_branch2c.gamma\n",
      "reinitializing layer bn3b_branch2c.beta\n",
      "reinitializing layer bn3b_branch2c.moving_mean\n",
      "reinitializing layer bn3b_branch2c.moving_variance\n",
      "LAYER:: add_21\n",
      "LAYER:: activation_66\n",
      "LAYER:: res3c_branch2a\n",
      "reinitializing layer res3c_branch2a.kernel\n",
      "reinitializing layer res3c_branch2a.bias\n",
      "LAYER:: bn3c_branch2a\n",
      "reinitializing layer bn3c_branch2a.gamma\n",
      "reinitializing layer bn3c_branch2a.beta\n",
      "reinitializing layer bn3c_branch2a.moving_mean\n",
      "reinitializing layer bn3c_branch2a.moving_variance\n",
      "LAYER:: activation_67\n",
      "LAYER:: res3c_branch2b\n",
      "reinitializing layer res3c_branch2b.kernel\n",
      "reinitializing layer res3c_branch2b.bias\n",
      "LAYER:: bn3c_branch2b\n",
      "reinitializing layer bn3c_branch2b.gamma\n",
      "reinitializing layer bn3c_branch2b.beta\n",
      "reinitializing layer bn3c_branch2b.moving_mean\n",
      "reinitializing layer bn3c_branch2b.moving_variance\n",
      "LAYER:: activation_68\n",
      "LAYER:: res3c_branch2c\n",
      "reinitializing layer res3c_branch2c.kernel\n",
      "reinitializing layer res3c_branch2c.bias\n",
      "LAYER:: bn3c_branch2c\n",
      "reinitializing layer bn3c_branch2c.gamma\n",
      "reinitializing layer bn3c_branch2c.beta\n",
      "reinitializing layer bn3c_branch2c.moving_mean\n",
      "reinitializing layer bn3c_branch2c.moving_variance\n",
      "LAYER:: add_22\n",
      "LAYER:: activation_69\n",
      "LAYER:: res3d_branch2a\n",
      "reinitializing layer res3d_branch2a.kernel\n",
      "reinitializing layer res3d_branch2a.bias\n",
      "LAYER:: bn3d_branch2a\n",
      "reinitializing layer bn3d_branch2a.gamma\n",
      "reinitializing layer bn3d_branch2a.beta\n",
      "reinitializing layer bn3d_branch2a.moving_mean\n",
      "reinitializing layer bn3d_branch2a.moving_variance\n",
      "LAYER:: activation_70\n",
      "LAYER:: res3d_branch2b\n",
      "reinitializing layer res3d_branch2b.kernel\n",
      "reinitializing layer res3d_branch2b.bias\n",
      "LAYER:: bn3d_branch2b\n",
      "reinitializing layer bn3d_branch2b.gamma\n",
      "reinitializing layer bn3d_branch2b.beta\n",
      "reinitializing layer bn3d_branch2b.moving_mean\n",
      "reinitializing layer bn3d_branch2b.moving_variance\n",
      "LAYER:: activation_71\n",
      "LAYER:: res3d_branch2c\n",
      "reinitializing layer res3d_branch2c.kernel\n",
      "reinitializing layer res3d_branch2c.bias\n",
      "LAYER:: bn3d_branch2c\n",
      "reinitializing layer bn3d_branch2c.gamma\n",
      "reinitializing layer bn3d_branch2c.beta\n",
      "reinitializing layer bn3d_branch2c.moving_mean\n",
      "reinitializing layer bn3d_branch2c.moving_variance\n",
      "LAYER:: add_23\n",
      "LAYER:: activation_72\n",
      "LAYER:: res4a_branch2a\n",
      "reinitializing layer res4a_branch2a.kernel\n",
      "reinitializing layer res4a_branch2a.bias\n",
      "LAYER:: bn4a_branch2a\n",
      "reinitializing layer bn4a_branch2a.gamma\n",
      "reinitializing layer bn4a_branch2a.beta\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reinitializing layer bn4a_branch2a.moving_mean\n",
      "reinitializing layer bn4a_branch2a.moving_variance\n",
      "LAYER:: activation_73\n",
      "LAYER:: res4a_branch2b\n",
      "reinitializing layer res4a_branch2b.kernel\n",
      "reinitializing layer res4a_branch2b.bias\n",
      "LAYER:: bn4a_branch2b\n",
      "reinitializing layer bn4a_branch2b.gamma\n",
      "reinitializing layer bn4a_branch2b.beta\n",
      "reinitializing layer bn4a_branch2b.moving_mean\n",
      "reinitializing layer bn4a_branch2b.moving_variance\n",
      "LAYER:: activation_74\n",
      "LAYER:: res4a_branch2c\n",
      "reinitializing layer res4a_branch2c.kernel\n",
      "reinitializing layer res4a_branch2c.bias\n",
      "LAYER:: res4a_branch1\n",
      "reinitializing layer res4a_branch1.kernel\n",
      "reinitializing layer res4a_branch1.bias\n",
      "LAYER:: bn4a_branch2c\n",
      "reinitializing layer bn4a_branch2c.gamma\n",
      "reinitializing layer bn4a_branch2c.beta\n",
      "reinitializing layer bn4a_branch2c.moving_mean\n",
      "reinitializing layer bn4a_branch2c.moving_variance\n",
      "LAYER:: bn4a_branch1\n",
      "reinitializing layer bn4a_branch1.gamma\n",
      "reinitializing layer bn4a_branch1.beta\n",
      "reinitializing layer bn4a_branch1.moving_mean\n",
      "reinitializing layer bn4a_branch1.moving_variance\n",
      "LAYER:: add_24\n",
      "LAYER:: activation_75\n",
      "LAYER:: res4b_branch2a\n",
      "reinitializing layer res4b_branch2a.kernel\n",
      "reinitializing layer res4b_branch2a.bias\n",
      "LAYER:: bn4b_branch2a\n",
      "reinitializing layer bn4b_branch2a.gamma\n",
      "reinitializing layer bn4b_branch2a.beta\n",
      "reinitializing layer bn4b_branch2a.moving_mean\n",
      "reinitializing layer bn4b_branch2a.moving_variance\n",
      "LAYER:: activation_76\n",
      "LAYER:: res4b_branch2b\n",
      "reinitializing layer res4b_branch2b.kernel\n",
      "reinitializing layer res4b_branch2b.bias\n",
      "LAYER:: bn4b_branch2b\n",
      "reinitializing layer bn4b_branch2b.gamma\n",
      "reinitializing layer bn4b_branch2b.beta\n",
      "reinitializing layer bn4b_branch2b.moving_mean\n",
      "reinitializing layer bn4b_branch2b.moving_variance\n",
      "LAYER:: activation_77\n",
      "LAYER:: res4b_branch2c\n",
      "reinitializing layer res4b_branch2c.kernel\n",
      "reinitializing layer res4b_branch2c.bias\n",
      "LAYER:: bn4b_branch2c\n",
      "reinitializing layer bn4b_branch2c.gamma\n",
      "reinitializing layer bn4b_branch2c.beta\n",
      "reinitializing layer bn4b_branch2c.moving_mean\n",
      "reinitializing layer bn4b_branch2c.moving_variance\n",
      "LAYER:: add_25\n",
      "LAYER:: activation_78\n",
      "LAYER:: res4c_branch2a\n",
      "reinitializing layer res4c_branch2a.kernel\n",
      "reinitializing layer res4c_branch2a.bias\n",
      "LAYER:: bn4c_branch2a\n",
      "reinitializing layer bn4c_branch2a.gamma\n",
      "reinitializing layer bn4c_branch2a.beta\n",
      "reinitializing layer bn4c_branch2a.moving_mean\n",
      "reinitializing layer bn4c_branch2a.moving_variance\n",
      "LAYER:: activation_79\n",
      "LAYER:: res4c_branch2b\n",
      "reinitializing layer res4c_branch2b.kernel\n",
      "reinitializing layer res4c_branch2b.bias\n",
      "LAYER:: bn4c_branch2b\n",
      "reinitializing layer bn4c_branch2b.gamma\n",
      "reinitializing layer bn4c_branch2b.beta\n",
      "reinitializing layer bn4c_branch2b.moving_mean\n",
      "reinitializing layer bn4c_branch2b.moving_variance\n",
      "LAYER:: activation_80\n",
      "LAYER:: res4c_branch2c\n",
      "reinitializing layer res4c_branch2c.kernel\n",
      "reinitializing layer res4c_branch2c.bias\n",
      "LAYER:: bn4c_branch2c\n",
      "reinitializing layer bn4c_branch2c.gamma\n",
      "reinitializing layer bn4c_branch2c.beta\n",
      "reinitializing layer bn4c_branch2c.moving_mean\n",
      "reinitializing layer bn4c_branch2c.moving_variance\n",
      "LAYER:: add_26\n",
      "LAYER:: activation_81\n",
      "LAYER:: res4d_branch2a\n",
      "reinitializing layer res4d_branch2a.kernel\n",
      "reinitializing layer res4d_branch2a.bias\n",
      "LAYER:: bn4d_branch2a\n",
      "reinitializing layer bn4d_branch2a.gamma\n",
      "reinitializing layer bn4d_branch2a.beta\n",
      "reinitializing layer bn4d_branch2a.moving_mean\n",
      "reinitializing layer bn4d_branch2a.moving_variance\n",
      "LAYER:: activation_82\n",
      "LAYER:: res4d_branch2b\n",
      "reinitializing layer res4d_branch2b.kernel\n",
      "reinitializing layer res4d_branch2b.bias\n",
      "LAYER:: bn4d_branch2b\n",
      "reinitializing layer bn4d_branch2b.gamma\n",
      "reinitializing layer bn4d_branch2b.beta\n",
      "reinitializing layer bn4d_branch2b.moving_mean\n",
      "reinitializing layer bn4d_branch2b.moving_variance\n",
      "LAYER:: activation_83\n",
      "LAYER:: res4d_branch2c\n",
      "reinitializing layer res4d_branch2c.kernel\n",
      "reinitializing layer res4d_branch2c.bias\n",
      "LAYER:: bn4d_branch2c\n",
      "reinitializing layer bn4d_branch2c.gamma\n",
      "reinitializing layer bn4d_branch2c.beta\n",
      "reinitializing layer bn4d_branch2c.moving_mean\n",
      "reinitializing layer bn4d_branch2c.moving_variance\n",
      "LAYER:: add_27\n",
      "LAYER:: activation_84\n",
      "LAYER:: res4e_branch2a\n",
      "reinitializing layer res4e_branch2a.kernel\n",
      "reinitializing layer res4e_branch2a.bias\n",
      "LAYER:: bn4e_branch2a\n",
      "reinitializing layer bn4e_branch2a.gamma\n",
      "reinitializing layer bn4e_branch2a.beta\n",
      "reinitializing layer bn4e_branch2a.moving_mean\n",
      "reinitializing layer bn4e_branch2a.moving_variance\n",
      "LAYER:: activation_85\n",
      "LAYER:: res4e_branch2b\n",
      "reinitializing layer res4e_branch2b.kernel\n",
      "reinitializing layer res4e_branch2b.bias\n",
      "LAYER:: bn4e_branch2b\n",
      "reinitializing layer bn4e_branch2b.gamma\n",
      "reinitializing layer bn4e_branch2b.beta\n",
      "reinitializing layer bn4e_branch2b.moving_mean\n",
      "reinitializing layer bn4e_branch2b.moving_variance\n",
      "LAYER:: activation_86\n",
      "LAYER:: res4e_branch2c\n",
      "reinitializing layer res4e_branch2c.kernel\n",
      "reinitializing layer res4e_branch2c.bias\n",
      "LAYER:: bn4e_branch2c\n",
      "reinitializing layer bn4e_branch2c.gamma\n",
      "reinitializing layer bn4e_branch2c.beta\n",
      "reinitializing layer bn4e_branch2c.moving_mean\n",
      "reinitializing layer bn4e_branch2c.moving_variance\n",
      "LAYER:: add_28\n",
      "LAYER:: activation_87\n",
      "LAYER:: res4f_branch2a\n",
      "reinitializing layer res4f_branch2a.kernel\n",
      "reinitializing layer res4f_branch2a.bias\n",
      "LAYER:: bn4f_branch2a\n",
      "reinitializing layer bn4f_branch2a.gamma\n",
      "reinitializing layer bn4f_branch2a.beta\n",
      "reinitializing layer bn4f_branch2a.moving_mean\n",
      "reinitializing layer bn4f_branch2a.moving_variance\n",
      "LAYER:: activation_88\n",
      "LAYER:: res4f_branch2b\n",
      "reinitializing layer res4f_branch2b.kernel\n",
      "reinitializing layer res4f_branch2b.bias\n",
      "LAYER:: bn4f_branch2b\n",
      "reinitializing layer bn4f_branch2b.gamma\n",
      "reinitializing layer bn4f_branch2b.beta\n",
      "reinitializing layer bn4f_branch2b.moving_mean\n",
      "reinitializing layer bn4f_branch2b.moving_variance\n",
      "LAYER:: activation_89\n",
      "LAYER:: res4f_branch2c\n",
      "reinitializing layer res4f_branch2c.kernel\n",
      "reinitializing layer res4f_branch2c.bias\n",
      "LAYER:: bn4f_branch2c\n",
      "reinitializing layer bn4f_branch2c.gamma\n",
      "reinitializing layer bn4f_branch2c.beta\n",
      "reinitializing layer bn4f_branch2c.moving_mean\n",
      "reinitializing layer bn4f_branch2c.moving_variance\n",
      "LAYER:: add_29\n",
      "LAYER:: activation_90\n",
      "LAYER:: res5a_branch2a\n",
      "reinitializing layer res5a_branch2a.kernel\n",
      "reinitializing layer res5a_branch2a.bias\n",
      "LAYER:: bn5a_branch2a\n",
      "reinitializing layer bn5a_branch2a.gamma\n",
      "reinitializing layer bn5a_branch2a.beta\n",
      "reinitializing layer bn5a_branch2a.moving_mean\n",
      "reinitializing layer bn5a_branch2a.moving_variance\n",
      "LAYER:: activation_91\n",
      "LAYER:: res5a_branch2b\n",
      "reinitializing layer res5a_branch2b.kernel\n",
      "reinitializing layer res5a_branch2b.bias\n",
      "LAYER:: bn5a_branch2b\n",
      "reinitializing layer bn5a_branch2b.gamma\n",
      "reinitializing layer bn5a_branch2b.beta\n",
      "reinitializing layer bn5a_branch2b.moving_mean\n",
      "reinitializing layer bn5a_branch2b.moving_variance\n",
      "LAYER:: activation_92\n",
      "LAYER:: res5a_branch2c\n",
      "reinitializing layer res5a_branch2c.kernel\n",
      "reinitializing layer res5a_branch2c.bias\n",
      "LAYER:: res5a_branch1\n",
      "reinitializing layer res5a_branch1.kernel\n",
      "reinitializing layer res5a_branch1.bias\n",
      "LAYER:: bn5a_branch2c\n",
      "reinitializing layer bn5a_branch2c.gamma\n",
      "reinitializing layer bn5a_branch2c.beta\n",
      "reinitializing layer bn5a_branch2c.moving_mean\n",
      "reinitializing layer bn5a_branch2c.moving_variance\n",
      "LAYER:: bn5a_branch1\n",
      "reinitializing layer bn5a_branch1.gamma\n",
      "reinitializing layer bn5a_branch1.beta\n",
      "reinitializing layer bn5a_branch1.moving_mean\n",
      "reinitializing layer bn5a_branch1.moving_variance\n",
      "LAYER:: add_30\n",
      "LAYER:: activation_93\n",
      "LAYER:: res5b_branch2a\n",
      "reinitializing layer res5b_branch2a.kernel\n",
      "reinitializing layer res5b_branch2a.bias\n",
      "LAYER:: bn5b_branch2a\n",
      "reinitializing layer bn5b_branch2a.gamma\n",
      "reinitializing layer bn5b_branch2a.beta\n",
      "reinitializing layer bn5b_branch2a.moving_mean\n",
      "reinitializing layer bn5b_branch2a.moving_variance\n",
      "LAYER:: activation_94\n",
      "LAYER:: res5b_branch2b\n",
      "reinitializing layer res5b_branch2b.kernel\n",
      "reinitializing layer res5b_branch2b.bias\n",
      "LAYER:: bn5b_branch2b\n",
      "reinitializing layer bn5b_branch2b.gamma\n",
      "reinitializing layer bn5b_branch2b.beta\n",
      "reinitializing layer bn5b_branch2b.moving_mean\n",
      "reinitializing layer bn5b_branch2b.moving_variance\n",
      "LAYER:: activation_95\n",
      "LAYER:: res5b_branch2c\n",
      "reinitializing layer res5b_branch2c.kernel\n",
      "reinitializing layer res5b_branch2c.bias\n",
      "LAYER:: bn5b_branch2c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reinitializing layer bn5b_branch2c.gamma\n",
      "reinitializing layer bn5b_branch2c.beta\n",
      "reinitializing layer bn5b_branch2c.moving_mean\n",
      "reinitializing layer bn5b_branch2c.moving_variance\n",
      "LAYER:: add_31\n",
      "LAYER:: activation_96\n",
      "LAYER:: res5c_branch2a\n",
      "reinitializing layer res5c_branch2a.kernel\n",
      "reinitializing layer res5c_branch2a.bias\n",
      "LAYER:: bn5c_branch2a\n",
      "reinitializing layer bn5c_branch2a.gamma\n",
      "reinitializing layer bn5c_branch2a.beta\n",
      "reinitializing layer bn5c_branch2a.moving_mean\n",
      "reinitializing layer bn5c_branch2a.moving_variance\n",
      "LAYER:: activation_97\n",
      "LAYER:: res5c_branch2b\n",
      "reinitializing layer res5c_branch2b.kernel\n",
      "reinitializing layer res5c_branch2b.bias\n",
      "LAYER:: bn5c_branch2b\n",
      "reinitializing layer bn5c_branch2b.gamma\n",
      "reinitializing layer bn5c_branch2b.beta\n",
      "reinitializing layer bn5c_branch2b.moving_mean\n",
      "reinitializing layer bn5c_branch2b.moving_variance\n",
      "LAYER:: activation_98\n",
      "LAYER:: res5c_branch2c\n",
      "reinitializing layer res5c_branch2c.kernel\n",
      "reinitializing layer res5c_branch2c.bias\n",
      "LAYER:: bn5c_branch2c\n",
      "reinitializing layer bn5c_branch2c.gamma\n",
      "reinitializing layer bn5c_branch2c.beta\n",
      "reinitializing layer bn5c_branch2c.moving_mean\n",
      "reinitializing layer bn5c_branch2c.moving_variance\n",
      "LAYER:: add_32\n",
      "LAYER:: activation_99\n",
      "LAYER:: avg_pool\n",
      "LAYER:: dense_2\n",
      "reinitializing layer dense_2.kernel\n",
      "reinitializing layer dense_2.bias\n",
      "LAYER:: activation_100\n",
      "Train on 1053 samples, validate on 264 samples\n",
      "Epoch 1/100\n",
      "1053/1053 [==============================] - 112s 107ms/step - loss: 0.7481 - mse: 0.7481 - val_loss: 0.1905 - val_mse: 0.1905\n",
      "Epoch 2/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0210 - mse: 0.0210 - val_loss: 0.1373 - val_mse: 0.1373\n",
      "Epoch 3/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.2769 - val_mse: 0.2769\n",
      "Epoch 4/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.7541 - val_mse: 0.7541\n",
      "Epoch 5/100\n",
      "1053/1053 [==============================] - 101s 95ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 30.3030 - val_mse: 30.3030\n",
      "Epoch 6/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.2411 - val_mse: 0.2411\n",
      "Epoch 7/100\n",
      "1053/1053 [==============================] - 101s 95ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0675 - val_mse: 0.0675\n",
      "Epoch 8/100\n",
      "1053/1053 [==============================] - 101s 95ms/step - loss: 0.0190 - mse: 0.0190 - val_loss: 0.0562 - val_mse: 0.0562\n",
      "Epoch 9/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0399 - mse: 0.0399 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 10/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 11/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0079 - mse: 0.0079 - val_loss: 0.3530 - val_mse: 0.3530\n",
      "Epoch 12/100\n",
      "1053/1053 [==============================] - 101s 95ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 13/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0910 - val_mse: 0.0910\n",
      "Epoch 14/100\n",
      "1053/1053 [==============================] - 101s 95ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0425 - val_mse: 0.0425\n",
      "Epoch 15/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.2862 - val_mse: 0.2862\n",
      "Epoch 16/100\n",
      "1053/1053 [==============================] - 101s 95ms/step - loss: 0.0068 - mse: 0.0068 - val_loss: 0.1302 - val_mse: 0.1302\n",
      "Epoch 17/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0069 - val_mse: 0.0069\n",
      "Epoch 18/100\n",
      "1053/1053 [==============================] - 101s 95ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 19/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0050 - val_mse: 0.0050\n",
      "Epoch 20/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.6716 - mse: 0.6716 - val_loss: 1427888.2642 - val_mse: 1427888.2500\n",
      "Epoch 21/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0341 - mse: 0.0341 - val_loss: 0.0585 - val_mse: 0.0585\n",
      "Epoch 22/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0238 - mse: 0.0238 - val_loss: 0.1154 - val_mse: 0.1154\n",
      "Epoch 23/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 0.0196 - val_mse: 0.0196\n",
      "Epoch 24/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0162 - val_mse: 0.0162\n",
      "Epoch 25/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0159 - mse: 0.0159 - val_loss: 0.0379 - val_mse: 0.0379\n",
      "Epoch 26/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0213 - val_mse: 0.0213\n",
      "Epoch 27/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 28/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0130 - val_mse: 0.0130\n",
      "Epoch 29/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0479 - val_mse: 0.0479\n",
      "Epoch 30/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0105 - val_mse: 0.0105\n",
      "Epoch 31/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0507 - val_mse: 0.0507\n",
      "Epoch 32/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0071 - val_mse: 0.0071\n",
      "Epoch 33/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0272 - mse: 0.0272 - val_loss: 0.0121 - val_mse: 0.0121\n",
      "Epoch 34/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0122 - val_mse: 0.0122\n",
      "Epoch 35/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0142 - mse: 0.0142 - val_loss: 0.0107 - val_mse: 0.0107\n",
      "Epoch 36/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 37/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0071 - val_mse: 0.0071\n",
      "Epoch 38/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 39/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0056 - val_mse: 0.0056\n",
      "Epoch 40/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0213 - val_mse: 0.0213\n",
      "Epoch 41/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 42/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0058 - val_mse: 0.0058\n",
      "Epoch 43/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0057 - val_mse: 0.0057\n",
      "Epoch 44/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 45/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 46/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 47/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 48/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0049 - val_mse: 0.0049\n",
      "Epoch 49/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0050 - val_mse: 0.0050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 51/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 52/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 53/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 54/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 55/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.0538 - val_mse: 0.0538\n",
      "Epoch 56/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0282 - val_mse: 0.0282\n",
      "Epoch 57/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0061 - mse: 0.0061 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 58/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0056 - val_mse: 0.0056\n",
      "Epoch 59/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 60/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0064 - val_mse: 0.0064\n",
      "Epoch 61/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0056 - val_mse: 0.0056\n",
      "Epoch 62/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 63/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 64/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0070 - val_mse: 0.0070\n",
      "Epoch 65/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0308 - val_mse: 0.0308\n",
      "Epoch 66/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 67/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 68/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0069 - val_mse: 0.0069\n",
      "Epoch 69/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0119 - val_mse: 0.0119\n",
      "Epoch 70/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0050 - val_mse: 0.0050\n",
      "Epoch 71/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0161 - val_mse: 0.0161\n",
      "Epoch 72/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 73/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 74/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 75/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0133 - val_mse: 0.0133\n",
      "Epoch 76/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 77/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0047 - val_mse: 0.0047\n",
      "Epoch 78/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.1418 - val_mse: 0.1418\n",
      "Epoch 79/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 80/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 81/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 82/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0078 - val_mse: 0.0078\n",
      "Epoch 83/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 84/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 85/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 86/100\n",
      "1053/1053 [==============================] - 99s 94ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 87/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 88/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0225 - val_mse: 0.0225\n",
      "Epoch 89/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0101 - val_mse: 0.0101\n",
      "Epoch 90/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 91/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 92/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 93/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 94/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 95/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 96/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 97/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0071 - val_mse: 0.0071\n",
      "Epoch 98/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 99/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 100/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "330/330 [==============================] - 30s 92ms/step\n",
      "(330,)\n",
      "(330,)\n",
      "[ 7.99171627e-01  7.05888901e+01  2.56571808e+02  2.21235580e+02\n",
      "  4.66696198e+02  1.24943886e+02  2.42347839e+02  6.18162193e+01\n",
      "  1.56719681e+02  1.50196075e+02  2.63978851e+02  1.68397655e+01\n",
      "  1.52633636e+02  7.51615143e+01  1.06628326e+02  3.39997978e+01\n",
      "  1.47808319e+02  1.31395523e+02  2.59041077e+02  2.08797775e+02\n",
      "  4.82192017e+02  2.68154572e+02  2.90782806e+02  7.88633957e+01\n",
      "  5.74778709e+01  1.81234833e+02  1.27031174e+02  7.58766785e+01\n",
      "  1.38821182e+02  1.33583282e+02  1.39684738e+02  1.11155777e+02\n",
      "  1.23211044e+02  5.94539185e+01  1.16801178e+02  1.50895386e+02\n",
      "  2.94008026e+02  1.14162636e+01  4.53422775e+01  4.00829559e+02\n",
      "  2.86050446e+02  2.85255066e+02  1.55639221e+02  2.32158279e+02\n",
      "  2.84300873e+02  2.98756592e+02  3.39040756e+01  8.49847507e+00\n",
      "  8.66054688e+01  1.42400055e+02  9.54471529e-01  9.08130188e+01\n",
      "  1.21963425e+02  4.91876335e+01  1.85092484e+02  1.78519424e+02\n",
      "  1.20966599e+02  2.46248993e+02  2.42169373e+02 -2.35844398e+00\n",
      "  2.11285019e+02  2.92678223e+02  3.31519775e+01  4.29310852e+02\n",
      "  2.39861511e+02  2.83223816e+02  2.57452942e+02  2.23832901e+02\n",
      "  2.67610840e+02  9.58539963e+01  2.35251602e+02  1.80426193e+02\n",
      "  1.55934036e+02  1.57313141e+02  5.63760567e+01  1.90769135e+02\n",
      "  1.53944550e+02  1.41728592e+02  1.14765778e+02  6.87376862e+01\n",
      "  1.13735855e+02  3.53828674e+02  1.19768448e+02  2.15289917e+02\n",
      "  1.25194252e+02  4.79981155e+01  4.78740463e+01  5.94045677e+01\n",
      "  1.86020844e+02  9.36043015e+01  2.47938568e+02  5.17005491e+00\n",
      "  1.70205505e+02  1.58162384e+02  2.92907257e+02  2.29945206e+02\n",
      "  9.74290390e+01  1.90769135e+02  6.61911774e+01  6.37371407e+01\n",
      "  1.80783783e+02  8.73556519e+01  7.48016052e+01  7.12694979e+00\n",
      " -1.22251368e+00  4.78294907e+01  1.70070023e+02  1.85283966e+02\n",
      "  3.79594994e+01  1.35096979e+01  6.86261368e+01  1.61096329e+02\n",
      "  1.37878830e+02  4.72749977e+01  1.13750900e+02  3.87671265e+02\n",
      "  6.98620844e+00  2.71205200e+02  2.72808151e+01  2.72314331e+02\n",
      "  2.57347992e+02  1.43234436e+02  1.56212509e+02  2.39211060e+02\n",
      "  2.42357254e+02  1.33635284e+02  2.62655396e+02  3.31178589e+01\n",
      "  7.55726395e+01  1.26086342e+02  2.34983856e+02  3.19416260e+02\n",
      "  4.39071259e+02  3.78861809e+01  4.74005966e+01  2.22420212e+02\n",
      "  6.21976395e+01  1.23343788e+02  4.62524414e+01  4.68998535e+02\n",
      "  6.14641571e+01  5.35948296e+01  2.28315592e-01  4.31528290e+02\n",
      "  7.65190735e+01  1.01148781e+02  1.17374313e+02  8.83402634e+01\n",
      "  1.23573936e+02  1.63047348e+02  2.75121063e+02  8.17999802e+01\n",
      "  9.67897491e+01  7.84844160e-02  4.48413391e+02  7.53937674e+00\n",
      "  1.46959412e+02  7.37085953e+01  2.07926006e+01  1.43674118e+02\n",
      "  1.61447174e+02 -1.15513135e+03  9.44600525e+01  1.21008408e+02\n",
      "  3.30349236e+01  2.16502666e+01  2.55382065e+02  1.21591553e+02\n",
      "  2.12385773e+02  2.86756191e+01  2.83431931e+01  1.16160217e+02\n",
      "  1.70100937e+01  1.49922668e+02  5.20134277e+01  1.84375977e+02\n",
      "  2.88074402e+02  1.16562500e+02  2.56843353e+02  7.96356812e+01\n",
      "  4.36222137e+02  1.63848984e+02  1.16106712e+02  1.23155251e+02\n",
      "  7.05900421e+01  1.24928337e+02  1.53319473e+02  1.94778992e+02\n",
      "  1.49356140e+02  2.68929310e+01  1.23796173e+02  9.58231354e+01\n",
      "  1.42990131e+01  1.34239243e+02  3.83111786e+02  1.49502426e+02\n",
      "  1.09226814e+02  7.78572845e+01  1.01371056e+02  5.54527054e+01\n",
      "  8.71922455e+01  1.61463760e+02  1.79467316e+02  1.13050552e+02\n",
      "  4.85448875e+01  6.22835045e+01  1.21458511e+02  1.95284958e+02\n",
      "  1.00321770e+02  2.32180298e+02  2.11796158e+02  1.60098129e+02\n",
      "  1.34409164e+02  1.50057800e+02  1.27544014e+02  5.73972206e+01\n",
      "  2.49124146e+02  2.60937958e+02  4.30757675e+01  2.73919678e+02\n",
      "  1.28109390e+02  1.55621918e+02  1.62575321e+01  1.31210342e+02\n",
      "  2.91114166e+02  3.12921963e+01  1.45134926e+01  1.20536011e+02\n",
      "  1.25171959e+02  4.81901283e+01  2.33188934e+02  8.52131844e+00\n",
      "  2.09689148e+02  2.35239899e+02  6.59886932e+01  1.02587700e+02\n",
      "  1.62921051e+02  4.74709358e+01  1.03206253e+02  1.61608524e+01\n",
      "  2.27135147e+02  7.68951492e+01  1.27012405e+02  6.21716690e+01\n",
      "  2.97499390e+02  7.36411972e+01  6.03220787e+01  1.52580582e+02\n",
      "  3.18638802e+01  1.49149567e+02  1.51612411e+02  2.05014709e+02\n",
      "  1.49599640e+02  6.42724457e+01  3.34006592e+02  2.62813110e+02\n",
      "  1.00083359e+02  2.11190430e+02  3.74215164e+01  1.01699982e+02\n",
      "  7.00836029e+01  1.40560028e+02  1.46993912e+02  4.90952179e+02\n",
      "  2.15547943e+02  6.03481712e+01  7.12025452e+01  2.86463623e+02\n",
      "  1.19557243e+02  1.19722099e+02  1.10302315e+02  1.00769676e+02\n",
      "  2.47225311e+02  2.40464951e+02  1.56798811e+01  1.60170120e+02\n",
      "  1.78279465e+02  2.39211060e+02  1.80805527e+02  1.28936707e+02\n",
      "  1.13563057e+02  2.12934372e+02  2.54371918e+02  2.24886063e+02\n",
      "  8.46465149e+01  1.81756916e+01  1.29289276e+02 -1.11531472e+00\n",
      "  2.88847961e+02  2.63976837e+02  1.50652359e+02  1.51996429e+02\n",
      "  6.58302689e+01  3.10138798e+01  2.81119812e+02  2.33117813e+02\n",
      "  1.15264008e+02  1.18372566e+02  1.49989243e+02  2.58209534e+02\n",
      "  1.81231476e+02  4.81885529e+02  1.79072678e+02  9.35759354e+01\n",
      "  1.23307991e+02  8.69158173e+01  1.87116470e+02  8.66987228e+01\n",
      "  4.05141418e+02  6.96725235e+01  6.09161530e+01  1.97204418e+01\n",
      "  1.56298401e+02  1.52857304e+01  2.27098709e+02  4.57284241e+02\n",
      "  3.41972961e+02  1.35307022e+02  5.51022034e+01  3.11683258e+02\n",
      "  2.43584183e+02  1.27506912e+02  2.75019073e+02  2.71945679e+02\n",
      "  2.99924088e+01  1.46200958e+02  3.98294067e+01  1.70580658e+02\n",
      "  1.57339310e+02  6.52289276e+01]\n",
      "[  6.  54. 275. 240. 450. 108. 225.  58. 150. 162. 250.  80. 162.  64.\n",
      " 100.  46. 174. 128. 250. 184. 500. 300. 285.  68.  88.  30. 120.   8.\n",
      " 128. 154. 144. 126.  80.  52. 112. 158. 300.  18.  66. 450. 265. 290.\n",
      " 168. 255. 295. 300.  32.  24.  96. 132.   6.  92. 186.   8. 180. 196.\n",
      " 108. 230. 225.   8. 215. 290.  54. 450. 250. 270. 230. 200. 225.  80.\n",
      " 245.   4. 190. 168.  44. 225. 182. 122. 120.   4. 124. 350.  56.   2.\n",
      "  12.  84.  84.  78.  84.  82. 225.  18. 176. 174. 198. 225. 100. 225.\n",
      "  46.  60. 188.  88.  70.  10.  24.  48. 172. 186.  52.  10.   2. 188.\n",
      " 156.  48. 116. 400.  30. 275.  40. 275. 215. 156. 190. 225. 245. 172.\n",
      " 235. 102.   6. 122. 265. 295. 450.  46.  86. 196.  84. 112.  10. 500.\n",
      "  20.  58.  34. 500.  14.  82. 104.  92. 154. 188. 285. 100. 120.  34.\n",
      " 500.  16. 132.  46.  12. 136.  22.   1. 100.  90.  58.  38. 270. 112.\n",
      " 215.  32.  26. 124.  16. 144.  90. 182. 300. 134. 215. 102. 450. 128.\n",
      "  94. 138.  72. 146. 158.   0. 112.  40. 178.  98.  32. 136. 400.   4.\n",
      " 112.  28. 106.  28.  74. 200. 192.  96.  58.  32. 140. 172.  72.  40.\n",
      " 200. 156. 162. 142. 134.  76. 255. 240.  58. 280. 124. 128.  28. 136.\n",
      " 275.  26.  16. 128. 134.  64. 225.  32. 186. 220.  52. 154. 184.  54.\n",
      " 114.  18. 220.  94. 130.  60. 295.  74.  60. 164.  10. 198. 138. 200.\n",
      " 128.  74. 350. 285. 102.   2.   4.  92.  86. 154. 190. 500. 186.  42.\n",
      "  62. 275. 110. 124. 116.  98. 275. 265.  28. 182. 198. 225. 170. 154.\n",
      "  86. 235. 300. 240.  84.  36. 150.  14. 300. 250. 164. 160.  78.  22.\n",
      " 300. 230. 112. 118. 118. 250. 178. 500.   0.  90. 120.  80. 186.  90.\n",
      " 450.  74.  64.  24. 140.  36. 215. 450. 350. 136.  68. 290. 220. 116.\n",
      " 300. 290.   8. 120.  48. 188. 164.  86.]\n",
      "mse =  1461.8393758682405\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pic = X_train[1]\n",
    "model = Sequential()\n",
    "#model.add(Input(shape=pic.shape))\n",
    "# model.add(Conv2D(filters=4, kernel_size=(7,7), strides=1, input_shape=pic.shape))\n",
    "Kerasmodel = keras.applications.resnet50.ResNet50(include_top=True\n",
    "                , weights=None#'imagenet'\n",
    "                # , input_tensor=inputs\n",
    "                , input_shape=pic.shape\n",
    "                )\n",
    "# Kerasmodel._layers.pop()\n",
    "# Kerasmodel.layers.pop()\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(1,kernel_initializer='he_normal'))\n",
    "model.add(Activation('linear')) #softmax\n",
    "model = Model( inputs=Kerasmodel.input , outputs=model(Kerasmodel.layers[-2].output))\n",
    "model.compile(loss='mean_squared_error', #mse binary_crossentropy Dice-coefficient loss function vs cross-entropy\n",
    "                optimizer=optimizers.Adam(lr=5e-3),\n",
    "                metrics=['mse'])\n",
    "\n",
    "model.summary()  \n",
    "time_str = str(time.time())\n",
    "print(time_str)\n",
    "reinitLayers(model)\n",
    "early_stop = EarlyStopping(monitor='val_mean_squared_error', patience=30)\n",
    "# model.load_weights('./modelWights/regression_model'+model_fn.__name__+'.h5')\n",
    "model_checkpoint = ModelCheckpoint('./modelWights/weights_'+'resnet50_without_pre'+time_str+'_color.h5', monitor='val_loss', save_best_only=True)\n",
    "tensorboard = keras.callbacks.tensorboard_v1.TensorBoard(log_dir=\"~/data/projects_logs/water_logs/resnet50_without_pre_color_{}\".format(time_str), histogram_freq=2,write_grads=True)\n",
    "history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=100, validation_data=(X_val, y_val), callbacks=[model_checkpoint, ])\n",
    "# print(history.history)\n",
    "y_pred_ = model.predict(X_test, batch_size=1, verbose=1)\n",
    "y_pred_, y_test_ = post_training_data(y_test, y_pred_)\n",
    "predicted_file = 'regress_'+\"resnet50_without_pre\"+\"resnet50\"+time_str+'_color.csv'\n",
    "save_result(\"./result/\"+predicted_file,y_pred_,y_test_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1570958763.4950016\n",
      "LAYER:: input_2\n",
      "LAYER:: conv1_pad\n",
      "LAYER:: conv1\n",
      "reinitializing layer conv1.kernel\n",
      "reinitializing layer conv1.bias\n",
      "LAYER:: bn_conv1\n",
      "reinitializing layer bn_conv1.gamma\n",
      "reinitializing layer bn_conv1.beta\n",
      "reinitializing layer bn_conv1.moving_mean\n",
      "reinitializing layer bn_conv1.moving_variance\n",
      "LAYER:: activation_51\n",
      "LAYER:: pool1_pad\n",
      "LAYER:: max_pooling2d_2\n",
      "LAYER:: res2a_branch2a\n",
      "reinitializing layer res2a_branch2a.kernel\n",
      "reinitializing layer res2a_branch2a.bias\n",
      "LAYER:: bn2a_branch2a\n",
      "reinitializing layer bn2a_branch2a.gamma\n",
      "reinitializing layer bn2a_branch2a.beta\n",
      "reinitializing layer bn2a_branch2a.moving_mean\n",
      "reinitializing layer bn2a_branch2a.moving_variance\n",
      "LAYER:: activation_52\n",
      "LAYER:: res2a_branch2b\n",
      "reinitializing layer res2a_branch2b.kernel\n",
      "reinitializing layer res2a_branch2b.bias\n",
      "LAYER:: bn2a_branch2b\n",
      "reinitializing layer bn2a_branch2b.gamma\n",
      "reinitializing layer bn2a_branch2b.beta\n",
      "reinitializing layer bn2a_branch2b.moving_mean\n",
      "reinitializing layer bn2a_branch2b.moving_variance\n",
      "LAYER:: activation_53\n",
      "LAYER:: res2a_branch2c\n",
      "reinitializing layer res2a_branch2c.kernel\n",
      "reinitializing layer res2a_branch2c.bias\n",
      "LAYER:: res2a_branch1\n",
      "reinitializing layer res2a_branch1.kernel\n",
      "reinitializing layer res2a_branch1.bias\n",
      "LAYER:: bn2a_branch2c\n",
      "reinitializing layer bn2a_branch2c.gamma\n",
      "reinitializing layer bn2a_branch2c.beta\n",
      "reinitializing layer bn2a_branch2c.moving_mean\n",
      "reinitializing layer bn2a_branch2c.moving_variance\n",
      "LAYER:: bn2a_branch1\n",
      "reinitializing layer bn2a_branch1.gamma\n",
      "reinitializing layer bn2a_branch1.beta\n",
      "reinitializing layer bn2a_branch1.moving_mean\n",
      "reinitializing layer bn2a_branch1.moving_variance\n",
      "LAYER:: add_17\n",
      "LAYER:: activation_54\n",
      "LAYER:: res2b_branch2a\n",
      "reinitializing layer res2b_branch2a.kernel\n",
      "reinitializing layer res2b_branch2a.bias\n",
      "LAYER:: bn2b_branch2a\n",
      "reinitializing layer bn2b_branch2a.gamma\n",
      "reinitializing layer bn2b_branch2a.beta\n",
      "reinitializing layer bn2b_branch2a.moving_mean\n",
      "reinitializing layer bn2b_branch2a.moving_variance\n",
      "LAYER:: activation_55\n",
      "LAYER:: res2b_branch2b\n",
      "reinitializing layer res2b_branch2b.kernel\n",
      "reinitializing layer res2b_branch2b.bias\n",
      "LAYER:: bn2b_branch2b\n",
      "reinitializing layer bn2b_branch2b.gamma\n",
      "reinitializing layer bn2b_branch2b.beta\n",
      "reinitializing layer bn2b_branch2b.moving_mean\n",
      "reinitializing layer bn2b_branch2b.moving_variance\n",
      "LAYER:: activation_56\n",
      "LAYER:: res2b_branch2c\n",
      "reinitializing layer res2b_branch2c.kernel\n",
      "reinitializing layer res2b_branch2c.bias\n",
      "LAYER:: bn2b_branch2c\n",
      "reinitializing layer bn2b_branch2c.gamma\n",
      "reinitializing layer bn2b_branch2c.beta\n",
      "reinitializing layer bn2b_branch2c.moving_mean\n",
      "reinitializing layer bn2b_branch2c.moving_variance\n",
      "LAYER:: add_18\n",
      "LAYER:: activation_57\n",
      "LAYER:: res2c_branch2a\n",
      "reinitializing layer res2c_branch2a.kernel\n",
      "reinitializing layer res2c_branch2a.bias\n",
      "LAYER:: bn2c_branch2a\n",
      "reinitializing layer bn2c_branch2a.gamma\n",
      "reinitializing layer bn2c_branch2a.beta\n",
      "reinitializing layer bn2c_branch2a.moving_mean\n",
      "reinitializing layer bn2c_branch2a.moving_variance\n",
      "LAYER:: activation_58\n",
      "LAYER:: res2c_branch2b\n",
      "reinitializing layer res2c_branch2b.kernel\n",
      "reinitializing layer res2c_branch2b.bias\n",
      "LAYER:: bn2c_branch2b\n",
      "reinitializing layer bn2c_branch2b.gamma\n",
      "reinitializing layer bn2c_branch2b.beta\n",
      "reinitializing layer bn2c_branch2b.moving_mean\n",
      "reinitializing layer bn2c_branch2b.moving_variance\n",
      "LAYER:: activation_59\n",
      "LAYER:: res2c_branch2c\n",
      "reinitializing layer res2c_branch2c.kernel\n",
      "reinitializing layer res2c_branch2c.bias\n",
      "LAYER:: bn2c_branch2c\n",
      "reinitializing layer bn2c_branch2c.gamma\n",
      "reinitializing layer bn2c_branch2c.beta\n",
      "reinitializing layer bn2c_branch2c.moving_mean\n",
      "reinitializing layer bn2c_branch2c.moving_variance\n",
      "LAYER:: add_19\n",
      "LAYER:: activation_60\n",
      "LAYER:: res3a_branch2a\n",
      "reinitializing layer res3a_branch2a.kernel\n",
      "reinitializing layer res3a_branch2a.bias\n",
      "LAYER:: bn3a_branch2a\n",
      "reinitializing layer bn3a_branch2a.gamma\n",
      "reinitializing layer bn3a_branch2a.beta\n",
      "reinitializing layer bn3a_branch2a.moving_mean\n",
      "reinitializing layer bn3a_branch2a.moving_variance\n",
      "LAYER:: activation_61\n",
      "LAYER:: res3a_branch2b\n",
      "reinitializing layer res3a_branch2b.kernel\n",
      "reinitializing layer res3a_branch2b.bias\n",
      "LAYER:: bn3a_branch2b\n",
      "reinitializing layer bn3a_branch2b.gamma\n",
      "reinitializing layer bn3a_branch2b.beta\n",
      "reinitializing layer bn3a_branch2b.moving_mean\n",
      "reinitializing layer bn3a_branch2b.moving_variance\n",
      "LAYER:: activation_62\n",
      "LAYER:: res3a_branch2c\n",
      "reinitializing layer res3a_branch2c.kernel\n",
      "reinitializing layer res3a_branch2c.bias\n",
      "LAYER:: res3a_branch1\n",
      "reinitializing layer res3a_branch1.kernel\n",
      "reinitializing layer res3a_branch1.bias\n",
      "LAYER:: bn3a_branch2c\n",
      "reinitializing layer bn3a_branch2c.gamma\n",
      "reinitializing layer bn3a_branch2c.beta\n",
      "reinitializing layer bn3a_branch2c.moving_mean\n",
      "reinitializing layer bn3a_branch2c.moving_variance\n",
      "LAYER:: bn3a_branch1\n",
      "reinitializing layer bn3a_branch1.gamma\n",
      "reinitializing layer bn3a_branch1.beta\n",
      "reinitializing layer bn3a_branch1.moving_mean\n",
      "reinitializing layer bn3a_branch1.moving_variance\n",
      "LAYER:: add_20\n",
      "LAYER:: activation_63\n",
      "LAYER:: res3b_branch2a\n",
      "reinitializing layer res3b_branch2a.kernel\n",
      "reinitializing layer res3b_branch2a.bias\n",
      "LAYER:: bn3b_branch2a\n",
      "reinitializing layer bn3b_branch2a.gamma\n",
      "reinitializing layer bn3b_branch2a.beta\n",
      "reinitializing layer bn3b_branch2a.moving_mean\n",
      "reinitializing layer bn3b_branch2a.moving_variance\n",
      "LAYER:: activation_64\n",
      "LAYER:: res3b_branch2b\n",
      "reinitializing layer res3b_branch2b.kernel\n",
      "reinitializing layer res3b_branch2b.bias\n",
      "LAYER:: bn3b_branch2b\n",
      "reinitializing layer bn3b_branch2b.gamma\n",
      "reinitializing layer bn3b_branch2b.beta\n",
      "reinitializing layer bn3b_branch2b.moving_mean\n",
      "reinitializing layer bn3b_branch2b.moving_variance\n",
      "LAYER:: activation_65\n",
      "LAYER:: res3b_branch2c\n",
      "reinitializing layer res3b_branch2c.kernel\n",
      "reinitializing layer res3b_branch2c.bias\n",
      "LAYER:: bn3b_branch2c\n",
      "reinitializing layer bn3b_branch2c.gamma\n",
      "reinitializing layer bn3b_branch2c.beta\n",
      "reinitializing layer bn3b_branch2c.moving_mean\n",
      "reinitializing layer bn3b_branch2c.moving_variance\n",
      "LAYER:: add_21\n",
      "LAYER:: activation_66\n",
      "LAYER:: res3c_branch2a\n",
      "reinitializing layer res3c_branch2a.kernel\n",
      "reinitializing layer res3c_branch2a.bias\n",
      "LAYER:: bn3c_branch2a\n",
      "reinitializing layer bn3c_branch2a.gamma\n",
      "reinitializing layer bn3c_branch2a.beta\n",
      "reinitializing layer bn3c_branch2a.moving_mean\n",
      "reinitializing layer bn3c_branch2a.moving_variance\n",
      "LAYER:: activation_67\n",
      "LAYER:: res3c_branch2b\n",
      "reinitializing layer res3c_branch2b.kernel\n",
      "reinitializing layer res3c_branch2b.bias\n",
      "LAYER:: bn3c_branch2b\n",
      "reinitializing layer bn3c_branch2b.gamma\n",
      "reinitializing layer bn3c_branch2b.beta\n",
      "reinitializing layer bn3c_branch2b.moving_mean\n",
      "reinitializing layer bn3c_branch2b.moving_variance\n",
      "LAYER:: activation_68\n",
      "LAYER:: res3c_branch2c\n",
      "reinitializing layer res3c_branch2c.kernel\n",
      "reinitializing layer res3c_branch2c.bias\n",
      "LAYER:: bn3c_branch2c\n",
      "reinitializing layer bn3c_branch2c.gamma\n",
      "reinitializing layer bn3c_branch2c.beta\n",
      "reinitializing layer bn3c_branch2c.moving_mean\n",
      "reinitializing layer bn3c_branch2c.moving_variance\n",
      "LAYER:: add_22\n",
      "LAYER:: activation_69\n",
      "LAYER:: res3d_branch2a\n",
      "reinitializing layer res3d_branch2a.kernel\n",
      "reinitializing layer res3d_branch2a.bias\n",
      "LAYER:: bn3d_branch2a\n",
      "reinitializing layer bn3d_branch2a.gamma\n",
      "reinitializing layer bn3d_branch2a.beta\n",
      "reinitializing layer bn3d_branch2a.moving_mean\n",
      "reinitializing layer bn3d_branch2a.moving_variance\n",
      "LAYER:: activation_70\n",
      "LAYER:: res3d_branch2b\n",
      "reinitializing layer res3d_branch2b.kernel\n",
      "reinitializing layer res3d_branch2b.bias\n",
      "LAYER:: bn3d_branch2b\n",
      "reinitializing layer bn3d_branch2b.gamma\n",
      "reinitializing layer bn3d_branch2b.beta\n",
      "reinitializing layer bn3d_branch2b.moving_mean\n",
      "reinitializing layer bn3d_branch2b.moving_variance\n",
      "LAYER:: activation_71\n",
      "LAYER:: res3d_branch2c\n",
      "reinitializing layer res3d_branch2c.kernel\n",
      "reinitializing layer res3d_branch2c.bias\n",
      "LAYER:: bn3d_branch2c\n",
      "reinitializing layer bn3d_branch2c.gamma\n",
      "reinitializing layer bn3d_branch2c.beta\n",
      "reinitializing layer bn3d_branch2c.moving_mean\n",
      "reinitializing layer bn3d_branch2c.moving_variance\n",
      "LAYER:: add_23\n",
      "LAYER:: activation_72\n",
      "LAYER:: res4a_branch2a\n",
      "reinitializing layer res4a_branch2a.kernel\n",
      "reinitializing layer res4a_branch2a.bias\n",
      "LAYER:: bn4a_branch2a\n",
      "reinitializing layer bn4a_branch2a.gamma\n",
      "reinitializing layer bn4a_branch2a.beta\n",
      "reinitializing layer bn4a_branch2a.moving_mean\n",
      "reinitializing layer bn4a_branch2a.moving_variance\n",
      "LAYER:: activation_73\n",
      "LAYER:: res4a_branch2b\n",
      "reinitializing layer res4a_branch2b.kernel\n",
      "reinitializing layer res4a_branch2b.bias\n",
      "LAYER:: bn4a_branch2b\n",
      "reinitializing layer bn4a_branch2b.gamma\n",
      "reinitializing layer bn4a_branch2b.beta\n",
      "reinitializing layer bn4a_branch2b.moving_mean\n",
      "reinitializing layer bn4a_branch2b.moving_variance\n",
      "LAYER:: activation_74\n",
      "LAYER:: res4a_branch2c\n",
      "reinitializing layer res4a_branch2c.kernel\n",
      "reinitializing layer res4a_branch2c.bias\n",
      "LAYER:: res4a_branch1\n",
      "reinitializing layer res4a_branch1.kernel\n",
      "reinitializing layer res4a_branch1.bias\n",
      "LAYER:: bn4a_branch2c\n",
      "reinitializing layer bn4a_branch2c.gamma\n",
      "reinitializing layer bn4a_branch2c.beta\n",
      "reinitializing layer bn4a_branch2c.moving_mean\n",
      "reinitializing layer bn4a_branch2c.moving_variance\n",
      "LAYER:: bn4a_branch1\n",
      "reinitializing layer bn4a_branch1.gamma\n",
      "reinitializing layer bn4a_branch1.beta\n",
      "reinitializing layer bn4a_branch1.moving_mean\n",
      "reinitializing layer bn4a_branch1.moving_variance\n",
      "LAYER:: add_24\n",
      "LAYER:: activation_75\n",
      "LAYER:: res4b_branch2a\n",
      "reinitializing layer res4b_branch2a.kernel\n",
      "reinitializing layer res4b_branch2a.bias\n",
      "LAYER:: bn4b_branch2a\n",
      "reinitializing layer bn4b_branch2a.gamma\n",
      "reinitializing layer bn4b_branch2a.beta\n",
      "reinitializing layer bn4b_branch2a.moving_mean\n",
      "reinitializing layer bn4b_branch2a.moving_variance\n",
      "LAYER:: activation_76\n",
      "LAYER:: res4b_branch2b\n",
      "reinitializing layer res4b_branch2b.kernel\n",
      "reinitializing layer res4b_branch2b.bias\n",
      "LAYER:: bn4b_branch2b\n",
      "reinitializing layer bn4b_branch2b.gamma\n",
      "reinitializing layer bn4b_branch2b.beta\n",
      "reinitializing layer bn4b_branch2b.moving_mean\n",
      "reinitializing layer bn4b_branch2b.moving_variance\n",
      "LAYER:: activation_77\n",
      "LAYER:: res4b_branch2c\n",
      "reinitializing layer res4b_branch2c.kernel\n",
      "reinitializing layer res4b_branch2c.bias\n",
      "LAYER:: bn4b_branch2c\n",
      "reinitializing layer bn4b_branch2c.gamma\n",
      "reinitializing layer bn4b_branch2c.beta\n",
      "reinitializing layer bn4b_branch2c.moving_mean\n",
      "reinitializing layer bn4b_branch2c.moving_variance\n",
      "LAYER:: add_25\n",
      "LAYER:: activation_78\n",
      "LAYER:: res4c_branch2a\n",
      "reinitializing layer res4c_branch2a.kernel\n",
      "reinitializing layer res4c_branch2a.bias\n",
      "LAYER:: bn4c_branch2a\n",
      "reinitializing layer bn4c_branch2a.gamma\n",
      "reinitializing layer bn4c_branch2a.beta\n",
      "reinitializing layer bn4c_branch2a.moving_mean\n",
      "reinitializing layer bn4c_branch2a.moving_variance\n",
      "LAYER:: activation_79\n",
      "LAYER:: res4c_branch2b\n",
      "reinitializing layer res4c_branch2b.kernel\n",
      "reinitializing layer res4c_branch2b.bias\n",
      "LAYER:: bn4c_branch2b\n",
      "reinitializing layer bn4c_branch2b.gamma\n",
      "reinitializing layer bn4c_branch2b.beta\n",
      "reinitializing layer bn4c_branch2b.moving_mean\n",
      "reinitializing layer bn4c_branch2b.moving_variance\n",
      "LAYER:: activation_80\n",
      "LAYER:: res4c_branch2c\n",
      "reinitializing layer res4c_branch2c.kernel\n",
      "reinitializing layer res4c_branch2c.bias\n",
      "LAYER:: bn4c_branch2c\n",
      "reinitializing layer bn4c_branch2c.gamma\n",
      "reinitializing layer bn4c_branch2c.beta\n",
      "reinitializing layer bn4c_branch2c.moving_mean\n",
      "reinitializing layer bn4c_branch2c.moving_variance\n",
      "LAYER:: add_26\n",
      "LAYER:: activation_81\n",
      "LAYER:: res4d_branch2a\n",
      "reinitializing layer res4d_branch2a.kernel\n",
      "reinitializing layer res4d_branch2a.bias\n",
      "LAYER:: bn4d_branch2a\n",
      "reinitializing layer bn4d_branch2a.gamma\n",
      "reinitializing layer bn4d_branch2a.beta\n",
      "reinitializing layer bn4d_branch2a.moving_mean\n",
      "reinitializing layer bn4d_branch2a.moving_variance\n",
      "LAYER:: activation_82\n",
      "LAYER:: res4d_branch2b\n",
      "reinitializing layer res4d_branch2b.kernel\n",
      "reinitializing layer res4d_branch2b.bias\n",
      "LAYER:: bn4d_branch2b\n",
      "reinitializing layer bn4d_branch2b.gamma\n",
      "reinitializing layer bn4d_branch2b.beta\n",
      "reinitializing layer bn4d_branch2b.moving_mean\n",
      "reinitializing layer bn4d_branch2b.moving_variance\n",
      "LAYER:: activation_83\n",
      "LAYER:: res4d_branch2c\n",
      "reinitializing layer res4d_branch2c.kernel\n",
      "reinitializing layer res4d_branch2c.bias\n",
      "LAYER:: bn4d_branch2c\n",
      "reinitializing layer bn4d_branch2c.gamma\n",
      "reinitializing layer bn4d_branch2c.beta\n",
      "reinitializing layer bn4d_branch2c.moving_mean\n",
      "reinitializing layer bn4d_branch2c.moving_variance\n",
      "LAYER:: add_27\n",
      "LAYER:: activation_84\n",
      "LAYER:: res4e_branch2a\n",
      "reinitializing layer res4e_branch2a.kernel\n",
      "reinitializing layer res4e_branch2a.bias\n",
      "LAYER:: bn4e_branch2a\n",
      "reinitializing layer bn4e_branch2a.gamma\n",
      "reinitializing layer bn4e_branch2a.beta\n",
      "reinitializing layer bn4e_branch2a.moving_mean\n",
      "reinitializing layer bn4e_branch2a.moving_variance\n",
      "LAYER:: activation_85\n",
      "LAYER:: res4e_branch2b\n",
      "reinitializing layer res4e_branch2b.kernel\n",
      "reinitializing layer res4e_branch2b.bias\n",
      "LAYER:: bn4e_branch2b\n",
      "reinitializing layer bn4e_branch2b.gamma\n",
      "reinitializing layer bn4e_branch2b.beta\n",
      "reinitializing layer bn4e_branch2b.moving_mean\n",
      "reinitializing layer bn4e_branch2b.moving_variance\n",
      "LAYER:: activation_86\n",
      "LAYER:: res4e_branch2c\n",
      "reinitializing layer res4e_branch2c.kernel\n",
      "reinitializing layer res4e_branch2c.bias\n",
      "LAYER:: bn4e_branch2c\n",
      "reinitializing layer bn4e_branch2c.gamma\n",
      "reinitializing layer bn4e_branch2c.beta\n",
      "reinitializing layer bn4e_branch2c.moving_mean\n",
      "reinitializing layer bn4e_branch2c.moving_variance\n",
      "LAYER:: add_28\n",
      "LAYER:: activation_87\n",
      "LAYER:: res4f_branch2a\n",
      "reinitializing layer res4f_branch2a.kernel\n",
      "reinitializing layer res4f_branch2a.bias\n",
      "LAYER:: bn4f_branch2a\n",
      "reinitializing layer bn4f_branch2a.gamma\n",
      "reinitializing layer bn4f_branch2a.beta\n",
      "reinitializing layer bn4f_branch2a.moving_mean\n",
      "reinitializing layer bn4f_branch2a.moving_variance\n",
      "LAYER:: activation_88\n",
      "LAYER:: res4f_branch2b\n",
      "reinitializing layer res4f_branch2b.kernel\n",
      "reinitializing layer res4f_branch2b.bias\n",
      "LAYER:: bn4f_branch2b\n",
      "reinitializing layer bn4f_branch2b.gamma\n",
      "reinitializing layer bn4f_branch2b.beta\n",
      "reinitializing layer bn4f_branch2b.moving_mean\n",
      "reinitializing layer bn4f_branch2b.moving_variance\n",
      "LAYER:: activation_89\n",
      "LAYER:: res4f_branch2c\n",
      "reinitializing layer res4f_branch2c.kernel\n",
      "reinitializing layer res4f_branch2c.bias\n",
      "LAYER:: bn4f_branch2c\n",
      "reinitializing layer bn4f_branch2c.gamma\n",
      "reinitializing layer bn4f_branch2c.beta\n",
      "reinitializing layer bn4f_branch2c.moving_mean\n",
      "reinitializing layer bn4f_branch2c.moving_variance\n",
      "LAYER:: add_29\n",
      "LAYER:: activation_90\n",
      "LAYER:: res5a_branch2a\n",
      "reinitializing layer res5a_branch2a.kernel\n",
      "reinitializing layer res5a_branch2a.bias\n",
      "LAYER:: bn5a_branch2a\n",
      "reinitializing layer bn5a_branch2a.gamma\n",
      "reinitializing layer bn5a_branch2a.beta\n",
      "reinitializing layer bn5a_branch2a.moving_mean\n",
      "reinitializing layer bn5a_branch2a.moving_variance\n",
      "LAYER:: activation_91\n",
      "LAYER:: res5a_branch2b\n",
      "reinitializing layer res5a_branch2b.kernel\n",
      "reinitializing layer res5a_branch2b.bias\n",
      "LAYER:: bn5a_branch2b\n",
      "reinitializing layer bn5a_branch2b.gamma\n",
      "reinitializing layer bn5a_branch2b.beta\n",
      "reinitializing layer bn5a_branch2b.moving_mean\n",
      "reinitializing layer bn5a_branch2b.moving_variance\n",
      "LAYER:: activation_92\n",
      "LAYER:: res5a_branch2c\n",
      "reinitializing layer res5a_branch2c.kernel\n",
      "reinitializing layer res5a_branch2c.bias\n",
      "LAYER:: res5a_branch1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reinitializing layer res5a_branch1.kernel\n",
      "reinitializing layer res5a_branch1.bias\n",
      "LAYER:: bn5a_branch2c\n",
      "reinitializing layer bn5a_branch2c.gamma\n",
      "reinitializing layer bn5a_branch2c.beta\n",
      "reinitializing layer bn5a_branch2c.moving_mean\n",
      "reinitializing layer bn5a_branch2c.moving_variance\n",
      "LAYER:: bn5a_branch1\n",
      "reinitializing layer bn5a_branch1.gamma\n",
      "reinitializing layer bn5a_branch1.beta\n",
      "reinitializing layer bn5a_branch1.moving_mean\n",
      "reinitializing layer bn5a_branch1.moving_variance\n",
      "LAYER:: add_30\n",
      "LAYER:: activation_93\n",
      "LAYER:: res5b_branch2a\n",
      "reinitializing layer res5b_branch2a.kernel\n",
      "reinitializing layer res5b_branch2a.bias\n",
      "LAYER:: bn5b_branch2a\n",
      "reinitializing layer bn5b_branch2a.gamma\n",
      "reinitializing layer bn5b_branch2a.beta\n",
      "reinitializing layer bn5b_branch2a.moving_mean\n",
      "reinitializing layer bn5b_branch2a.moving_variance\n",
      "LAYER:: activation_94\n",
      "LAYER:: res5b_branch2b\n",
      "reinitializing layer res5b_branch2b.kernel\n",
      "reinitializing layer res5b_branch2b.bias\n",
      "LAYER:: bn5b_branch2b\n",
      "reinitializing layer bn5b_branch2b.gamma\n",
      "reinitializing layer bn5b_branch2b.beta\n",
      "reinitializing layer bn5b_branch2b.moving_mean\n",
      "reinitializing layer bn5b_branch2b.moving_variance\n",
      "LAYER:: activation_95\n",
      "LAYER:: res5b_branch2c\n",
      "reinitializing layer res5b_branch2c.kernel\n",
      "reinitializing layer res5b_branch2c.bias\n",
      "LAYER:: bn5b_branch2c\n",
      "reinitializing layer bn5b_branch2c.gamma\n",
      "reinitializing layer bn5b_branch2c.beta\n",
      "reinitializing layer bn5b_branch2c.moving_mean\n",
      "reinitializing layer bn5b_branch2c.moving_variance\n",
      "LAYER:: add_31\n",
      "LAYER:: activation_96\n",
      "LAYER:: res5c_branch2a\n",
      "reinitializing layer res5c_branch2a.kernel\n",
      "reinitializing layer res5c_branch2a.bias\n",
      "LAYER:: bn5c_branch2a\n",
      "reinitializing layer bn5c_branch2a.gamma\n",
      "reinitializing layer bn5c_branch2a.beta\n",
      "reinitializing layer bn5c_branch2a.moving_mean\n",
      "reinitializing layer bn5c_branch2a.moving_variance\n",
      "LAYER:: activation_97\n",
      "LAYER:: res5c_branch2b\n",
      "reinitializing layer res5c_branch2b.kernel\n",
      "reinitializing layer res5c_branch2b.bias\n",
      "LAYER:: bn5c_branch2b\n",
      "reinitializing layer bn5c_branch2b.gamma\n",
      "reinitializing layer bn5c_branch2b.beta\n",
      "reinitializing layer bn5c_branch2b.moving_mean\n",
      "reinitializing layer bn5c_branch2b.moving_variance\n",
      "LAYER:: activation_98\n",
      "LAYER:: res5c_branch2c\n",
      "reinitializing layer res5c_branch2c.kernel\n",
      "reinitializing layer res5c_branch2c.bias\n",
      "LAYER:: bn5c_branch2c\n",
      "reinitializing layer bn5c_branch2c.gamma\n",
      "reinitializing layer bn5c_branch2c.beta\n",
      "reinitializing layer bn5c_branch2c.moving_mean\n",
      "reinitializing layer bn5c_branch2c.moving_variance\n",
      "LAYER:: add_32\n",
      "LAYER:: activation_99\n",
      "LAYER:: avg_pool\n",
      "LAYER:: dense_2\n",
      "reinitializing layer dense_2.kernel\n",
      "reinitializing layer dense_2.bias\n",
      "LAYER:: activation_100\n",
      "Train on 1053 samples, validate on 264 samples\n",
      "Epoch 1/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 4.4908 - mse: 4.4908 - val_loss: 1.9031 - val_mse: 1.9031\n",
      "Epoch 2/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0354 - mse: 0.0354 - val_loss: 0.1391 - val_mse: 0.1391\n",
      "Epoch 3/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0295 - mse: 0.0295 - val_loss: 0.0386 - val_mse: 0.0386\n",
      "Epoch 4/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0337 - mse: 0.0337 - val_loss: 0.0771 - val_mse: 0.0771\n",
      "Epoch 5/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0252 - mse: 0.0252 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 6/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0239 - mse: 0.0239 - val_loss: 0.0523 - val_mse: 0.0523\n",
      "Epoch 7/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0208 - mse: 0.0208 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 8/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0329 - mse: 0.0329 - val_loss: 0.0175 - val_mse: 0.0175\n",
      "Epoch 9/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.0523 - val_mse: 0.0523\n",
      "Epoch 10/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0371 - mse: 0.0371 - val_loss: 0.0230 - val_mse: 0.0230\n",
      "Epoch 11/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0305 - mse: 0.0305 - val_loss: 0.0519 - val_mse: 0.0519\n",
      "Epoch 12/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0207 - mse: 0.0207 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 13/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0254 - mse: 0.0254 - val_loss: 0.0374 - val_mse: 0.0374\n",
      "Epoch 14/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0361 - mse: 0.0361 - val_loss: 0.0260 - val_mse: 0.0260\n",
      "Epoch 15/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0434 - val_mse: 0.0434\n",
      "Epoch 16/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0208 - mse: 0.0208 - val_loss: 0.0246 - val_mse: 0.0246\n",
      "Epoch 17/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0272 - mse: 0.0272 - val_loss: 0.0874 - val_mse: 0.0874\n",
      "Epoch 18/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0203 - mse: 0.0203 - val_loss: 0.0148 - val_mse: 0.0148\n",
      "Epoch 19/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0190 - mse: 0.0190 - val_loss: 0.0154 - val_mse: 0.0154\n",
      "Epoch 20/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0257 - mse: 0.0257 - val_loss: 0.0299 - val_mse: 0.0299\n",
      "Epoch 21/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0162 - mse: 0.0162 - val_loss: 0.0293 - val_mse: 0.0293\n",
      "Epoch 22/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0101 - val_mse: 0.0101\n",
      "Epoch 23/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "Epoch 24/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 0.0049 - val_mse: 0.0049\n",
      "Epoch 25/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0367 - val_mse: 0.0367\n",
      "Epoch 26/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.1356 - val_mse: 0.1356\n",
      "Epoch 27/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 28/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 29/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0208 - val_mse: 0.0208\n",
      "Epoch 30/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0102 - val_mse: 0.0102\n",
      "Epoch 31/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 32/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 33/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 34/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 35/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0069 - val_mse: 0.0069\n",
      "Epoch 36/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0068 - mse: 0.0068 - val_loss: 0.0149 - val_mse: 0.0149\n",
      "Epoch 37/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0176 - val_mse: 0.0176\n",
      "Epoch 38/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 39/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 40/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 41/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 42/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0171 - val_mse: 0.0171\n",
      "Epoch 43/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0155 - val_mse: 0.0155\n",
      "Epoch 44/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 45/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 46/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0056 - val_mse: 0.0056\n",
      "Epoch 47/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 48/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 49/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0070 - val_mse: 0.0070\n",
      "Epoch 50/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 51/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 52/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 53/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 54/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 55/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 56/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 57/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 58/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 59/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 60/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 61/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0065 - val_mse: 0.0065\n",
      "Epoch 62/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 63/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 64/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0049 - val_mse: 0.0049\n",
      "Epoch 65/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 66/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 67/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 68/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 69/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 70/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 71/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 72/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 73/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 74/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 75/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 76/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 77/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 78/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 79/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 80/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 81/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 82/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 83/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 84/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 85/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 86/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 87/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 88/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0078 - val_mse: 0.0078\n",
      "Epoch 89/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 90/100\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0064 - val_mse: 0.0064\n",
      "Epoch 91/100\n",
      "1053/1053 [==============================] - 78s 74ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 92/100\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 93/100\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 94/100\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 95/100\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 96/100\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 97/100\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 98/100\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 100/100\n",
      "1053/1053 [==============================] - 48s 46ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "330/330 [==============================] - 6s 18ms/step\n",
      "(330,)\n",
      "(330,)\n",
      "[ -7.544741   47.626415  236.28307   209.78804   420.91638   117.17326\n",
      " 222.61697    18.179714  141.29085   128.65845   250.33588    10.225937\n",
      " 145.00607   174.92581    88.36127    10.171339  120.657684  106.71148\n",
      " 248.38469   197.00632   418.27786   244.2723    251.34259    71.2164\n",
      "  39.180725   78.391335  106.03248   -47.693863  123.54485   116.79374\n",
      " 140.03738    71.069176  101.034874   36.90246   119.82554   162.93727\n",
      " 262.19946     1.9213855  23.854048  365.85297   262.20544   244.87167\n",
      " 138.31134   225.40646   257.0662    290.03662    15.760049   -7.8072696\n",
      "  81.86143   133.60622   -21.93369    88.34348   121.369316    4.7524424\n",
      " 168.24792   180.93779    95.21875   233.06107   217.97433   -37.948265\n",
      " 189.36035   282.90726    19.304827  390.0171    215.77122   250.07063\n",
      " 227.38408   208.50679   222.75856    79.047455  217.59177   202.28915\n",
      " 152.59525   147.8613     38.36018   167.71146   131.13275   118.12429\n",
      " 103.527855  -20.299227   98.1616    325.01422    44.963226  250.19765\n",
      "  45.889183   11.161655   11.3208885  54.176525   93.61354    89.767296\n",
      " 235.44666    -5.821183  148.43753   129.15802   255.315     206.5841\n",
      "  86.496216  167.71146    43.335037   57.565174  160.45287    67.31544\n",
      "  66.544235  -25.756449   -2.3973136  30.918465  172.94882   168.90213\n",
      "  53.07017   -25.040329   71.43794   132.82085   120.13938    36.42173\n",
      "  98.05011   363.67035     2.80647   247.13684    28.448149  247.18022\n",
      " 220.1317    143.29796   145.57938   233.19702   231.21924   130.70154\n",
      " 243.12613    54.632397   -8.446977  116.124245  223.10953   270.17117\n",
      " 397.1566     24.48666    43.228783  198.59033    43.603508  104.7533\n",
      "  -9.596721  446.03668   -22.045464   43.91273    -6.400123  407.64954\n",
      "   2.3481398  69.17976   114.10904    83.34753   111.966354  165.66293\n",
      " 259.00516    73.71997    86.01153    -9.14754   388.2574    -11.737481\n",
      " 134.69455    30.154272   21.75489   121.24386    81.77201   457.7083\n",
      "  70.52003   110.72448    19.483149   32.24835   242.45724   116.481064\n",
      " 188.51753    22.653833   13.440251   94.78059     1.5812516 128.73375\n",
      "  44.919945  186.4012    269.20624    94.96113   235.45375    60.276016\n",
      " 391.18332   137.51643    92.69423   110.15444    72.76636   105.94131\n",
      " 150.65411   194.68394   147.34962    10.659352  118.06405   101.17079\n",
      "  99.05763   121.39222   369.21045   106.78393   109.292656   35.305202\n",
      " 104.444954   31.23507    61.816616  169.69656   150.10748    77.418015\n",
      "  47.33461    39.85426   115.00986   172.86032    55.37124    91.87502\n",
      " 209.4645    129.58925   121.71666   158.1569    100.6798     43.452507\n",
      " 218.5452    240.33011    37.459717  244.82166   121.17084   134.02895\n",
      " -28.241798  114.48824   273.74313   -15.839577  -36.99681   107.749535\n",
      " 116.468      39.599495  205.17032    14.908611  185.06844   217.72688\n",
      "  41.045307   88.73838   141.53093    51.508823   86.86345   -10.562703\n",
      " 208.1274    120.71628   119.56962   111.74704   274.46823    58.203323\n",
      "  62.32603   136.55313    -9.980261  126.45179   142.02353   193.62164\n",
      " 149.20486    41.66414   310.71173   255.63837    83.64146   201.30045\n",
      "  -6.165519  101.07034    63.072533  126.418144  135.87097   432.94055\n",
      " 187.85063    33.656197   48.088154  261.04523   115.11218   124.848785\n",
      " 106.515526   89.6995    243.59476   221.79404   -24.780735  150.18724\n",
      " 176.887     233.19702   170.87831   115.94918   117.87513   201.21837\n",
      " 253.07602   216.63388    61.656178   12.930974  113.43577   -39.53038\n",
      " 263.62842   233.49231   127.17688   132.53459    66.789085   -6.466046\n",
      " 263.93134   235.36368    87.44893   105.6616    146.83179   239.3512\n",
      " 157.8131    427.06805   166.14499    77.84506   116.55928    56.86465\n",
      " 148.9656     79.02629   380.5779     66.98029    59.539288   -9.253487\n",
      " 142.74223    14.390901  210.19003   412.14365   336.71906   117.09898\n",
      "  34.84908   265.2418    232.2742    116.359924  251.3928    260.47815\n",
      " -26.332811  140.02988    25.762037  158.241     137.08034    37.760048 ]\n",
      "[  6.  54. 275. 240. 450. 108. 225.  58. 150. 162. 250.  80. 162.  64.\n",
      " 100.  46. 174. 128. 250. 184. 500. 300. 285.  68.  88.  30. 120.   8.\n",
      " 128. 154. 144. 126.  80.  52. 112. 158. 300.  18.  66. 450. 265. 290.\n",
      " 168. 255. 295. 300.  32.  24.  96. 132.   6.  92. 186.   8. 180. 196.\n",
      " 108. 230. 225.   8. 215. 290.  54. 450. 250. 270. 230. 200. 225.  80.\n",
      " 245.   4. 190. 168.  44. 225. 182. 122. 120.   4. 124. 350.  56.   2.\n",
      "  12.  84.  84.  78.  84.  82. 225.  18. 176. 174. 198. 225. 100. 225.\n",
      "  46.  60. 188.  88.  70.  10.  24.  48. 172. 186.  52.  10.   2. 188.\n",
      " 156.  48. 116. 400.  30. 275.  40. 275. 215. 156. 190. 225. 245. 172.\n",
      " 235. 102.   6. 122. 265. 295. 450.  46.  86. 196.  84. 112.  10. 500.\n",
      "  20.  58.  34. 500.  14.  82. 104.  92. 154. 188. 285. 100. 120.  34.\n",
      " 500.  16. 132.  46.  12. 136.  22.   1. 100.  90.  58.  38. 270. 112.\n",
      " 215.  32.  26. 124.  16. 144.  90. 182. 300. 134. 215. 102. 450. 128.\n",
      "  94. 138.  72. 146. 158.   0. 112.  40. 178.  98.  32. 136. 400.   4.\n",
      " 112.  28. 106.  28.  74. 200. 192.  96.  58.  32. 140. 172.  72.  40.\n",
      " 200. 156. 162. 142. 134.  76. 255. 240.  58. 280. 124. 128.  28. 136.\n",
      " 275.  26.  16. 128. 134.  64. 225.  32. 186. 220.  52. 154. 184.  54.\n",
      " 114.  18. 220.  94. 130.  60. 295.  74.  60. 164.  10. 198. 138. 200.\n",
      " 128.  74. 350. 285. 102.   2.   4.  92.  86. 154. 190. 500. 186.  42.\n",
      "  62. 275. 110. 124. 116.  98. 275. 265.  28. 182. 198. 225. 170. 154.\n",
      "  86. 235. 300. 240.  84.  36. 150.  14. 300. 250. 164. 160.  78.  22.\n",
      " 300. 230. 112. 118. 118. 250. 178. 500.   0.  90. 120.  80. 186.  90.\n",
      " 450.  74.  64.  24. 140.  36. 215. 450. 350. 136.  68. 290. 220. 116.\n",
      " 300. 290.   8. 120.  48. 188. 164.  86.]\n",
      "mse =  2219.763282957576\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "time_str = str(time.time())\n",
    "print(time_str)\n",
    "reinitLayers(model)\n",
    "early_stop = EarlyStopping(monitor='val_mean_squared_error', patience=30)\n",
    "# model.load_weights('./modelWights/regression_model'+model_fn.__name__+'.h5')\n",
    "model_checkpoint = ModelCheckpoint('./modelWights/weights_'+'resnet50_without_pre'+time_str+'_color.h5', monitor='val_loss', save_best_only=True)\n",
    "tensorboard = keras.callbacks.tensorboard_v1.TensorBoard(log_dir=\"~/data/projects_logs/water_logs/resnet50_without_pre_color_{}\".format(time_str), histogram_freq=2,write_grads=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=100, validation_data=(X_val, y_val), callbacks=[model_checkpoint, ])\n",
    "# print(history.history)\n",
    "y_pred_ = model.predict(X_test, batch_size=1, verbose=1)\n",
    "y_pred_, y_test_ = post_training_data(y_test, y_pred_)\n",
    "predicted_file = 'regress_'+\"resnet50_without_pre\"+\"resnet50\"+time_str+'_color.csv'\n",
    "save_result(\"./result/\"+predicted_file,y_pred_,y_test_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.602876 -1.714657 -1.725154 -1.905815 ... -1.99617  -1.835018 -1.672766 -1.55472 ]\n",
      " [-1.626547 -1.703639 -1.874007 -2.015578 ... -2.038206 -1.992903 -1.842401 -1.556368]\n",
      " [-2.45059  -2.495802 -2.756412 -2.987875 ... -3.205384 -2.886243 -2.12324  -2.002175]\n",
      " [-2.581464 -2.636156 -3.228589 -3.844085 ... -4.178961 -3.674651 -2.766613 -2.335279]\n",
      " ...\n",
      " [-2.635763 -2.492539 -3.298693 -4.123997 ... -4.648807 -3.982989 -3.119166 -2.588822]\n",
      " [-2.355711 -2.239166 -3.107065 -3.694455 ... -4.223949 -3.626656 -2.724855 -2.437797]\n",
      " [-1.976302 -1.940042 -2.678933 -2.927193 ... -3.009618 -2.561657 -2.244801 -1.837124]\n",
      " [-1.748684 -1.95183  -2.336588 -2.763162 ... -3.112839 -2.657984 -2.096123 -1.754172]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_3rd_layer_output = K.function([model.layers[0].input, K.learning_phase()],\n",
    "                                  [model.layers[-3].output])\n",
    "# output in test mode = 0\n",
    "layer_output = get_3rd_layer_output([X_train[2].reshape(1,512,512,3), 0])[0]\n",
    "imshow(layer_output[0,:,:,0])\n",
    "print(layer_output[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pretrained with imagenet weight tend to have negative activation??\n",
    "- network architacture: ResNet50 have larger absolute activation (e.g. 326) while mobileNet have lower value (e.g. 1e-20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## resnet 50 color without pretrain--results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330/330 [==============================] - 60s 182ms/step\n",
      "(330,)\n",
      "(330,)\n",
      "[  28.603256    71.07702    274.13205    235.63416    444.87213\n",
      "  121.185776   223.74332     55.099144   151.6922     162.6203\n",
      "  237.1217      49.00352    151.48047     20.321041   109.658264\n",
      "   49.619377   154.97488    134.90668    248.74939    198.93352\n",
      "  505.0509     239.85258    284.63034     74.45407     69.19576\n",
      "   69.405495   130.32161     26.75192    136.03673    149.9048\n",
      "  154.9479     125.91618    111.04576     38.923145   115.37618\n",
      "  149.87564    286.33975     31.314335    74.96099    428.89722\n",
      "  250.07385    278.27795    172.63026    247.9583     284.84143\n",
      "  314.19244     39.688393    22.28883    101.418594   140.69962\n",
      "    8.597553    94.09465    152.2221      25.92963    191.65405\n",
      "  173.46999    108.332115   246.38612    229.86555      2.8510242\n",
      "  231.05772    316.58746     43.660717   451.05524    223.35466\n",
      "  270.56265    225.81323    215.14874    263.69553    104.63653\n",
      "  235.94928     54.252686   190.36946    163.11998     66.14015\n",
      "  202.39096    151.91853    137.7165     123.39221     15.393316\n",
      "  128.87839    362.3257      56.845665    60.81222     53.17995\n",
      "   55.411354    55.83682     92.50316     95.78079     89.017914\n",
      "  218.06999     26.370481   180.71106    166.52011    226.22984\n",
      "  223.9408     104.37238    202.39096     67.05632     65.58372\n",
      "  182.82724     94.50992     86.448944    21.762505    18.670261\n",
      "   43.936127   167.16226    186.88995     57.88771     25.355816\n",
      "   23.017794   163.39954    148.00435     55.72851    104.89634\n",
      "  388.652       32.199265   288.36472     33.994606   272.97366\n",
      "  233.8669     152.66537    183.49335    241.08351    233.40051\n",
      "  163.97382    250.66763     57.8008      23.29497    120.55248\n",
      "  247.43826    304.4013     448.08377     53.226486    91.82785\n",
      "  232.53563     75.991005   115.703       13.274282   479.0145\n",
      "   31.14988     71.22266     26.857346   489.3194      25.946453\n",
      "  107.412445   105.907684    85.20969    147.7095     191.1077\n",
      "  287.66727     90.73392    113.29831     28.691635   493.55597\n",
      "   20.546616   152.5328      54.372208    26.439308   146.52928\n",
      "   72.32843   2067.43       108.06395    124.359055    51.63835\n",
      "   37.987225   243.89323    124.77912    231.6956      43.178333\n",
      "   28.087734   121.21997     18.437922   147.03442     92.83185\n",
      "  201.55684    295.66873    109.54043    244.4998      82.008705\n",
      "  459.1539     162.3712     115.15271    124.59966     83.33702\n",
      "  141.22673    169.56311     55.08422    132.6088      40.87603\n",
      "  173.99925     97.63991     22.71399    142.38524    409.671\n",
      "   13.458133   115.37365     85.272255   120.15626     44.37682\n",
      "   89.922455   178.30692    193.56018    106.80439     59.191406\n",
      "   53.626076   133.98734    173.73839     73.92793     66.50671\n",
      "  204.65756    150.79765    158.47821    132.48076    127.3565\n",
      "   89.57566    250.46439    268.0511      60.53433    285.13657\n",
      "  132.0194     150.30678     17.756521   131.82634    299.36392\n",
      "   27.917593    24.630428   122.58998    136.68172     66.37632\n",
      "  221.29233     37.708057   189.13585    209.2315      53.07475\n",
      "  130.42232    183.9745      65.01279    119.2778      17.302021\n",
      "  198.8748      91.85536    133.0171      68.80767    300.98413\n",
      "   81.45136     62.92917    139.49916     22.546858   172.78693\n",
      "  156.2802     182.27113    127.66205     77.99774    348.04385\n",
      "  299.32733    111.50222     63.233547    12.8348465   82.35704\n",
      "   79.99297    142.97285    160.69267    475.44214    206.62111\n",
      "   56.164623    78.95958    281.31165    105.278206   132.89297\n",
      "  112.969505   117.9215     254.31111    253.7032      17.054558\n",
      "  164.54791    190.63559    241.08351    171.32118    153.99844\n",
      "  106.41368    216.7791     256.77274    231.33727     79.22977\n",
      "   32.695004   131.4693      18.168837   293.322      244.56476\n",
      "  170.76122    143.82715     79.44168     19.155561   301.90753\n",
      "  236.31262    121.81826    112.93904    147.19853    248.30606\n",
      "  184.03107    494.75568     49.67212     96.18187    119.79883\n",
      "   87.221214   173.29053    101.79691    445.65497     72.92899\n",
      "   59.556553    30.865223   136.93066     32.83731    243.93332\n",
      "  454.62747    348.4858     145.34494     61.467396   254.75746\n",
      "  251.0843     121.17913    258.3737     283.5874      18.95885\n",
      "  147.4371      52.124924   180.78508    179.62614     87.387794 ]\n",
      "[  6.  54. 275. 240. 450. 108. 225.  58. 150. 162. 250.  80. 162.  64.\n",
      " 100.  46. 174. 128. 250. 184. 500. 300. 285.  68.  88.  30. 120.   8.\n",
      " 128. 154. 144. 126.  80.  52. 112. 158. 300.  18.  66. 450. 265. 290.\n",
      " 168. 255. 295. 300.  32.  24.  96. 132.   6.  92. 186.   8. 180. 196.\n",
      " 108. 230. 225.   8. 215. 290.  54. 450. 250. 270. 230. 200. 225.  80.\n",
      " 245.   4. 190. 168.  44. 225. 182. 122. 120.   4. 124. 350.  56.   2.\n",
      "  12.  84.  84.  78.  84.  82. 225.  18. 176. 174. 198. 225. 100. 225.\n",
      "  46.  60. 188.  88.  70.  10.  24.  48. 172. 186.  52.  10.   2. 188.\n",
      " 156.  48. 116. 400.  30. 275.  40. 275. 215. 156. 190. 225. 245. 172.\n",
      " 235. 102.   6. 122. 265. 295. 450.  46.  86. 196.  84. 112.  10. 500.\n",
      "  20.  58.  34. 500.  14.  82. 104.  92. 154. 188. 285. 100. 120.  34.\n",
      " 500.  16. 132.  46.  12. 136.  22.   1. 100.  90.  58.  38. 270. 112.\n",
      " 215.  32.  26. 124.  16. 144.  90. 182. 300. 134. 215. 102. 450. 128.\n",
      "  94. 138.  72. 146. 158.   0. 112.  40. 178.  98.  32. 136. 400.   4.\n",
      " 112.  28. 106.  28.  74. 200. 192.  96.  58.  32. 140. 172.  72.  40.\n",
      " 200. 156. 162. 142. 134.  76. 255. 240.  58. 280. 124. 128.  28. 136.\n",
      " 275.  26.  16. 128. 134.  64. 225.  32. 186. 220.  52. 154. 184.  54.\n",
      " 114.  18. 220.  94. 130.  60. 295.  74.  60. 164.  10. 198. 138. 200.\n",
      " 128.  74. 350. 285. 102.   2.   4.  92.  86. 154. 190. 500. 186.  42.\n",
      "  62. 275. 110. 124. 116.  98. 275. 265.  28. 182. 198. 225. 170. 154.\n",
      "  86. 235. 300. 240.  84.  36. 150.  14. 300. 250. 164. 160.  78.  22.\n",
      " 300. 230. 112. 118. 118. 250. 178. 500.   0.  90. 120.  80. 186.  90.\n",
      " 450.  74.  64.  24. 140.  36. 215. 450. 350. 136.  68. 290. 220. 116.\n",
      " 300. 290.   8. 120.  48. 188. 164.  86.]\n",
      "mse =  1037.0219513345353\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('./modelWights/weights_resnet50_without_pre1570593544.982001.h5')\n",
    "y_pred_ = model.predict(X_test, batch_size=1, verbose=1)\n",
    "\n",
    "time_str = str(time.time())\n",
    "y_pred_, y_test_ = post_training_data(y_test, y_pred_)\n",
    "predicted_file = 'regress_'+\"resnet50_without_pre\"+\"resnet50\"+time_str+'_pred_color.csv'\n",
    "save_result(\"./result/\"+predicted_file,y_pred_,y_test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## resnet 50 gray scale without pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 512, 512, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 518, 518, 3)  0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 256, 256, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 256, 256, 64) 256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 256, 256, 64) 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 258, 258, 64) 0           activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 128, 128, 64) 0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 128, 128, 64) 4160        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 128, 128, 64) 256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 128, 128, 64) 0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 128, 128, 64) 36928       activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 128, 128, 64) 256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 128, 128, 64) 0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 128, 128, 256 16640       activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 128, 128, 256 16640       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 128, 128, 256 1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 128, 128, 256 1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 128, 128, 256 0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 128, 128, 256 0           add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 128, 128, 64) 16448       activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 128, 128, 64) 256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 128, 128, 64) 0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 128, 128, 64) 36928       activation_105[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 128, 128, 64) 256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 128, 128, 64) 0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 128, 128, 256 16640       activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 128, 128, 256 1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 128, 128, 256 0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, 128, 128, 256 0           add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 128, 128, 64) 16448       activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 128, 128, 64) 256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 128, 128, 64) 0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 128, 128, 64) 36928       activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 128, 128, 64) 256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 128, 128, 64) 0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 128, 128, 256 16640       activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 128, 128, 256 1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 128, 128, 256 0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 128, 128, 256 0           add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 64, 64, 128)  32896       activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, 64, 64, 128)  0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, 64, 64, 128)  0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_112[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 64, 64, 512)  131584      activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 64, 64, 512)  2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 64, 64, 512)  0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, 64, 64, 512)  0           add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 64, 64, 128)  65664       activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, 64, 64, 128)  0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 64, 64, 128)  0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, 64, 64, 512)  0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 64, 64, 512)  0           add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 64, 64, 128)  65664       activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 64, 64, 128)  0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 64, 64, 128)  0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (None, 64, 64, 512)  0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 64, 64, 512)  0           add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 64, 64, 128)  65664       activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 64, 64, 128)  0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 64, 64, 128)  0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (None, 64, 64, 512)  0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 64, 64, 512)  0           add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 32, 32, 256)  131328      activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 32, 32, 256)  0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 32, 32, 256)  0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_124[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 32, 32, 1024) 525312      activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 32, 32, 1024) 4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (None, 32, 32, 1024) 0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 32, 32, 1024) 0           add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, 32, 32, 256)  0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, 32, 32, 256)  0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_41 (Add)                    (None, 32, 32, 1024) 0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, 32, 32, 1024) 0           add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, 32, 32, 256)  0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, 32, 32, 256)  0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_42 (Add)                    (None, 32, 32, 1024) 0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, 32, 32, 1024) 0           add_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 32, 32, 256)  0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, 32, 32, 256)  0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_133[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_43 (Add)                    (None, 32, 32, 1024) 0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 32, 32, 1024) 0           add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 32, 32, 256)  0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 32, 32, 256)  0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_44 (Add)                    (None, 32, 32, 1024) 0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 32, 32, 1024) 0           add_44[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, 32, 32, 256)  0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 32, 32, 256)  0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_45 (Add)                    (None, 32, 32, 1024) 0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, 32, 32, 1024) 0           add_45[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 16, 16, 512)  524800      activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 16, 16, 512)  2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, 16, 16, 512)  0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 16, 16, 512)  2359808     activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 16, 16, 512)  2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 16, 16, 512)  0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 16, 16, 2048) 1050624     activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 16, 16, 2048) 2099200     activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, 16, 16, 2048) 8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 16, 16, 2048) 8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_46 (Add)                    (None, 16, 16, 2048) 0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 16, 16, 2048) 0           add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 16, 16, 512)  1049088     activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 16, 16, 512)  2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 16, 16, 512)  0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 16, 16, 512)  2359808     activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 16, 16, 512)  2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, 16, 16, 512)  0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 16, 16, 2048) 1050624     activation_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, 16, 16, 2048) 8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_47 (Add)                    (None, 16, 16, 2048) 0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, 16, 16, 2048) 0           add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 16, 16, 512)  1049088     activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 16, 16, 512)  2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, 16, 16, 512)  0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 16, 16, 512)  2359808     activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 16, 16, 512)  2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, 16, 16, 512)  0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, 16, 16, 2048) 1050624     activation_148[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, 16, 16, 2048) 8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_48 (Add)                    (None, 16, 16, 2048) 0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, 16, 16, 2048) 0           add_48[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 1)            2049        avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 23,589,761\n",
      "Trainable params: 23,536,641\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n",
      "1570968328.1483557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER:: input_3\n",
      "LAYER:: conv1_pad\n",
      "LAYER:: conv1\n",
      "reinitializing layer conv1.kernel\n",
      "reinitializing layer conv1.bias\n",
      "LAYER:: bn_conv1\n",
      "reinitializing layer bn_conv1.gamma\n",
      "reinitializing layer bn_conv1.beta\n",
      "reinitializing layer bn_conv1.moving_mean\n",
      "reinitializing layer bn_conv1.moving_variance\n",
      "LAYER:: activation_101\n",
      "LAYER:: pool1_pad\n",
      "LAYER:: max_pooling2d_3\n",
      "LAYER:: res2a_branch2a\n",
      "reinitializing layer res2a_branch2a.kernel\n",
      "reinitializing layer res2a_branch2a.bias\n",
      "LAYER:: bn2a_branch2a\n",
      "reinitializing layer bn2a_branch2a.gamma\n",
      "reinitializing layer bn2a_branch2a.beta\n",
      "reinitializing layer bn2a_branch2a.moving_mean\n",
      "reinitializing layer bn2a_branch2a.moving_variance\n",
      "LAYER:: activation_102\n",
      "LAYER:: res2a_branch2b\n",
      "reinitializing layer res2a_branch2b.kernel\n",
      "reinitializing layer res2a_branch2b.bias\n",
      "LAYER:: bn2a_branch2b\n",
      "reinitializing layer bn2a_branch2b.gamma\n",
      "reinitializing layer bn2a_branch2b.beta\n",
      "reinitializing layer bn2a_branch2b.moving_mean\n",
      "reinitializing layer bn2a_branch2b.moving_variance\n",
      "LAYER:: activation_103\n",
      "LAYER:: res2a_branch2c\n",
      "reinitializing layer res2a_branch2c.kernel\n",
      "reinitializing layer res2a_branch2c.bias\n",
      "LAYER:: res2a_branch1\n",
      "reinitializing layer res2a_branch1.kernel\n",
      "reinitializing layer res2a_branch1.bias\n",
      "LAYER:: bn2a_branch2c\n",
      "reinitializing layer bn2a_branch2c.gamma\n",
      "reinitializing layer bn2a_branch2c.beta\n",
      "reinitializing layer bn2a_branch2c.moving_mean\n",
      "reinitializing layer bn2a_branch2c.moving_variance\n",
      "LAYER:: bn2a_branch1\n",
      "reinitializing layer bn2a_branch1.gamma\n",
      "reinitializing layer bn2a_branch1.beta\n",
      "reinitializing layer bn2a_branch1.moving_mean\n",
      "reinitializing layer bn2a_branch1.moving_variance\n",
      "LAYER:: add_33\n",
      "LAYER:: activation_104\n",
      "LAYER:: res2b_branch2a\n",
      "reinitializing layer res2b_branch2a.kernel\n",
      "reinitializing layer res2b_branch2a.bias\n",
      "LAYER:: bn2b_branch2a\n",
      "reinitializing layer bn2b_branch2a.gamma\n",
      "reinitializing layer bn2b_branch2a.beta\n",
      "reinitializing layer bn2b_branch2a.moving_mean\n",
      "reinitializing layer bn2b_branch2a.moving_variance\n",
      "LAYER:: activation_105\n",
      "LAYER:: res2b_branch2b\n",
      "reinitializing layer res2b_branch2b.kernel\n",
      "reinitializing layer res2b_branch2b.bias\n",
      "LAYER:: bn2b_branch2b\n",
      "reinitializing layer bn2b_branch2b.gamma\n",
      "reinitializing layer bn2b_branch2b.beta\n",
      "reinitializing layer bn2b_branch2b.moving_mean\n",
      "reinitializing layer bn2b_branch2b.moving_variance\n",
      "LAYER:: activation_106\n",
      "LAYER:: res2b_branch2c\n",
      "reinitializing layer res2b_branch2c.kernel\n",
      "reinitializing layer res2b_branch2c.bias\n",
      "LAYER:: bn2b_branch2c\n",
      "reinitializing layer bn2b_branch2c.gamma\n",
      "reinitializing layer bn2b_branch2c.beta\n",
      "reinitializing layer bn2b_branch2c.moving_mean\n",
      "reinitializing layer bn2b_branch2c.moving_variance\n",
      "LAYER:: add_34\n",
      "LAYER:: activation_107\n",
      "LAYER:: res2c_branch2a\n",
      "reinitializing layer res2c_branch2a.kernel\n",
      "reinitializing layer res2c_branch2a.bias\n",
      "LAYER:: bn2c_branch2a\n",
      "reinitializing layer bn2c_branch2a.gamma\n",
      "reinitializing layer bn2c_branch2a.beta\n",
      "reinitializing layer bn2c_branch2a.moving_mean\n",
      "reinitializing layer bn2c_branch2a.moving_variance\n",
      "LAYER:: activation_108\n",
      "LAYER:: res2c_branch2b\n",
      "reinitializing layer res2c_branch2b.kernel\n",
      "reinitializing layer res2c_branch2b.bias\n",
      "LAYER:: bn2c_branch2b\n",
      "reinitializing layer bn2c_branch2b.gamma\n",
      "reinitializing layer bn2c_branch2b.beta\n",
      "reinitializing layer bn2c_branch2b.moving_mean\n",
      "reinitializing layer bn2c_branch2b.moving_variance\n",
      "LAYER:: activation_109\n",
      "LAYER:: res2c_branch2c\n",
      "reinitializing layer res2c_branch2c.kernel\n",
      "reinitializing layer res2c_branch2c.bias\n",
      "LAYER:: bn2c_branch2c\n",
      "reinitializing layer bn2c_branch2c.gamma\n",
      "reinitializing layer bn2c_branch2c.beta\n",
      "reinitializing layer bn2c_branch2c.moving_mean\n",
      "reinitializing layer bn2c_branch2c.moving_variance\n",
      "LAYER:: add_35\n",
      "LAYER:: activation_110\n",
      "LAYER:: res3a_branch2a\n",
      "reinitializing layer res3a_branch2a.kernel\n",
      "reinitializing layer res3a_branch2a.bias\n",
      "LAYER:: bn3a_branch2a\n",
      "reinitializing layer bn3a_branch2a.gamma\n",
      "reinitializing layer bn3a_branch2a.beta\n",
      "reinitializing layer bn3a_branch2a.moving_mean\n",
      "reinitializing layer bn3a_branch2a.moving_variance\n",
      "LAYER:: activation_111\n",
      "LAYER:: res3a_branch2b\n",
      "reinitializing layer res3a_branch2b.kernel\n",
      "reinitializing layer res3a_branch2b.bias\n",
      "LAYER:: bn3a_branch2b\n",
      "reinitializing layer bn3a_branch2b.gamma\n",
      "reinitializing layer bn3a_branch2b.beta\n",
      "reinitializing layer bn3a_branch2b.moving_mean\n",
      "reinitializing layer bn3a_branch2b.moving_variance\n",
      "LAYER:: activation_112\n",
      "LAYER:: res3a_branch2c\n",
      "reinitializing layer res3a_branch2c.kernel\n",
      "reinitializing layer res3a_branch2c.bias\n",
      "LAYER:: res3a_branch1\n",
      "reinitializing layer res3a_branch1.kernel\n",
      "reinitializing layer res3a_branch1.bias\n",
      "LAYER:: bn3a_branch2c\n",
      "reinitializing layer bn3a_branch2c.gamma\n",
      "reinitializing layer bn3a_branch2c.beta\n",
      "reinitializing layer bn3a_branch2c.moving_mean\n",
      "reinitializing layer bn3a_branch2c.moving_variance\n",
      "LAYER:: bn3a_branch1\n",
      "reinitializing layer bn3a_branch1.gamma\n",
      "reinitializing layer bn3a_branch1.beta\n",
      "reinitializing layer bn3a_branch1.moving_mean\n",
      "reinitializing layer bn3a_branch1.moving_variance\n",
      "LAYER:: add_36\n",
      "LAYER:: activation_113\n",
      "LAYER:: res3b_branch2a\n",
      "reinitializing layer res3b_branch2a.kernel\n",
      "reinitializing layer res3b_branch2a.bias\n",
      "LAYER:: bn3b_branch2a\n",
      "reinitializing layer bn3b_branch2a.gamma\n",
      "reinitializing layer bn3b_branch2a.beta\n",
      "reinitializing layer bn3b_branch2a.moving_mean\n",
      "reinitializing layer bn3b_branch2a.moving_variance\n",
      "LAYER:: activation_114\n",
      "LAYER:: res3b_branch2b\n",
      "reinitializing layer res3b_branch2b.kernel\n",
      "reinitializing layer res3b_branch2b.bias\n",
      "LAYER:: bn3b_branch2b\n",
      "reinitializing layer bn3b_branch2b.gamma\n",
      "reinitializing layer bn3b_branch2b.beta\n",
      "reinitializing layer bn3b_branch2b.moving_mean\n",
      "reinitializing layer bn3b_branch2b.moving_variance\n",
      "LAYER:: activation_115\n",
      "LAYER:: res3b_branch2c\n",
      "reinitializing layer res3b_branch2c.kernel\n",
      "reinitializing layer res3b_branch2c.bias\n",
      "LAYER:: bn3b_branch2c\n",
      "reinitializing layer bn3b_branch2c.gamma\n",
      "reinitializing layer bn3b_branch2c.beta\n",
      "reinitializing layer bn3b_branch2c.moving_mean\n",
      "reinitializing layer bn3b_branch2c.moving_variance\n",
      "LAYER:: add_37\n",
      "LAYER:: activation_116\n",
      "LAYER:: res3c_branch2a\n",
      "reinitializing layer res3c_branch2a.kernel\n",
      "reinitializing layer res3c_branch2a.bias\n",
      "LAYER:: bn3c_branch2a\n",
      "reinitializing layer bn3c_branch2a.gamma\n",
      "reinitializing layer bn3c_branch2a.beta\n",
      "reinitializing layer bn3c_branch2a.moving_mean\n",
      "reinitializing layer bn3c_branch2a.moving_variance\n",
      "LAYER:: activation_117\n",
      "LAYER:: res3c_branch2b\n",
      "reinitializing layer res3c_branch2b.kernel\n",
      "reinitializing layer res3c_branch2b.bias\n",
      "LAYER:: bn3c_branch2b\n",
      "reinitializing layer bn3c_branch2b.gamma\n",
      "reinitializing layer bn3c_branch2b.beta\n",
      "reinitializing layer bn3c_branch2b.moving_mean\n",
      "reinitializing layer bn3c_branch2b.moving_variance\n",
      "LAYER:: activation_118\n",
      "LAYER:: res3c_branch2c\n",
      "reinitializing layer res3c_branch2c.kernel\n",
      "reinitializing layer res3c_branch2c.bias\n",
      "LAYER:: bn3c_branch2c\n",
      "reinitializing layer bn3c_branch2c.gamma\n",
      "reinitializing layer bn3c_branch2c.beta\n",
      "reinitializing layer bn3c_branch2c.moving_mean\n",
      "reinitializing layer bn3c_branch2c.moving_variance\n",
      "LAYER:: add_38\n",
      "LAYER:: activation_119\n",
      "LAYER:: res3d_branch2a\n",
      "reinitializing layer res3d_branch2a.kernel\n",
      "reinitializing layer res3d_branch2a.bias\n",
      "LAYER:: bn3d_branch2a\n",
      "reinitializing layer bn3d_branch2a.gamma\n",
      "reinitializing layer bn3d_branch2a.beta\n",
      "reinitializing layer bn3d_branch2a.moving_mean\n",
      "reinitializing layer bn3d_branch2a.moving_variance\n",
      "LAYER:: activation_120\n",
      "LAYER:: res3d_branch2b\n",
      "reinitializing layer res3d_branch2b.kernel\n",
      "reinitializing layer res3d_branch2b.bias\n",
      "LAYER:: bn3d_branch2b\n",
      "reinitializing layer bn3d_branch2b.gamma\n",
      "reinitializing layer bn3d_branch2b.beta\n",
      "reinitializing layer bn3d_branch2b.moving_mean\n",
      "reinitializing layer bn3d_branch2b.moving_variance\n",
      "LAYER:: activation_121\n",
      "LAYER:: res3d_branch2c\n",
      "reinitializing layer res3d_branch2c.kernel\n",
      "reinitializing layer res3d_branch2c.bias\n",
      "LAYER:: bn3d_branch2c\n",
      "reinitializing layer bn3d_branch2c.gamma\n",
      "reinitializing layer bn3d_branch2c.beta\n",
      "reinitializing layer bn3d_branch2c.moving_mean\n",
      "reinitializing layer bn3d_branch2c.moving_variance\n",
      "LAYER:: add_39\n",
      "LAYER:: activation_122\n",
      "LAYER:: res4a_branch2a\n",
      "reinitializing layer res4a_branch2a.kernel\n",
      "reinitializing layer res4a_branch2a.bias\n",
      "LAYER:: bn4a_branch2a\n",
      "reinitializing layer bn4a_branch2a.gamma\n",
      "reinitializing layer bn4a_branch2a.beta\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reinitializing layer bn4a_branch2a.moving_mean\n",
      "reinitializing layer bn4a_branch2a.moving_variance\n",
      "LAYER:: activation_123\n",
      "LAYER:: res4a_branch2b\n",
      "reinitializing layer res4a_branch2b.kernel\n",
      "reinitializing layer res4a_branch2b.bias\n",
      "LAYER:: bn4a_branch2b\n",
      "reinitializing layer bn4a_branch2b.gamma\n",
      "reinitializing layer bn4a_branch2b.beta\n",
      "reinitializing layer bn4a_branch2b.moving_mean\n",
      "reinitializing layer bn4a_branch2b.moving_variance\n",
      "LAYER:: activation_124\n",
      "LAYER:: res4a_branch2c\n",
      "reinitializing layer res4a_branch2c.kernel\n",
      "reinitializing layer res4a_branch2c.bias\n",
      "LAYER:: res4a_branch1\n",
      "reinitializing layer res4a_branch1.kernel\n",
      "reinitializing layer res4a_branch1.bias\n",
      "LAYER:: bn4a_branch2c\n",
      "reinitializing layer bn4a_branch2c.gamma\n",
      "reinitializing layer bn4a_branch2c.beta\n",
      "reinitializing layer bn4a_branch2c.moving_mean\n",
      "reinitializing layer bn4a_branch2c.moving_variance\n",
      "LAYER:: bn4a_branch1\n",
      "reinitializing layer bn4a_branch1.gamma\n",
      "reinitializing layer bn4a_branch1.beta\n",
      "reinitializing layer bn4a_branch1.moving_mean\n",
      "reinitializing layer bn4a_branch1.moving_variance\n",
      "LAYER:: add_40\n",
      "LAYER:: activation_125\n",
      "LAYER:: res4b_branch2a\n",
      "reinitializing layer res4b_branch2a.kernel\n",
      "reinitializing layer res4b_branch2a.bias\n",
      "LAYER:: bn4b_branch2a\n",
      "reinitializing layer bn4b_branch2a.gamma\n",
      "reinitializing layer bn4b_branch2a.beta\n",
      "reinitializing layer bn4b_branch2a.moving_mean\n",
      "reinitializing layer bn4b_branch2a.moving_variance\n",
      "LAYER:: activation_126\n",
      "LAYER:: res4b_branch2b\n",
      "reinitializing layer res4b_branch2b.kernel\n",
      "reinitializing layer res4b_branch2b.bias\n",
      "LAYER:: bn4b_branch2b\n",
      "reinitializing layer bn4b_branch2b.gamma\n",
      "reinitializing layer bn4b_branch2b.beta\n",
      "reinitializing layer bn4b_branch2b.moving_mean\n",
      "reinitializing layer bn4b_branch2b.moving_variance\n",
      "LAYER:: activation_127\n",
      "LAYER:: res4b_branch2c\n",
      "reinitializing layer res4b_branch2c.kernel\n",
      "reinitializing layer res4b_branch2c.bias\n",
      "LAYER:: bn4b_branch2c\n",
      "reinitializing layer bn4b_branch2c.gamma\n",
      "reinitializing layer bn4b_branch2c.beta\n",
      "reinitializing layer bn4b_branch2c.moving_mean\n",
      "reinitializing layer bn4b_branch2c.moving_variance\n",
      "LAYER:: add_41\n",
      "LAYER:: activation_128\n",
      "LAYER:: res4c_branch2a\n",
      "reinitializing layer res4c_branch2a.kernel\n",
      "reinitializing layer res4c_branch2a.bias\n",
      "LAYER:: bn4c_branch2a\n",
      "reinitializing layer bn4c_branch2a.gamma\n",
      "reinitializing layer bn4c_branch2a.beta\n",
      "reinitializing layer bn4c_branch2a.moving_mean\n",
      "reinitializing layer bn4c_branch2a.moving_variance\n",
      "LAYER:: activation_129\n",
      "LAYER:: res4c_branch2b\n",
      "reinitializing layer res4c_branch2b.kernel\n",
      "reinitializing layer res4c_branch2b.bias\n",
      "LAYER:: bn4c_branch2b\n",
      "reinitializing layer bn4c_branch2b.gamma\n",
      "reinitializing layer bn4c_branch2b.beta\n",
      "reinitializing layer bn4c_branch2b.moving_mean\n",
      "reinitializing layer bn4c_branch2b.moving_variance\n",
      "LAYER:: activation_130\n",
      "LAYER:: res4c_branch2c\n",
      "reinitializing layer res4c_branch2c.kernel\n",
      "reinitializing layer res4c_branch2c.bias\n",
      "LAYER:: bn4c_branch2c\n",
      "reinitializing layer bn4c_branch2c.gamma\n",
      "reinitializing layer bn4c_branch2c.beta\n",
      "reinitializing layer bn4c_branch2c.moving_mean\n",
      "reinitializing layer bn4c_branch2c.moving_variance\n",
      "LAYER:: add_42\n",
      "LAYER:: activation_131\n",
      "LAYER:: res4d_branch2a\n",
      "reinitializing layer res4d_branch2a.kernel\n",
      "reinitializing layer res4d_branch2a.bias\n",
      "LAYER:: bn4d_branch2a\n",
      "reinitializing layer bn4d_branch2a.gamma\n",
      "reinitializing layer bn4d_branch2a.beta\n",
      "reinitializing layer bn4d_branch2a.moving_mean\n",
      "reinitializing layer bn4d_branch2a.moving_variance\n",
      "LAYER:: activation_132\n",
      "LAYER:: res4d_branch2b\n",
      "reinitializing layer res4d_branch2b.kernel\n",
      "reinitializing layer res4d_branch2b.bias\n",
      "LAYER:: bn4d_branch2b\n",
      "reinitializing layer bn4d_branch2b.gamma\n",
      "reinitializing layer bn4d_branch2b.beta\n",
      "reinitializing layer bn4d_branch2b.moving_mean\n",
      "reinitializing layer bn4d_branch2b.moving_variance\n",
      "LAYER:: activation_133\n",
      "LAYER:: res4d_branch2c\n",
      "reinitializing layer res4d_branch2c.kernel\n",
      "reinitializing layer res4d_branch2c.bias\n",
      "LAYER:: bn4d_branch2c\n",
      "reinitializing layer bn4d_branch2c.gamma\n",
      "reinitializing layer bn4d_branch2c.beta\n",
      "reinitializing layer bn4d_branch2c.moving_mean\n",
      "reinitializing layer bn4d_branch2c.moving_variance\n",
      "LAYER:: add_43\n",
      "LAYER:: activation_134\n",
      "LAYER:: res4e_branch2a\n",
      "reinitializing layer res4e_branch2a.kernel\n",
      "reinitializing layer res4e_branch2a.bias\n",
      "LAYER:: bn4e_branch2a\n",
      "reinitializing layer bn4e_branch2a.gamma\n",
      "reinitializing layer bn4e_branch2a.beta\n",
      "reinitializing layer bn4e_branch2a.moving_mean\n",
      "reinitializing layer bn4e_branch2a.moving_variance\n",
      "LAYER:: activation_135\n",
      "LAYER:: res4e_branch2b\n",
      "reinitializing layer res4e_branch2b.kernel\n",
      "reinitializing layer res4e_branch2b.bias\n",
      "LAYER:: bn4e_branch2b\n",
      "reinitializing layer bn4e_branch2b.gamma\n",
      "reinitializing layer bn4e_branch2b.beta\n",
      "reinitializing layer bn4e_branch2b.moving_mean\n",
      "reinitializing layer bn4e_branch2b.moving_variance\n",
      "LAYER:: activation_136\n",
      "LAYER:: res4e_branch2c\n",
      "reinitializing layer res4e_branch2c.kernel\n",
      "reinitializing layer res4e_branch2c.bias\n",
      "LAYER:: bn4e_branch2c\n",
      "reinitializing layer bn4e_branch2c.gamma\n",
      "reinitializing layer bn4e_branch2c.beta\n",
      "reinitializing layer bn4e_branch2c.moving_mean\n",
      "reinitializing layer bn4e_branch2c.moving_variance\n",
      "LAYER:: add_44\n",
      "LAYER:: activation_137\n",
      "LAYER:: res4f_branch2a\n",
      "reinitializing layer res4f_branch2a.kernel\n",
      "reinitializing layer res4f_branch2a.bias\n",
      "LAYER:: bn4f_branch2a\n",
      "reinitializing layer bn4f_branch2a.gamma\n",
      "reinitializing layer bn4f_branch2a.beta\n",
      "reinitializing layer bn4f_branch2a.moving_mean\n",
      "reinitializing layer bn4f_branch2a.moving_variance\n",
      "LAYER:: activation_138\n",
      "LAYER:: res4f_branch2b\n",
      "reinitializing layer res4f_branch2b.kernel\n",
      "reinitializing layer res4f_branch2b.bias\n",
      "LAYER:: bn4f_branch2b\n",
      "reinitializing layer bn4f_branch2b.gamma\n",
      "reinitializing layer bn4f_branch2b.beta\n",
      "reinitializing layer bn4f_branch2b.moving_mean\n",
      "reinitializing layer bn4f_branch2b.moving_variance\n",
      "LAYER:: activation_139\n",
      "LAYER:: res4f_branch2c\n",
      "reinitializing layer res4f_branch2c.kernel\n",
      "reinitializing layer res4f_branch2c.bias\n",
      "LAYER:: bn4f_branch2c\n",
      "reinitializing layer bn4f_branch2c.gamma\n",
      "reinitializing layer bn4f_branch2c.beta\n",
      "reinitializing layer bn4f_branch2c.moving_mean\n",
      "reinitializing layer bn4f_branch2c.moving_variance\n",
      "LAYER:: add_45\n",
      "LAYER:: activation_140\n",
      "LAYER:: res5a_branch2a\n",
      "reinitializing layer res5a_branch2a.kernel\n",
      "reinitializing layer res5a_branch2a.bias\n",
      "LAYER:: bn5a_branch2a\n",
      "reinitializing layer bn5a_branch2a.gamma\n",
      "reinitializing layer bn5a_branch2a.beta\n",
      "reinitializing layer bn5a_branch2a.moving_mean\n",
      "reinitializing layer bn5a_branch2a.moving_variance\n",
      "LAYER:: activation_141\n",
      "LAYER:: res5a_branch2b\n",
      "reinitializing layer res5a_branch2b.kernel\n",
      "reinitializing layer res5a_branch2b.bias\n",
      "LAYER:: bn5a_branch2b\n",
      "reinitializing layer bn5a_branch2b.gamma\n",
      "reinitializing layer bn5a_branch2b.beta\n",
      "reinitializing layer bn5a_branch2b.moving_mean\n",
      "reinitializing layer bn5a_branch2b.moving_variance\n",
      "LAYER:: activation_142\n",
      "LAYER:: res5a_branch2c\n",
      "reinitializing layer res5a_branch2c.kernel\n",
      "reinitializing layer res5a_branch2c.bias\n",
      "LAYER:: res5a_branch1\n",
      "reinitializing layer res5a_branch1.kernel\n",
      "reinitializing layer res5a_branch1.bias\n",
      "LAYER:: bn5a_branch2c\n",
      "reinitializing layer bn5a_branch2c.gamma\n",
      "reinitializing layer bn5a_branch2c.beta\n",
      "reinitializing layer bn5a_branch2c.moving_mean\n",
      "reinitializing layer bn5a_branch2c.moving_variance\n",
      "LAYER:: bn5a_branch1\n",
      "reinitializing layer bn5a_branch1.gamma\n",
      "reinitializing layer bn5a_branch1.beta\n",
      "reinitializing layer bn5a_branch1.moving_mean\n",
      "reinitializing layer bn5a_branch1.moving_variance\n",
      "LAYER:: add_46\n",
      "LAYER:: activation_143\n",
      "LAYER:: res5b_branch2a\n",
      "reinitializing layer res5b_branch2a.kernel\n",
      "reinitializing layer res5b_branch2a.bias\n",
      "LAYER:: bn5b_branch2a\n",
      "reinitializing layer bn5b_branch2a.gamma\n",
      "reinitializing layer bn5b_branch2a.beta\n",
      "reinitializing layer bn5b_branch2a.moving_mean\n",
      "reinitializing layer bn5b_branch2a.moving_variance\n",
      "LAYER:: activation_144\n",
      "LAYER:: res5b_branch2b\n",
      "reinitializing layer res5b_branch2b.kernel\n",
      "reinitializing layer res5b_branch2b.bias\n",
      "LAYER:: bn5b_branch2b\n",
      "reinitializing layer bn5b_branch2b.gamma\n",
      "reinitializing layer bn5b_branch2b.beta\n",
      "reinitializing layer bn5b_branch2b.moving_mean\n",
      "reinitializing layer bn5b_branch2b.moving_variance\n",
      "LAYER:: activation_145\n",
      "LAYER:: res5b_branch2c\n",
      "reinitializing layer res5b_branch2c.kernel\n",
      "reinitializing layer res5b_branch2c.bias\n",
      "LAYER:: bn5b_branch2c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reinitializing layer bn5b_branch2c.gamma\n",
      "reinitializing layer bn5b_branch2c.beta\n",
      "reinitializing layer bn5b_branch2c.moving_mean\n",
      "reinitializing layer bn5b_branch2c.moving_variance\n",
      "LAYER:: add_47\n",
      "LAYER:: activation_146\n",
      "LAYER:: res5c_branch2a\n",
      "reinitializing layer res5c_branch2a.kernel\n",
      "reinitializing layer res5c_branch2a.bias\n",
      "LAYER:: bn5c_branch2a\n",
      "reinitializing layer bn5c_branch2a.gamma\n",
      "reinitializing layer bn5c_branch2a.beta\n",
      "reinitializing layer bn5c_branch2a.moving_mean\n",
      "reinitializing layer bn5c_branch2a.moving_variance\n",
      "LAYER:: activation_147\n",
      "LAYER:: res5c_branch2b\n",
      "reinitializing layer res5c_branch2b.kernel\n",
      "reinitializing layer res5c_branch2b.bias\n",
      "LAYER:: bn5c_branch2b\n",
      "reinitializing layer bn5c_branch2b.gamma\n",
      "reinitializing layer bn5c_branch2b.beta\n",
      "reinitializing layer bn5c_branch2b.moving_mean\n",
      "reinitializing layer bn5c_branch2b.moving_variance\n",
      "LAYER:: activation_148\n",
      "LAYER:: res5c_branch2c\n",
      "reinitializing layer res5c_branch2c.kernel\n",
      "reinitializing layer res5c_branch2c.bias\n",
      "LAYER:: bn5c_branch2c\n",
      "reinitializing layer bn5c_branch2c.gamma\n",
      "reinitializing layer bn5c_branch2c.beta\n",
      "reinitializing layer bn5c_branch2c.moving_mean\n",
      "reinitializing layer bn5c_branch2c.moving_variance\n",
      "LAYER:: add_48\n",
      "LAYER:: activation_149\n",
      "LAYER:: avg_pool\n",
      "LAYER:: dense_3\n",
      "reinitializing layer dense_3.kernel\n",
      "reinitializing layer dense_3.bias\n",
      "LAYER:: activation_150\n",
      "Train on 1053 samples, validate on 264 samples\n",
      "Epoch 1/100\n",
      "1053/1053 [==============================] - 57s 54ms/step - loss: 0.6762 - mse: 0.6762 - val_loss: 0.0362 - val_mse: 0.0362\n",
      "Epoch 2/100\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0359 - mse: 0.0359 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 3/100\n",
      "1053/1053 [==============================] - 49s 47ms/step - loss: 0.0263 - mse: 0.0263 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 4/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0188 - mse: 0.0188 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 5/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0200 - mse: 0.0200 - val_loss: 0.2748 - val_mse: 0.2748\n",
      "Epoch 6/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0192 - mse: 0.0192 - val_loss: 0.0758 - val_mse: 0.0758\n",
      "Epoch 7/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0994 - val_mse: 0.0994\n",
      "Epoch 8/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0284 - mse: 0.0284 - val_loss: 15.3982 - val_mse: 15.3982\n",
      "Epoch 9/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0257 - mse: 0.0257 - val_loss: 0.0127 - val_mse: 0.0127\n",
      "Epoch 10/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.1575 - val_mse: 0.1575\n",
      "Epoch 11/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0275 - val_mse: 0.0275\n",
      "Epoch 12/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0460 - val_mse: 0.0460\n",
      "Epoch 13/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0497 - val_mse: 0.0497\n",
      "Epoch 14/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0118 - val_mse: 0.0118\n",
      "Epoch 15/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0145 - val_mse: 0.0145\n",
      "Epoch 16/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0292 - val_mse: 0.0292\n",
      "Epoch 17/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0285 - val_mse: 0.0285\n",
      "Epoch 18/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0080 - mse: 0.0080 - val_loss: 0.0116 - val_mse: 0.0116\n",
      "Epoch 19/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0247 - val_mse: 0.0247\n",
      "Epoch 20/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 21/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 22/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0130 - val_mse: 0.0130\n",
      "Epoch 23/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 24/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0079 - mse: 0.0079 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 25/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 26/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0733 - mse: 0.0733 - val_loss: 1.7596 - val_mse: 1.7596\n",
      "Epoch 27/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0495 - mse: 0.0495 - val_loss: 0.0725 - val_mse: 0.0725\n",
      "Epoch 28/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0412 - mse: 0.0412 - val_loss: 0.0436 - val_mse: 0.0436\n",
      "Epoch 29/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0255 - mse: 0.0255 - val_loss: 0.0241 - val_mse: 0.0241\n",
      "Epoch 30/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.0206 - val_mse: 0.0206\n",
      "Epoch 31/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0150 - val_mse: 0.0150\n",
      "Epoch 32/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0237 - val_mse: 0.0237\n",
      "Epoch 33/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 34/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0105 - val_mse: 0.0105\n",
      "Epoch 35/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 36/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0215 - val_mse: 0.0215\n",
      "Epoch 37/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0047 - val_mse: 0.0047\n",
      "Epoch 38/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 39/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0102 - val_mse: 0.0102\n",
      "Epoch 40/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 41/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0068 - val_mse: 0.0068\n",
      "Epoch 42/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 43/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 44/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 45/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0235 - val_mse: 0.0235\n",
      "Epoch 46/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0054 - val_mse: 0.0054\n",
      "Epoch 47/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 48/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 49/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 50/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 51/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 52/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 53/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0056 - val_mse: 0.0056\n",
      "Epoch 54/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 55/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 56/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 57/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 58/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0068 - val_mse: 0.0068\n",
      "Epoch 59/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0237 - val_mse: 0.0237\n",
      "Epoch 60/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 61/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0069 - val_mse: 0.0069\n",
      "Epoch 62/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 63/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 64/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0048 - val_mse: 0.0048\n",
      "Epoch 65/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 66/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 67/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 68/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 69/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 70/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 71/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 72/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 73/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0054 - val_mse: 0.0054\n",
      "Epoch 74/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 75/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 76/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 77/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 78/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 79/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 80/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0054 - val_mse: 0.0054\n",
      "Epoch 81/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 82/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 83/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 84/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0048 - val_mse: 0.0048\n",
      "Epoch 85/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 86/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 87/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 88/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 89/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 90/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 91/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 92/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 93/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 94/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 95/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 96/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 97/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 98/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 99/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 100/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "330/330 [==============================] - 10s 30ms/step\n",
      "(330,)\n",
      "(330,)\n",
      "[ -7.446453   53.135277  223.67418   210.4035    471.18216   125.017914\n",
      " 219.98862    51.357075  145.97772   133.10277   257.6798     39.927803\n",
      " 151.63358    67.58877    97.1323     30.76788   125.259224  127.97031\n",
      " 240.29529   183.89453   518.62317   227.91382   273.39178    71.37605\n",
      "  64.14423    34.44144   116.54241   -49.127953  139.28792   145.57764\n",
      " 136.34354    99.62728    97.162994   59.63349   118.28564   150.5286\n",
      " 274.28696    10.263056   25.694363  401.45874   249.70007   254.33305\n",
      " 154.83868   204.36046   263.4952    277.48776    49.680546   35.337643\n",
      "  89.99295   125.08632   -35.06939    93.79156   145.77711     4.6622305\n",
      " 155.46451   162.64961   106.25636   227.63312   241.54704   -11.832192\n",
      " 192.71854   289.60388    38.65661   438.96674   218.57646   257.70474\n",
      " 230.43326   194.4259    233.61382    71.24262   220.43205   -67.1701\n",
      " 170.57219   153.45616    52.278175  191.26108   141.0865    145.3007\n",
      " 109.81713   -56.855694  103.38438   359.20245     8.3598795 -22.83661\n",
      "  -2.2487788  84.46641    91.8072     90.91032    75.75476    85.84335\n",
      " 219.89853    10.838673  165.64987   143.58508   242.78181   200.54326\n",
      "  96.78686   191.26108    45.869984   56.535572  174.83774    83.16059\n",
      "  71.59084     0.6566495  25.151037   43.275818  148.81512   174.0379\n",
      "  53.297916  -16.550377  -28.34931   165.52855   125.51239    44.236927\n",
      " 112.75992   365.32425    13.670921  260.2766     37.894398  244.37816\n",
      " 246.94502   144.27243   146.83601   225.37888   211.68045   137.73352\n",
      " 226.08907    63.10852    -7.0971847 115.27469   224.66594   291.3577\n",
      " 467.96436    33.597275   44.846012  213.90549    60.810684   87.936165\n",
      "  23.206203  522.7779    -47.310814   71.26423    11.723086  488.2623\n",
      " -19.271969   90.528725   95.59572    82.0991    108.66858   159.158\n",
      " 260.7938     52.21072    93.213615   29.532776  470.79147   -38.897797\n",
      " 143.32309   -29.78155    11.823505  130.17937    14.647469  274.57794\n",
      "  83.90553   100.27545    37.703632   45.195454  241.67952   117.88173\n",
      " 189.18909    52.521244   -1.3330281 107.340225   18.881008  125.123055\n",
      "  62.39941   172.67834   259.70792   141.00658   238.95682    72.78373\n",
      " 435.61047   132.3191    118.893265  118.2449     71.910065   96.69351\n",
      " 127.57118    38.01924   162.99855    22.940338  130.45512    85.41803\n",
      "   9.397552  113.2805    375.03827   -31.606913   96.347305   -3.9337277\n",
      "  98.2123      9.487867   98.29476   171.63295   147.07654   110.767746\n",
      "  43.60318    52.44609   134.68768   173.6265     69.27716    26.174173\n",
      " 200.8879    136.05193   133.71764   120.95865    97.91448    77.79978\n",
      " 213.28804   240.82281    57.74173   258.52728   105.221954  136.06302\n",
      "  33.47723   131.49052   298.75885    -7.354185  -38.470997  100.05807\n",
      " 126.56759    53.63999   209.06923    36.71787   178.72412   195.77472\n",
      "  32.624798  108.05825   154.74022    24.673521   99.482124  -23.083002\n",
      " 204.0505     73.32614   130.6567     57.356155  263.5832     62.22803\n",
      "  54.29236   143.18153   -29.699266   71.57407   135.56758   178.1294\n",
      " 117.86903    47.22409   312.3616    255.94417    96.96797   -11.498347\n",
      "  18.902756   88.93058    72.9848    120.586395  147.20671   514.50543\n",
      " 171.45126    42.801342   68.73374   267.78473   109.45839   113.12336\n",
      " 107.435715   92.311356  230.35074   224.60333    39.30861   144.0281\n",
      " 172.10751   225.37888   153.87978   120.23549   118.27166   203.78944\n",
      " 228.94528   210.70781    73.61716    20.937502  128.38321    -5.941853\n",
      " 266.87015   223.30022   140.88231   141.74243    34.126015    4.556507\n",
      " 272.67673   219.25327    91.369156  110.40241   147.40034   239.70792\n",
      " 183.6099    512.6343     16.429707   71.46926   117.109985   58.5216\n",
      " 171.16031   102.70469   440.57916    67.10926    48.665062    4.454404\n",
      " 119.48276    20.002209  217.25017   452.40854   334.35022   128.22244\n",
      "  72.09721   284.39856   231.43568   104.22946   262.19803   244.87752\n",
      "  -2.7653127 149.69405    42.40115   172.9231    146.4203     47.054558 ]\n",
      "[  6.  54. 275. 240. 450. 108. 225.  58. 150. 162. 250.  80. 162.  64.\n",
      " 100.  46. 174. 128. 250. 184. 500. 300. 285.  68.  88.  30. 120.   8.\n",
      " 128. 154. 144. 126.  80.  52. 112. 158. 300.  18.  66. 450. 265. 290.\n",
      " 168. 255. 295. 300.  32.  24.  96. 132.   6.  92. 186.   8. 180. 196.\n",
      " 108. 230. 225.   8. 215. 290.  54. 450. 250. 270. 230. 200. 225.  80.\n",
      " 245.   4. 190. 168.  44. 225. 182. 122. 120.   4. 124. 350.  56.   2.\n",
      "  12.  84.  84.  78.  84.  82. 225.  18. 176. 174. 198. 225. 100. 225.\n",
      "  46.  60. 188.  88.  70.  10.  24.  48. 172. 186.  52.  10.   2. 188.\n",
      " 156.  48. 116. 400.  30. 275.  40. 275. 215. 156. 190. 225. 245. 172.\n",
      " 235. 102.   6. 122. 265. 295. 450.  46.  86. 196.  84. 112.  10. 500.\n",
      "  20.  58.  34. 500.  14.  82. 104.  92. 154. 188. 285. 100. 120.  34.\n",
      " 500.  16. 132.  46.  12. 136.  22.   1. 100.  90.  58.  38. 270. 112.\n",
      " 215.  32.  26. 124.  16. 144.  90. 182. 300. 134. 215. 102. 450. 128.\n",
      "  94. 138.  72. 146. 158.   0. 112.  40. 178.  98.  32. 136. 400.   4.\n",
      " 112.  28. 106.  28.  74. 200. 192.  96.  58.  32. 140. 172.  72.  40.\n",
      " 200. 156. 162. 142. 134.  76. 255. 240.  58. 280. 124. 128.  28. 136.\n",
      " 275.  26.  16. 128. 134.  64. 225.  32. 186. 220.  52. 154. 184.  54.\n",
      " 114.  18. 220.  94. 130.  60. 295.  74.  60. 164.  10. 198. 138. 200.\n",
      " 128.  74. 350. 285. 102.   2.   4.  92.  86. 154. 190. 500. 186.  42.\n",
      "  62. 275. 110. 124. 116.  98. 275. 265.  28. 182. 198. 225. 170. 154.\n",
      "  86. 235. 300. 240.  84.  36. 150.  14. 300. 250. 164. 160.  78.  22.\n",
      " 300. 230. 112. 118. 118. 250. 178. 500.   0.  90. 120.  80. 186.  90.\n",
      " 450.  74.  64.  24. 140.  36. 215. 450. 350. 136.  68. 290. 220. 116.\n",
      " 300. 290.   8. 120.  48. 188. 164.  86.]\n",
      "mse =  760.0761754139123\n"
     ]
    }
   ],
   "source": [
    "pic = X_train[1]\n",
    "model = Sequential()\n",
    "#model.add(Input(shape=pic.shape))\n",
    "# model.add(Conv2D(filters=4, kernel_size=(7,7), strides=1, input_shape=pic.shape))\n",
    "Kerasmodel = keras.applications.resnet50.ResNet50(include_top=True\n",
    "                , weights=None#'imagenet'\n",
    "                # , input_tensor=inputs\n",
    "                , input_shape=pic.shape\n",
    "                )\n",
    "# Kerasmodel._layers.pop()\n",
    "# Kerasmodel.layers.pop()\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(1,kernel_initializer='he_normal'))\n",
    "model.add(Activation('linear')) #softmax\n",
    "model = Model( inputs=Kerasmodel.input , outputs=model(Kerasmodel.layers[-2].output))\n",
    "model.compile(loss='mean_squared_error', #mse binary_crossentropy Dice-coefficient loss function vs cross-entropy\n",
    "                optimizer=optimizers.Adam(lr=5e-3),\n",
    "                metrics=['mse'])\n",
    "\n",
    "model.summary()  \n",
    "time_str = str(time.time())\n",
    "print(time_str)\n",
    "reinitLayers(model)\n",
    "early_stop = EarlyStopping(monitor='val_mean_squared_error', patience=30)\n",
    "# model.load_weights('./modelWights/regression_model'+model_fn.__name__+'.h5')\n",
    "model_checkpoint = ModelCheckpoint('./modelWights/weights_'+'resnet50_without_pre'+time_str+'_gray_scale.h5', monitor='val_loss', save_best_only=True)\n",
    "tensorboard = keras.callbacks.tensorboard_v1.TensorBoard(log_dir=\"~/data/projects_logs/water_logs/resnet50_without_pre_color_{}\".format(time_str), histogram_freq=2,write_grads=True)\n",
    "\n",
    "history = model.fit(X_train_g, y_train, batch_size=BATCH_SIZE, epochs=100, validation_data=(X_val_g, y_val), callbacks=[model_checkpoint,])\n",
    "# print(history.history)\n",
    "\n",
    "y_pred_ = model.predict(X_test_g, batch_size=1, verbose=1)\n",
    "\n",
    "time_str = str(time.time())\n",
    "y_pred_, y_test_ = post_training_data(y_test, y_pred_)\n",
    "predicted_file = 'regress_'+\"resnet50_without_pre\"+\"resnet50\"+time_str+'_without_pretrain_gray_scale.csv'\n",
    "save_result(\"./result/\"+predicted_file,y_pred_,y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 512, 512, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 518, 518, 3)  0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 256, 256, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 256, 256, 64) 256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_151 (Activation)     (None, 256, 256, 64) 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 258, 258, 64) 0           activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 128, 128, 64) 0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 128, 128, 64) 4160        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 128, 128, 64) 256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_152 (Activation)     (None, 128, 128, 64) 0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 128, 128, 64) 36928       activation_152[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 128, 128, 64) 256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_153 (Activation)     (None, 128, 128, 64) 0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 128, 128, 256 16640       activation_153[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 128, 128, 256 16640       max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 128, 128, 256 1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 128, 128, 256 1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_49 (Add)                    (None, 128, 128, 256 0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_154 (Activation)     (None, 128, 128, 256 0           add_49[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 128, 128, 64) 16448       activation_154[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 128, 128, 64) 256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, 128, 128, 64) 0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 128, 128, 64) 36928       activation_155[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 128, 128, 64) 256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, 128, 128, 64) 0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 128, 128, 256 16640       activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 128, 128, 256 1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_50 (Add)                    (None, 128, 128, 256 0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_154[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, 128, 128, 256 0           add_50[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 128, 128, 64) 16448       activation_157[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 128, 128, 64) 256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, 128, 128, 64) 0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 128, 128, 64) 36928       activation_158[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 128, 128, 64) 256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, 128, 128, 64) 0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 128, 128, 256 16640       activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 128, 128, 256 1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_51 (Add)                    (None, 128, 128, 256 0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_157[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_160 (Activation)     (None, 128, 128, 256 0           add_51[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 64, 64, 128)  32896       activation_160[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_161 (Activation)     (None, 64, 64, 128)  0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_161[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_162 (Activation)     (None, 64, 64, 128)  0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_162[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 64, 64, 512)  131584      activation_160[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 64, 64, 512)  2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_52 (Add)                    (None, 64, 64, 512)  0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_163 (Activation)     (None, 64, 64, 512)  0           add_52[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 64, 64, 128)  65664       activation_163[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_164 (Activation)     (None, 64, 64, 128)  0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_164[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_165 (Activation)     (None, 64, 64, 128)  0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_165[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_53 (Add)                    (None, 64, 64, 512)  0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_163[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_166 (Activation)     (None, 64, 64, 512)  0           add_53[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 64, 64, 128)  65664       activation_166[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_167 (Activation)     (None, 64, 64, 128)  0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_167[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_168 (Activation)     (None, 64, 64, 128)  0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_54 (Add)                    (None, 64, 64, 512)  0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_166[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_169 (Activation)     (None, 64, 64, 512)  0           add_54[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 64, 64, 128)  65664       activation_169[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_170 (Activation)     (None, 64, 64, 128)  0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_170[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_171 (Activation)     (None, 64, 64, 128)  0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_171[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_55 (Add)                    (None, 64, 64, 512)  0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_169[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_172 (Activation)     (None, 64, 64, 512)  0           add_55[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 32, 32, 256)  131328      activation_172[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_173 (Activation)     (None, 32, 32, 256)  0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_173[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_174 (Activation)     (None, 32, 32, 256)  0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_174[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 32, 32, 1024) 525312      activation_172[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 32, 32, 1024) 4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_56 (Add)                    (None, 32, 32, 1024) 0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_175 (Activation)     (None, 32, 32, 1024) 0           add_56[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_175[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_176 (Activation)     (None, 32, 32, 256)  0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_176[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_177 (Activation)     (None, 32, 32, 256)  0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_177[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_57 (Add)                    (None, 32, 32, 1024) 0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_175[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_178 (Activation)     (None, 32, 32, 1024) 0           add_57[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_178[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_179 (Activation)     (None, 32, 32, 256)  0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_179[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_180 (Activation)     (None, 32, 32, 256)  0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_180[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_58 (Add)                    (None, 32, 32, 1024) 0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_178[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_181 (Activation)     (None, 32, 32, 1024) 0           add_58[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_182 (Activation)     (None, 32, 32, 256)  0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_182[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_183 (Activation)     (None, 32, 32, 256)  0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_183[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_59 (Add)                    (None, 32, 32, 1024) 0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_184 (Activation)     (None, 32, 32, 1024) 0           add_59[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_184[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_185 (Activation)     (None, 32, 32, 256)  0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_185[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_186 (Activation)     (None, 32, 32, 256)  0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_186[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_60 (Add)                    (None, 32, 32, 1024) 0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_184[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_187 (Activation)     (None, 32, 32, 1024) 0           add_60[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_187[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_188 (Activation)     (None, 32, 32, 256)  0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_188[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_189 (Activation)     (None, 32, 32, 256)  0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_189[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_61 (Add)                    (None, 32, 32, 1024) 0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_187[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_190 (Activation)     (None, 32, 32, 1024) 0           add_61[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 16, 16, 512)  524800      activation_190[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 16, 16, 512)  2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_191 (Activation)     (None, 16, 16, 512)  0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 16, 16, 512)  2359808     activation_191[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 16, 16, 512)  2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_192 (Activation)     (None, 16, 16, 512)  0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 16, 16, 2048) 1050624     activation_192[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 16, 16, 2048) 2099200     activation_190[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, 16, 16, 2048) 8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 16, 16, 2048) 8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_62 (Add)                    (None, 16, 16, 2048) 0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_193 (Activation)     (None, 16, 16, 2048) 0           add_62[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 16, 16, 512)  1049088     activation_193[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 16, 16, 512)  2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_194 (Activation)     (None, 16, 16, 512)  0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 16, 16, 512)  2359808     activation_194[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 16, 16, 512)  2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_195 (Activation)     (None, 16, 16, 512)  0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 16, 16, 2048) 1050624     activation_195[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, 16, 16, 2048) 8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_63 (Add)                    (None, 16, 16, 2048) 0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_193[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_196 (Activation)     (None, 16, 16, 2048) 0           add_63[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 16, 16, 512)  1049088     activation_196[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 16, 16, 512)  2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_197 (Activation)     (None, 16, 16, 512)  0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 16, 16, 512)  2359808     activation_197[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 16, 16, 512)  2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_198 (Activation)     (None, 16, 16, 512)  0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, 16, 16, 2048) 1050624     activation_198[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, 16, 16, 2048) 8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_64 (Add)                    (None, 16, 16, 2048) 0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_196[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_199 (Activation)     (None, 16, 16, 2048) 0           add_64[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           activation_199[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sequential_4 (Sequential)       (None, 1)            2049        avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 23,589,761\n",
      "Trainable params: 23,536,641\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n",
      "1570973776.0204873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER:: input_4\n",
      "LAYER:: conv1_pad\n",
      "LAYER:: conv1\n",
      "reinitializing layer conv1.kernel\n",
      "reinitializing layer conv1.bias\n",
      "LAYER:: bn_conv1\n",
      "reinitializing layer bn_conv1.gamma\n",
      "reinitializing layer bn_conv1.beta\n",
      "reinitializing layer bn_conv1.moving_mean\n",
      "reinitializing layer bn_conv1.moving_variance\n",
      "LAYER:: activation_151\n",
      "LAYER:: pool1_pad\n",
      "LAYER:: max_pooling2d_4\n",
      "LAYER:: res2a_branch2a\n",
      "reinitializing layer res2a_branch2a.kernel\n",
      "reinitializing layer res2a_branch2a.bias\n",
      "LAYER:: bn2a_branch2a\n",
      "reinitializing layer bn2a_branch2a.gamma\n",
      "reinitializing layer bn2a_branch2a.beta\n",
      "reinitializing layer bn2a_branch2a.moving_mean\n",
      "reinitializing layer bn2a_branch2a.moving_variance\n",
      "LAYER:: activation_152\n",
      "LAYER:: res2a_branch2b\n",
      "reinitializing layer res2a_branch2b.kernel\n",
      "reinitializing layer res2a_branch2b.bias\n",
      "LAYER:: bn2a_branch2b\n",
      "reinitializing layer bn2a_branch2b.gamma\n",
      "reinitializing layer bn2a_branch2b.beta\n",
      "reinitializing layer bn2a_branch2b.moving_mean\n",
      "reinitializing layer bn2a_branch2b.moving_variance\n",
      "LAYER:: activation_153\n",
      "LAYER:: res2a_branch2c\n",
      "reinitializing layer res2a_branch2c.kernel\n",
      "reinitializing layer res2a_branch2c.bias\n",
      "LAYER:: res2a_branch1\n",
      "reinitializing layer res2a_branch1.kernel\n",
      "reinitializing layer res2a_branch1.bias\n",
      "LAYER:: bn2a_branch2c\n",
      "reinitializing layer bn2a_branch2c.gamma\n",
      "reinitializing layer bn2a_branch2c.beta\n",
      "reinitializing layer bn2a_branch2c.moving_mean\n",
      "reinitializing layer bn2a_branch2c.moving_variance\n",
      "LAYER:: bn2a_branch1\n",
      "reinitializing layer bn2a_branch1.gamma\n",
      "reinitializing layer bn2a_branch1.beta\n",
      "reinitializing layer bn2a_branch1.moving_mean\n",
      "reinitializing layer bn2a_branch1.moving_variance\n",
      "LAYER:: add_49\n",
      "LAYER:: activation_154\n",
      "LAYER:: res2b_branch2a\n",
      "reinitializing layer res2b_branch2a.kernel\n",
      "reinitializing layer res2b_branch2a.bias\n",
      "LAYER:: bn2b_branch2a\n",
      "reinitializing layer bn2b_branch2a.gamma\n",
      "reinitializing layer bn2b_branch2a.beta\n",
      "reinitializing layer bn2b_branch2a.moving_mean\n",
      "reinitializing layer bn2b_branch2a.moving_variance\n",
      "LAYER:: activation_155\n",
      "LAYER:: res2b_branch2b\n",
      "reinitializing layer res2b_branch2b.kernel\n",
      "reinitializing layer res2b_branch2b.bias\n",
      "LAYER:: bn2b_branch2b\n",
      "reinitializing layer bn2b_branch2b.gamma\n",
      "reinitializing layer bn2b_branch2b.beta\n",
      "reinitializing layer bn2b_branch2b.moving_mean\n",
      "reinitializing layer bn2b_branch2b.moving_variance\n",
      "LAYER:: activation_156\n",
      "LAYER:: res2b_branch2c\n",
      "reinitializing layer res2b_branch2c.kernel\n",
      "reinitializing layer res2b_branch2c.bias\n",
      "LAYER:: bn2b_branch2c\n",
      "reinitializing layer bn2b_branch2c.gamma\n",
      "reinitializing layer bn2b_branch2c.beta\n",
      "reinitializing layer bn2b_branch2c.moving_mean\n",
      "reinitializing layer bn2b_branch2c.moving_variance\n",
      "LAYER:: add_50\n",
      "LAYER:: activation_157\n",
      "LAYER:: res2c_branch2a\n",
      "reinitializing layer res2c_branch2a.kernel\n",
      "reinitializing layer res2c_branch2a.bias\n",
      "LAYER:: bn2c_branch2a\n",
      "reinitializing layer bn2c_branch2a.gamma\n",
      "reinitializing layer bn2c_branch2a.beta\n",
      "reinitializing layer bn2c_branch2a.moving_mean\n",
      "reinitializing layer bn2c_branch2a.moving_variance\n",
      "LAYER:: activation_158\n",
      "LAYER:: res2c_branch2b\n",
      "reinitializing layer res2c_branch2b.kernel\n",
      "reinitializing layer res2c_branch2b.bias\n",
      "LAYER:: bn2c_branch2b\n",
      "reinitializing layer bn2c_branch2b.gamma\n",
      "reinitializing layer bn2c_branch2b.beta\n",
      "reinitializing layer bn2c_branch2b.moving_mean\n",
      "reinitializing layer bn2c_branch2b.moving_variance\n",
      "LAYER:: activation_159\n",
      "LAYER:: res2c_branch2c\n",
      "reinitializing layer res2c_branch2c.kernel\n",
      "reinitializing layer res2c_branch2c.bias\n",
      "LAYER:: bn2c_branch2c\n",
      "reinitializing layer bn2c_branch2c.gamma\n",
      "reinitializing layer bn2c_branch2c.beta\n",
      "reinitializing layer bn2c_branch2c.moving_mean\n",
      "reinitializing layer bn2c_branch2c.moving_variance\n",
      "LAYER:: add_51\n",
      "LAYER:: activation_160\n",
      "LAYER:: res3a_branch2a\n",
      "reinitializing layer res3a_branch2a.kernel\n",
      "reinitializing layer res3a_branch2a.bias\n",
      "LAYER:: bn3a_branch2a\n",
      "reinitializing layer bn3a_branch2a.gamma\n",
      "reinitializing layer bn3a_branch2a.beta\n",
      "reinitializing layer bn3a_branch2a.moving_mean\n",
      "reinitializing layer bn3a_branch2a.moving_variance\n",
      "LAYER:: activation_161\n",
      "LAYER:: res3a_branch2b\n",
      "reinitializing layer res3a_branch2b.kernel\n",
      "reinitializing layer res3a_branch2b.bias\n",
      "LAYER:: bn3a_branch2b\n",
      "reinitializing layer bn3a_branch2b.gamma\n",
      "reinitializing layer bn3a_branch2b.beta\n",
      "reinitializing layer bn3a_branch2b.moving_mean\n",
      "reinitializing layer bn3a_branch2b.moving_variance\n",
      "LAYER:: activation_162\n",
      "LAYER:: res3a_branch2c\n",
      "reinitializing layer res3a_branch2c.kernel\n",
      "reinitializing layer res3a_branch2c.bias\n",
      "LAYER:: res3a_branch1\n",
      "reinitializing layer res3a_branch1.kernel\n",
      "reinitializing layer res3a_branch1.bias\n",
      "LAYER:: bn3a_branch2c\n",
      "reinitializing layer bn3a_branch2c.gamma\n",
      "reinitializing layer bn3a_branch2c.beta\n",
      "reinitializing layer bn3a_branch2c.moving_mean\n",
      "reinitializing layer bn3a_branch2c.moving_variance\n",
      "LAYER:: bn3a_branch1\n",
      "reinitializing layer bn3a_branch1.gamma\n",
      "reinitializing layer bn3a_branch1.beta\n",
      "reinitializing layer bn3a_branch1.moving_mean\n",
      "reinitializing layer bn3a_branch1.moving_variance\n",
      "LAYER:: add_52\n",
      "LAYER:: activation_163\n",
      "LAYER:: res3b_branch2a\n",
      "reinitializing layer res3b_branch2a.kernel\n",
      "reinitializing layer res3b_branch2a.bias\n",
      "LAYER:: bn3b_branch2a\n",
      "reinitializing layer bn3b_branch2a.gamma\n",
      "reinitializing layer bn3b_branch2a.beta\n",
      "reinitializing layer bn3b_branch2a.moving_mean\n",
      "reinitializing layer bn3b_branch2a.moving_variance\n",
      "LAYER:: activation_164\n",
      "LAYER:: res3b_branch2b\n",
      "reinitializing layer res3b_branch2b.kernel\n",
      "reinitializing layer res3b_branch2b.bias\n",
      "LAYER:: bn3b_branch2b\n",
      "reinitializing layer bn3b_branch2b.gamma\n",
      "reinitializing layer bn3b_branch2b.beta\n",
      "reinitializing layer bn3b_branch2b.moving_mean\n",
      "reinitializing layer bn3b_branch2b.moving_variance\n",
      "LAYER:: activation_165\n",
      "LAYER:: res3b_branch2c\n",
      "reinitializing layer res3b_branch2c.kernel\n",
      "reinitializing layer res3b_branch2c.bias\n",
      "LAYER:: bn3b_branch2c\n",
      "reinitializing layer bn3b_branch2c.gamma\n",
      "reinitializing layer bn3b_branch2c.beta\n",
      "reinitializing layer bn3b_branch2c.moving_mean\n",
      "reinitializing layer bn3b_branch2c.moving_variance\n",
      "LAYER:: add_53\n",
      "LAYER:: activation_166\n",
      "LAYER:: res3c_branch2a\n",
      "reinitializing layer res3c_branch2a.kernel\n",
      "reinitializing layer res3c_branch2a.bias\n",
      "LAYER:: bn3c_branch2a\n",
      "reinitializing layer bn3c_branch2a.gamma\n",
      "reinitializing layer bn3c_branch2a.beta\n",
      "reinitializing layer bn3c_branch2a.moving_mean\n",
      "reinitializing layer bn3c_branch2a.moving_variance\n",
      "LAYER:: activation_167\n",
      "LAYER:: res3c_branch2b\n",
      "reinitializing layer res3c_branch2b.kernel\n",
      "reinitializing layer res3c_branch2b.bias\n",
      "LAYER:: bn3c_branch2b\n",
      "reinitializing layer bn3c_branch2b.gamma\n",
      "reinitializing layer bn3c_branch2b.beta\n",
      "reinitializing layer bn3c_branch2b.moving_mean\n",
      "reinitializing layer bn3c_branch2b.moving_variance\n",
      "LAYER:: activation_168\n",
      "LAYER:: res3c_branch2c\n",
      "reinitializing layer res3c_branch2c.kernel\n",
      "reinitializing layer res3c_branch2c.bias\n",
      "LAYER:: bn3c_branch2c\n",
      "reinitializing layer bn3c_branch2c.gamma\n",
      "reinitializing layer bn3c_branch2c.beta\n",
      "reinitializing layer bn3c_branch2c.moving_mean\n",
      "reinitializing layer bn3c_branch2c.moving_variance\n",
      "LAYER:: add_54\n",
      "LAYER:: activation_169\n",
      "LAYER:: res3d_branch2a\n",
      "reinitializing layer res3d_branch2a.kernel\n",
      "reinitializing layer res3d_branch2a.bias\n",
      "LAYER:: bn3d_branch2a\n",
      "reinitializing layer bn3d_branch2a.gamma\n",
      "reinitializing layer bn3d_branch2a.beta\n",
      "reinitializing layer bn3d_branch2a.moving_mean\n",
      "reinitializing layer bn3d_branch2a.moving_variance\n",
      "LAYER:: activation_170\n",
      "LAYER:: res3d_branch2b\n",
      "reinitializing layer res3d_branch2b.kernel\n",
      "reinitializing layer res3d_branch2b.bias\n",
      "LAYER:: bn3d_branch2b\n",
      "reinitializing layer bn3d_branch2b.gamma\n",
      "reinitializing layer bn3d_branch2b.beta\n",
      "reinitializing layer bn3d_branch2b.moving_mean\n",
      "reinitializing layer bn3d_branch2b.moving_variance\n",
      "LAYER:: activation_171\n",
      "LAYER:: res3d_branch2c\n",
      "reinitializing layer res3d_branch2c.kernel\n",
      "reinitializing layer res3d_branch2c.bias\n",
      "LAYER:: bn3d_branch2c\n",
      "reinitializing layer bn3d_branch2c.gamma\n",
      "reinitializing layer bn3d_branch2c.beta\n",
      "reinitializing layer bn3d_branch2c.moving_mean\n",
      "reinitializing layer bn3d_branch2c.moving_variance\n",
      "LAYER:: add_55\n",
      "LAYER:: activation_172\n",
      "LAYER:: res4a_branch2a\n",
      "reinitializing layer res4a_branch2a.kernel\n",
      "reinitializing layer res4a_branch2a.bias\n",
      "LAYER:: bn4a_branch2a\n",
      "reinitializing layer bn4a_branch2a.gamma\n",
      "reinitializing layer bn4a_branch2a.beta\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reinitializing layer bn4a_branch2a.moving_mean\n",
      "reinitializing layer bn4a_branch2a.moving_variance\n",
      "LAYER:: activation_173\n",
      "LAYER:: res4a_branch2b\n",
      "reinitializing layer res4a_branch2b.kernel\n",
      "reinitializing layer res4a_branch2b.bias\n",
      "LAYER:: bn4a_branch2b\n",
      "reinitializing layer bn4a_branch2b.gamma\n",
      "reinitializing layer bn4a_branch2b.beta\n",
      "reinitializing layer bn4a_branch2b.moving_mean\n",
      "reinitializing layer bn4a_branch2b.moving_variance\n",
      "LAYER:: activation_174\n",
      "LAYER:: res4a_branch2c\n",
      "reinitializing layer res4a_branch2c.kernel\n",
      "reinitializing layer res4a_branch2c.bias\n",
      "LAYER:: res4a_branch1\n",
      "reinitializing layer res4a_branch1.kernel\n",
      "reinitializing layer res4a_branch1.bias\n",
      "LAYER:: bn4a_branch2c\n",
      "reinitializing layer bn4a_branch2c.gamma\n",
      "reinitializing layer bn4a_branch2c.beta\n",
      "reinitializing layer bn4a_branch2c.moving_mean\n",
      "reinitializing layer bn4a_branch2c.moving_variance\n",
      "LAYER:: bn4a_branch1\n",
      "reinitializing layer bn4a_branch1.gamma\n",
      "reinitializing layer bn4a_branch1.beta\n",
      "reinitializing layer bn4a_branch1.moving_mean\n",
      "reinitializing layer bn4a_branch1.moving_variance\n",
      "LAYER:: add_56\n",
      "LAYER:: activation_175\n",
      "LAYER:: res4b_branch2a\n",
      "reinitializing layer res4b_branch2a.kernel\n",
      "reinitializing layer res4b_branch2a.bias\n",
      "LAYER:: bn4b_branch2a\n",
      "reinitializing layer bn4b_branch2a.gamma\n",
      "reinitializing layer bn4b_branch2a.beta\n",
      "reinitializing layer bn4b_branch2a.moving_mean\n",
      "reinitializing layer bn4b_branch2a.moving_variance\n",
      "LAYER:: activation_176\n",
      "LAYER:: res4b_branch2b\n",
      "reinitializing layer res4b_branch2b.kernel\n",
      "reinitializing layer res4b_branch2b.bias\n",
      "LAYER:: bn4b_branch2b\n",
      "reinitializing layer bn4b_branch2b.gamma\n",
      "reinitializing layer bn4b_branch2b.beta\n",
      "reinitializing layer bn4b_branch2b.moving_mean\n",
      "reinitializing layer bn4b_branch2b.moving_variance\n",
      "LAYER:: activation_177\n",
      "LAYER:: res4b_branch2c\n",
      "reinitializing layer res4b_branch2c.kernel\n",
      "reinitializing layer res4b_branch2c.bias\n",
      "LAYER:: bn4b_branch2c\n",
      "reinitializing layer bn4b_branch2c.gamma\n",
      "reinitializing layer bn4b_branch2c.beta\n",
      "reinitializing layer bn4b_branch2c.moving_mean\n",
      "reinitializing layer bn4b_branch2c.moving_variance\n",
      "LAYER:: add_57\n",
      "LAYER:: activation_178\n",
      "LAYER:: res4c_branch2a\n",
      "reinitializing layer res4c_branch2a.kernel\n",
      "reinitializing layer res4c_branch2a.bias\n",
      "LAYER:: bn4c_branch2a\n",
      "reinitializing layer bn4c_branch2a.gamma\n",
      "reinitializing layer bn4c_branch2a.beta\n",
      "reinitializing layer bn4c_branch2a.moving_mean\n",
      "reinitializing layer bn4c_branch2a.moving_variance\n",
      "LAYER:: activation_179\n",
      "LAYER:: res4c_branch2b\n",
      "reinitializing layer res4c_branch2b.kernel\n",
      "reinitializing layer res4c_branch2b.bias\n",
      "LAYER:: bn4c_branch2b\n",
      "reinitializing layer bn4c_branch2b.gamma\n",
      "reinitializing layer bn4c_branch2b.beta\n",
      "reinitializing layer bn4c_branch2b.moving_mean\n",
      "reinitializing layer bn4c_branch2b.moving_variance\n",
      "LAYER:: activation_180\n",
      "LAYER:: res4c_branch2c\n",
      "reinitializing layer res4c_branch2c.kernel\n",
      "reinitializing layer res4c_branch2c.bias\n",
      "LAYER:: bn4c_branch2c\n",
      "reinitializing layer bn4c_branch2c.gamma\n",
      "reinitializing layer bn4c_branch2c.beta\n",
      "reinitializing layer bn4c_branch2c.moving_mean\n",
      "reinitializing layer bn4c_branch2c.moving_variance\n",
      "LAYER:: add_58\n",
      "LAYER:: activation_181\n",
      "LAYER:: res4d_branch2a\n",
      "reinitializing layer res4d_branch2a.kernel\n",
      "reinitializing layer res4d_branch2a.bias\n",
      "LAYER:: bn4d_branch2a\n",
      "reinitializing layer bn4d_branch2a.gamma\n",
      "reinitializing layer bn4d_branch2a.beta\n",
      "reinitializing layer bn4d_branch2a.moving_mean\n",
      "reinitializing layer bn4d_branch2a.moving_variance\n",
      "LAYER:: activation_182\n",
      "LAYER:: res4d_branch2b\n",
      "reinitializing layer res4d_branch2b.kernel\n",
      "reinitializing layer res4d_branch2b.bias\n",
      "LAYER:: bn4d_branch2b\n",
      "reinitializing layer bn4d_branch2b.gamma\n",
      "reinitializing layer bn4d_branch2b.beta\n",
      "reinitializing layer bn4d_branch2b.moving_mean\n",
      "reinitializing layer bn4d_branch2b.moving_variance\n",
      "LAYER:: activation_183\n",
      "LAYER:: res4d_branch2c\n",
      "reinitializing layer res4d_branch2c.kernel\n",
      "reinitializing layer res4d_branch2c.bias\n",
      "LAYER:: bn4d_branch2c\n",
      "reinitializing layer bn4d_branch2c.gamma\n",
      "reinitializing layer bn4d_branch2c.beta\n",
      "reinitializing layer bn4d_branch2c.moving_mean\n",
      "reinitializing layer bn4d_branch2c.moving_variance\n",
      "LAYER:: add_59\n",
      "LAYER:: activation_184\n",
      "LAYER:: res4e_branch2a\n",
      "reinitializing layer res4e_branch2a.kernel\n",
      "reinitializing layer res4e_branch2a.bias\n",
      "LAYER:: bn4e_branch2a\n",
      "reinitializing layer bn4e_branch2a.gamma\n",
      "reinitializing layer bn4e_branch2a.beta\n",
      "reinitializing layer bn4e_branch2a.moving_mean\n",
      "reinitializing layer bn4e_branch2a.moving_variance\n",
      "LAYER:: activation_185\n",
      "LAYER:: res4e_branch2b\n",
      "reinitializing layer res4e_branch2b.kernel\n",
      "reinitializing layer res4e_branch2b.bias\n",
      "LAYER:: bn4e_branch2b\n",
      "reinitializing layer bn4e_branch2b.gamma\n",
      "reinitializing layer bn4e_branch2b.beta\n",
      "reinitializing layer bn4e_branch2b.moving_mean\n",
      "reinitializing layer bn4e_branch2b.moving_variance\n",
      "LAYER:: activation_186\n",
      "LAYER:: res4e_branch2c\n",
      "reinitializing layer res4e_branch2c.kernel\n",
      "reinitializing layer res4e_branch2c.bias\n",
      "LAYER:: bn4e_branch2c\n",
      "reinitializing layer bn4e_branch2c.gamma\n",
      "reinitializing layer bn4e_branch2c.beta\n",
      "reinitializing layer bn4e_branch2c.moving_mean\n",
      "reinitializing layer bn4e_branch2c.moving_variance\n",
      "LAYER:: add_60\n",
      "LAYER:: activation_187\n",
      "LAYER:: res4f_branch2a\n",
      "reinitializing layer res4f_branch2a.kernel\n",
      "reinitializing layer res4f_branch2a.bias\n",
      "LAYER:: bn4f_branch2a\n",
      "reinitializing layer bn4f_branch2a.gamma\n",
      "reinitializing layer bn4f_branch2a.beta\n",
      "reinitializing layer bn4f_branch2a.moving_mean\n",
      "reinitializing layer bn4f_branch2a.moving_variance\n",
      "LAYER:: activation_188\n",
      "LAYER:: res4f_branch2b\n",
      "reinitializing layer res4f_branch2b.kernel\n",
      "reinitializing layer res4f_branch2b.bias\n",
      "LAYER:: bn4f_branch2b\n",
      "reinitializing layer bn4f_branch2b.gamma\n",
      "reinitializing layer bn4f_branch2b.beta\n",
      "reinitializing layer bn4f_branch2b.moving_mean\n",
      "reinitializing layer bn4f_branch2b.moving_variance\n",
      "LAYER:: activation_189\n",
      "LAYER:: res4f_branch2c\n",
      "reinitializing layer res4f_branch2c.kernel\n",
      "reinitializing layer res4f_branch2c.bias\n",
      "LAYER:: bn4f_branch2c\n",
      "reinitializing layer bn4f_branch2c.gamma\n",
      "reinitializing layer bn4f_branch2c.beta\n",
      "reinitializing layer bn4f_branch2c.moving_mean\n",
      "reinitializing layer bn4f_branch2c.moving_variance\n",
      "LAYER:: add_61\n",
      "LAYER:: activation_190\n",
      "LAYER:: res5a_branch2a\n",
      "reinitializing layer res5a_branch2a.kernel\n",
      "reinitializing layer res5a_branch2a.bias\n",
      "LAYER:: bn5a_branch2a\n",
      "reinitializing layer bn5a_branch2a.gamma\n",
      "reinitializing layer bn5a_branch2a.beta\n",
      "reinitializing layer bn5a_branch2a.moving_mean\n",
      "reinitializing layer bn5a_branch2a.moving_variance\n",
      "LAYER:: activation_191\n",
      "LAYER:: res5a_branch2b\n",
      "reinitializing layer res5a_branch2b.kernel\n",
      "reinitializing layer res5a_branch2b.bias\n",
      "LAYER:: bn5a_branch2b\n",
      "reinitializing layer bn5a_branch2b.gamma\n",
      "reinitializing layer bn5a_branch2b.beta\n",
      "reinitializing layer bn5a_branch2b.moving_mean\n",
      "reinitializing layer bn5a_branch2b.moving_variance\n",
      "LAYER:: activation_192\n",
      "LAYER:: res5a_branch2c\n",
      "reinitializing layer res5a_branch2c.kernel\n",
      "reinitializing layer res5a_branch2c.bias\n",
      "LAYER:: res5a_branch1\n",
      "reinitializing layer res5a_branch1.kernel\n",
      "reinitializing layer res5a_branch1.bias\n",
      "LAYER:: bn5a_branch2c\n",
      "reinitializing layer bn5a_branch2c.gamma\n",
      "reinitializing layer bn5a_branch2c.beta\n",
      "reinitializing layer bn5a_branch2c.moving_mean\n",
      "reinitializing layer bn5a_branch2c.moving_variance\n",
      "LAYER:: bn5a_branch1\n",
      "reinitializing layer bn5a_branch1.gamma\n",
      "reinitializing layer bn5a_branch1.beta\n",
      "reinitializing layer bn5a_branch1.moving_mean\n",
      "reinitializing layer bn5a_branch1.moving_variance\n",
      "LAYER:: add_62\n",
      "LAYER:: activation_193\n",
      "LAYER:: res5b_branch2a\n",
      "reinitializing layer res5b_branch2a.kernel\n",
      "reinitializing layer res5b_branch2a.bias\n",
      "LAYER:: bn5b_branch2a\n",
      "reinitializing layer bn5b_branch2a.gamma\n",
      "reinitializing layer bn5b_branch2a.beta\n",
      "reinitializing layer bn5b_branch2a.moving_mean\n",
      "reinitializing layer bn5b_branch2a.moving_variance\n",
      "LAYER:: activation_194\n",
      "LAYER:: res5b_branch2b\n",
      "reinitializing layer res5b_branch2b.kernel\n",
      "reinitializing layer res5b_branch2b.bias\n",
      "LAYER:: bn5b_branch2b\n",
      "reinitializing layer bn5b_branch2b.gamma\n",
      "reinitializing layer bn5b_branch2b.beta\n",
      "reinitializing layer bn5b_branch2b.moving_mean\n",
      "reinitializing layer bn5b_branch2b.moving_variance\n",
      "LAYER:: activation_195\n",
      "LAYER:: res5b_branch2c\n",
      "reinitializing layer res5b_branch2c.kernel\n",
      "reinitializing layer res5b_branch2c.bias\n",
      "LAYER:: bn5b_branch2c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reinitializing layer bn5b_branch2c.gamma\n",
      "reinitializing layer bn5b_branch2c.beta\n",
      "reinitializing layer bn5b_branch2c.moving_mean\n",
      "reinitializing layer bn5b_branch2c.moving_variance\n",
      "LAYER:: add_63\n",
      "LAYER:: activation_196\n",
      "LAYER:: res5c_branch2a\n",
      "reinitializing layer res5c_branch2a.kernel\n",
      "reinitializing layer res5c_branch2a.bias\n",
      "LAYER:: bn5c_branch2a\n",
      "reinitializing layer bn5c_branch2a.gamma\n",
      "reinitializing layer bn5c_branch2a.beta\n",
      "reinitializing layer bn5c_branch2a.moving_mean\n",
      "reinitializing layer bn5c_branch2a.moving_variance\n",
      "LAYER:: activation_197\n",
      "LAYER:: res5c_branch2b\n",
      "reinitializing layer res5c_branch2b.kernel\n",
      "reinitializing layer res5c_branch2b.bias\n",
      "LAYER:: bn5c_branch2b\n",
      "reinitializing layer bn5c_branch2b.gamma\n",
      "reinitializing layer bn5c_branch2b.beta\n",
      "reinitializing layer bn5c_branch2b.moving_mean\n",
      "reinitializing layer bn5c_branch2b.moving_variance\n",
      "LAYER:: activation_198\n",
      "LAYER:: res5c_branch2c\n",
      "reinitializing layer res5c_branch2c.kernel\n",
      "reinitializing layer res5c_branch2c.bias\n",
      "LAYER:: bn5c_branch2c\n",
      "reinitializing layer bn5c_branch2c.gamma\n",
      "reinitializing layer bn5c_branch2c.beta\n",
      "reinitializing layer bn5c_branch2c.moving_mean\n",
      "reinitializing layer bn5c_branch2c.moving_variance\n",
      "LAYER:: add_64\n",
      "LAYER:: activation_199\n",
      "LAYER:: avg_pool\n",
      "LAYER:: dense_4\n",
      "reinitializing layer dense_4.kernel\n",
      "reinitializing layer dense_4.bias\n",
      "LAYER:: activation_200\n",
      "Train on 1053 samples, validate on 264 samples\n",
      "Epoch 1/100\n",
      "1053/1053 [==============================] - 58s 55ms/step - loss: 1.0501 - mse: 1.0501 - val_loss: 0.0791 - val_mse: 0.0791\n",
      "Epoch 2/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0265 - mse: 0.0265 - val_loss: 0.0272 - val_mse: 0.0272\n",
      "Epoch 3/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 4/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0213 - mse: 0.0213 - val_loss: 0.0187 - val_mse: 0.0187\n",
      "Epoch 5/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0189 - mse: 0.0189 - val_loss: 0.0189 - val_mse: 0.0189\n",
      "Epoch 6/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0190 - mse: 0.0190 - val_loss: 0.0299 - val_mse: 0.0299\n",
      "Epoch 7/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0286 - mse: 0.0286 - val_loss: 8613.1032 - val_mse: 8613.1025\n",
      "Epoch 8/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0260 - mse: 0.0260 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 9/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0212 - mse: 0.0212 - val_loss: 0.0236 - val_mse: 0.0236\n",
      "Epoch 10/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0160 - mse: 0.0160 - val_loss: 0.5094 - val_mse: 0.5094\n",
      "Epoch 11/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 12/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0580 - val_mse: 0.0580\n",
      "Epoch 13/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 14/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0289 - val_mse: 0.0289\n",
      "Epoch 15/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0370 - val_mse: 0.0370\n",
      "Epoch 16/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 17/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0163 - val_mse: 0.0163\n",
      "Epoch 18/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0178 - val_mse: 0.0178\n",
      "Epoch 19/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0079 - mse: 0.0079 - val_loss: 0.0069 - val_mse: 0.0069\n",
      "Epoch 20/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0150 - val_mse: 0.0150\n",
      "Epoch 21/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0338 - mse: 0.0338 - val_loss: 0.0312 - val_mse: 0.0312\n",
      "Epoch 22/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.0403 - val_mse: 0.0403\n",
      "Epoch 23/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 24/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0273 - val_mse: 0.0273\n",
      "Epoch 25/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0167 - val_mse: 0.0167\n",
      "Epoch 26/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0071 - val_mse: 0.0071\n",
      "Epoch 27/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0416 - val_mse: 0.0416\n",
      "Epoch 28/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 29/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 30/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0124 - val_mse: 0.0124\n",
      "Epoch 31/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0054 - val_mse: 0.0054\n",
      "Epoch 32/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 33/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0050 - val_mse: 0.0050\n",
      "Epoch 34/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0056 - val_mse: 0.0056\n",
      "Epoch 35/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0050 - val_mse: 0.0050\n",
      "Epoch 36/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 37/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 38/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 39/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0061 - mse: 0.0061 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 40/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0225 - val_mse: 0.0225\n",
      "Epoch 41/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0191 - val_mse: 0.0191\n",
      "Epoch 42/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 43/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0143 - val_mse: 0.0143\n",
      "Epoch 44/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0068 - val_mse: 0.0068\n",
      "Epoch 45/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 46/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 47/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0197 - val_mse: 0.0197\n",
      "Epoch 48/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 49/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0054 - val_mse: 0.0054\n",
      "Epoch 50/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 51/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0107 - val_mse: 0.0107\n",
      "Epoch 52/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0049 - val_mse: 0.0049\n",
      "Epoch 53/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0057 - val_mse: 0.0057\n",
      "Epoch 54/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 55/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 56/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 57/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0047 - val_mse: 0.0047\n",
      "Epoch 58/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 59/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 60/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 61/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 62/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 63/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 64/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 65/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 66/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 67/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 68/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 69/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 70/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0064 - val_mse: 0.0064\n",
      "Epoch 71/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 72/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 73/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 74/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 75/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 76/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 77/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0064 - val_mse: 0.0064\n",
      "Epoch 78/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 79/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 80/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 81/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 82/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 83/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 84/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 85/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 86/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 87/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 88/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 89/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 90/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 91/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 92/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 93/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 94/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 95/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 96/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 97/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 98/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 99/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 100/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "330/330 [==============================] - 11s 32ms/step\n",
      "(330,)\n",
      "(330,)\n",
      "[-6.91466522e+01  8.75552444e+01  2.68351227e+02  2.45064087e+02\n",
      "  4.60054565e+02  1.34228653e+02  2.40800140e+02  8.55017776e+01\n",
      "  1.71530457e+02  1.54464630e+02  2.36789078e+02  5.42985573e+01\n",
      "  1.46372406e+02  3.81278000e+01  1.11577415e+02  6.58740768e+01\n",
      "  1.33825745e+02  1.39092072e+02  2.66727875e+02  2.29268280e+02\n",
      "  4.78710815e+02  2.38822754e+02  2.86076141e+02  9.86079254e+01\n",
      "  1.06479103e+02 -2.28489933e+01  1.25889046e+02 -5.33468781e+01\n",
      "  1.70993652e+02  1.58514267e+02  1.72805511e+02  8.91098785e+01\n",
      "  1.20174965e+02  7.30587387e+01  1.26442566e+02  1.78421341e+02\n",
      "  3.15424103e+02  2.77271976e+01  5.83432999e+01  3.98259552e+02\n",
      "  2.75821777e+02  3.04163086e+02  1.64802109e+02  2.53422470e+02\n",
      "  3.12357819e+02  3.22963623e+02  6.90773926e+01  2.62349052e+01\n",
      "  1.00068642e+02  1.38968994e+02 -3.53983727e+01  8.92072830e+01\n",
      "  1.74078354e+02 -2.57955933e+01  2.01161667e+02  1.82684265e+02\n",
      "  1.32456406e+02  2.65390778e+02  2.40393692e+02  1.80679779e+01\n",
      "  1.98968094e+02  3.10695007e+02  6.04103050e+01  4.35045563e+02\n",
      "  2.37813416e+02  2.71512939e+02  2.54688202e+02  2.16270798e+02\n",
      "  2.66776733e+02  1.05097351e+02  2.38726074e+02 -5.29861526e+01\n",
      "  1.82664017e+02  1.82019623e+02  7.30115662e+01  1.63645477e+02\n",
      "  1.55263184e+02  1.27011299e+02  1.25891808e+02 -4.85101700e+01\n",
      "  1.27136681e+02  3.83273590e+02  4.38100100e+00 -5.63706665e+01\n",
      "  9.42162418e+00  8.13030243e+01  7.93621674e+01  8.95322495e+01\n",
      "  1.02340599e+02  8.98778915e+01  2.62770966e+02  5.05312500e+01\n",
      "  1.87880386e+02  1.53270691e+02  2.68846741e+02  2.12266769e+02\n",
      "  1.13212791e+02  1.63645477e+02  5.94328690e+01  6.75834579e+01\n",
      "  1.82584702e+02  9.98751221e+01  9.83652954e+01  5.98206482e+01\n",
      "  6.94053040e+01  6.32844925e+01  1.61005341e+02  2.13350143e+02\n",
      "  1.07249283e+02  4.20182838e+01 -5.67026520e+01  1.83976746e+02\n",
      "  1.59772308e+02  3.02365303e+01  1.23001488e+02  3.95394501e+02\n",
      "  5.85178146e+01  2.88176727e+02  5.06391487e+01  2.76267761e+02\n",
      "  2.66631989e+02  1.59946091e+02  1.42793350e+02  2.50755936e+02\n",
      "  2.43551224e+02  1.31676849e+02  2.93601929e+02  1.31955429e+02\n",
      " -4.73117142e+01  1.40873367e+02  2.67853516e+02  2.84804993e+02\n",
      "  4.44613678e+02  6.50788345e+01  9.01313019e+01  2.19899231e+02\n",
      "  9.00253754e+01  1.36016815e+02  9.20605621e+01  4.67701630e+02\n",
      " -9.87252502e+01  7.10234833e+01  6.56441040e+01  4.36562927e+02\n",
      " -6.62142105e+01  1.43648300e+02  1.26887428e+02  8.60912094e+01\n",
      "  1.31980545e+02  1.92957108e+02  3.00865173e+02  9.00012054e+01\n",
      "  9.84669647e+01  6.15341721e+01  4.27959869e+02 -2.96309967e+01\n",
      "  1.41967972e+02 -5.09906502e+01  3.09362717e+01  1.47356201e+02\n",
      "  1.58948822e+01  1.07331738e+03  8.75325241e+01  1.41918503e+02\n",
      "  7.54078903e+01  5.99312668e+01  2.69084534e+02  1.26512558e+02\n",
      "  1.97530533e+02  7.94735870e+01 -1.99643517e+00  1.09808624e+02\n",
      "  5.67883682e+01  1.31060455e+02  8.67080765e+01  1.94995865e+02\n",
      "  3.08623383e+02  1.30122711e+02  2.80400269e+02  8.38511047e+01\n",
      "  4.33987976e+02  1.42899841e+02  1.55515167e+02  1.39331360e+02\n",
      "  1.11898033e+02  1.52036438e+02  1.39576279e+02  2.55048542e+01\n",
      "  1.52218109e+02  9.08605728e+01  1.67595856e+02  1.16936028e+02\n",
      "  3.75509834e+01  1.66027969e+02  3.78505493e+02  6.50800765e-01\n",
      "  1.45878372e+02 -3.85320926e+00  1.18473000e+02  4.87519531e+01\n",
      "  9.69372177e+01  1.81351486e+02  1.43688232e+02  1.33327637e+02\n",
      "  7.75331192e+01  4.41436729e+01  1.52224930e+02  1.94235352e+02\n",
      "  7.99186249e+01 -3.40156631e+01  2.29307358e+02  1.54017838e+02\n",
      "  1.47538269e+02  1.31288528e+02  1.54458221e+02  9.20460358e+01\n",
      "  2.27534897e+02  2.80572449e+02  7.51893158e+01  2.76951141e+02\n",
      "  1.60013519e+02  1.43798141e+02  6.10468826e+01  1.42006241e+02\n",
      "  2.72151367e+02  9.73369293e+01 -2.00344391e+01  1.41753220e+02\n",
      "  1.45125076e+02  7.70892334e+01  2.37219727e+02  5.76514626e+01\n",
      "  2.20143112e+02  2.17938995e+02  9.74082947e+01  1.25827744e+02\n",
      "  1.94014862e+02  6.63270645e+01  1.09953690e+02 -5.30438137e+00\n",
      "  2.11702316e+02  1.28658829e+02  1.56899170e+02  1.39601562e+02\n",
      "  2.97904663e+02  1.13629593e+02  7.42328644e+01  1.45366058e+02\n",
      " -6.22177505e+01  1.53811035e+02  1.73238159e+02  2.20688873e+02\n",
      "  1.28566605e+02  6.82807083e+01  3.40575775e+02  2.93351715e+02\n",
      "  8.84897156e+01 -3.21622696e+01 -7.26388407e+00  9.30393524e+01\n",
      "  9.70094528e+01  1.59800674e+02  1.38215424e+02  4.66820557e+02\n",
      "  1.82898697e+02  5.57310295e+01  1.00468849e+02  3.03898773e+02\n",
      "  1.30353302e+02  1.54533188e+02  1.30464157e+02  1.14645752e+02\n",
      "  2.66122803e+02  2.64429352e+02  5.72000656e+01  1.64455887e+02\n",
      "  1.71862930e+02  2.50755936e+02  1.48732193e+02  1.39634735e+02\n",
      "  1.35855515e+02  2.30579819e+02  2.57850677e+02  2.46528320e+02\n",
      "  9.06798782e+01  4.89452171e+01  1.57644485e+02  5.48407822e+01\n",
      "  3.01796021e+02  2.51945847e+02  1.49659210e+02  1.36940125e+02\n",
      "  8.38087616e+01  6.08745232e+01  3.10503998e+02  2.56955231e+02\n",
      "  1.15500931e+02  1.22089165e+02  1.62391418e+02  2.73107697e+02\n",
      "  1.97712845e+02  4.65495758e+02  2.63191242e+01  1.12934196e+02\n",
      "  1.40780350e+02  9.12751541e+01  1.88691269e+02  1.05009064e+02\n",
      "  4.32596222e+02  7.90856400e+01  1.01706116e+02  5.08306999e+01\n",
      "  1.48713852e+02  4.48196106e+01  2.31268280e+02  4.55636169e+02\n",
      "  3.53646332e+02  1.46612030e+02  7.75107422e+01  2.84459656e+02\n",
      "  2.67325653e+02  1.66732834e+02  2.99248322e+02  2.92912292e+02\n",
      "  3.91912956e+01  1.62234268e+02  2.05756416e+01  1.66200119e+02\n",
      "  1.42981613e+02  7.41651840e+01]\n",
      "[  6.  54. 275. 240. 450. 108. 225.  58. 150. 162. 250.  80. 162.  64.\n",
      " 100.  46. 174. 128. 250. 184. 500. 300. 285.  68.  88.  30. 120.   8.\n",
      " 128. 154. 144. 126.  80.  52. 112. 158. 300.  18.  66. 450. 265. 290.\n",
      " 168. 255. 295. 300.  32.  24.  96. 132.   6.  92. 186.   8. 180. 196.\n",
      " 108. 230. 225.   8. 215. 290.  54. 450. 250. 270. 230. 200. 225.  80.\n",
      " 245.   4. 190. 168.  44. 225. 182. 122. 120.   4. 124. 350.  56.   2.\n",
      "  12.  84.  84.  78.  84.  82. 225.  18. 176. 174. 198. 225. 100. 225.\n",
      "  46.  60. 188.  88.  70.  10.  24.  48. 172. 186.  52.  10.   2. 188.\n",
      " 156.  48. 116. 400.  30. 275.  40. 275. 215. 156. 190. 225. 245. 172.\n",
      " 235. 102.   6. 122. 265. 295. 450.  46.  86. 196.  84. 112.  10. 500.\n",
      "  20.  58.  34. 500.  14.  82. 104.  92. 154. 188. 285. 100. 120.  34.\n",
      " 500.  16. 132.  46.  12. 136.  22.   1. 100.  90.  58.  38. 270. 112.\n",
      " 215.  32.  26. 124.  16. 144.  90. 182. 300. 134. 215. 102. 450. 128.\n",
      "  94. 138.  72. 146. 158.   0. 112.  40. 178.  98.  32. 136. 400.   4.\n",
      " 112.  28. 106.  28.  74. 200. 192.  96.  58.  32. 140. 172.  72.  40.\n",
      " 200. 156. 162. 142. 134.  76. 255. 240.  58. 280. 124. 128.  28. 136.\n",
      " 275.  26.  16. 128. 134.  64. 225.  32. 186. 220.  52. 154. 184.  54.\n",
      " 114.  18. 220.  94. 130.  60. 295.  74.  60. 164.  10. 198. 138. 200.\n",
      " 128.  74. 350. 285. 102.   2.   4.  92.  86. 154. 190. 500. 186.  42.\n",
      "  62. 275. 110. 124. 116.  98. 275. 265.  28. 182. 198. 225. 170. 154.\n",
      "  86. 235. 300. 240.  84.  36. 150.  14. 300. 250. 164. 160.  78.  22.\n",
      " 300. 230. 112. 118. 118. 250. 178. 500.   0.  90. 120.  80. 186.  90.\n",
      " 450.  74.  64.  24. 140.  36. 215. 450. 350. 136.  68. 290. 220. 116.\n",
      " 300. 290.   8. 120.  48. 188. 164.  86.]\n",
      "mse =  1428.164659302337\n"
     ]
    }
   ],
   "source": [
    "pic = X_train[1]\n",
    "model = Sequential()\n",
    "#model.add(Input(shape=pic.shape))\n",
    "# model.add(Conv2D(filters=4, kernel_size=(7,7), strides=1, input_shape=pic.shape))\n",
    "Kerasmodel = keras.applications.resnet50.ResNet50(include_top=True\n",
    "                , weights=None#'imagenet'\n",
    "                # , input_tensor=inputs\n",
    "                , input_shape=pic.shape\n",
    "                )\n",
    "# Kerasmodel._layers.pop()\n",
    "# Kerasmodel.layers.pop()\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(1,kernel_initializer='he_normal'))\n",
    "model.add(Activation('linear')) #softmax\n",
    "model = Model( inputs=Kerasmodel.input , outputs=model(Kerasmodel.layers[-2].output))\n",
    "model.compile(loss='mean_squared_error', #mse binary_crossentropy Dice-coefficient loss function vs cross-entropy\n",
    "                optimizer=optimizers.Adam(lr=5e-3),\n",
    "                metrics=['mse'])\n",
    "\n",
    "model.summary()  \n",
    "time_str = str(time.time())\n",
    "print(time_str)\n",
    "reinitLayers(model)\n",
    "early_stop = EarlyStopping(monitor='val_mean_squared_error', patience=30)\n",
    "# model.load_weights('./modelWights/regression_model'+model_fn.__name__+'.h5')\n",
    "model_checkpoint = ModelCheckpoint('./modelWights/weights_'+'resnet50_without_pre'+time_str+'_gray_scale.h5', monitor='val_loss', save_best_only=True)\n",
    "tensorboard = keras.callbacks.tensorboard_v1.TensorBoard(log_dir=\"~/data/projects_logs/water_logs/resnet50_without_pre_color_{}\".format(time_str), histogram_freq=2,write_grads=True)\n",
    "\n",
    "history = model.fit(X_train_g, y_train, batch_size=BATCH_SIZE, epochs=100, validation_data=(X_val_g, y_val), callbacks=[model_checkpoint,])\n",
    "# print(history.history)\n",
    "\n",
    "y_pred_ = model.predict(X_test_g, batch_size=1, verbose=1)\n",
    "\n",
    "time_str = str(time.time())\n",
    "y_pred_, y_test_ = post_training_data(y_test, y_pred_)\n",
    "predicted_file = 'regress_'+\"resnet50_without_pre\"+\"resnet50\"+time_str+'_without_pretrain_gray_scale.csv'\n",
    "save_result(\"./result/\"+predicted_file,y_pred_,y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 512, 512, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 518, 518, 3)  0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 256, 256, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 256, 256, 64) 256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_201 (Activation)     (None, 256, 256, 64) 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 258, 258, 64) 0           activation_201[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 128, 128, 64) 0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 128, 128, 64) 4160        max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 128, 128, 64) 256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_202 (Activation)     (None, 128, 128, 64) 0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 128, 128, 64) 36928       activation_202[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 128, 128, 64) 256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_203 (Activation)     (None, 128, 128, 64) 0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 128, 128, 256 16640       activation_203[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 128, 128, 256 16640       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 128, 128, 256 1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 128, 128, 256 1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_65 (Add)                    (None, 128, 128, 256 0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_204 (Activation)     (None, 128, 128, 256 0           add_65[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 128, 128, 64) 16448       activation_204[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 128, 128, 64) 256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_205 (Activation)     (None, 128, 128, 64) 0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 128, 128, 64) 36928       activation_205[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 128, 128, 64) 256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_206 (Activation)     (None, 128, 128, 64) 0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 128, 128, 256 16640       activation_206[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 128, 128, 256 1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_66 (Add)                    (None, 128, 128, 256 0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_204[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_207 (Activation)     (None, 128, 128, 256 0           add_66[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 128, 128, 64) 16448       activation_207[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 128, 128, 64) 256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_208 (Activation)     (None, 128, 128, 64) 0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 128, 128, 64) 36928       activation_208[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 128, 128, 64) 256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_209 (Activation)     (None, 128, 128, 64) 0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 128, 128, 256 16640       activation_209[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 128, 128, 256 1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_67 (Add)                    (None, 128, 128, 256 0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_207[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_210 (Activation)     (None, 128, 128, 256 0           add_67[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 64, 64, 128)  32896       activation_210[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_211 (Activation)     (None, 64, 64, 128)  0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_211[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_212 (Activation)     (None, 64, 64, 128)  0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_212[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 64, 64, 512)  131584      activation_210[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 64, 64, 512)  2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_68 (Add)                    (None, 64, 64, 512)  0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_213 (Activation)     (None, 64, 64, 512)  0           add_68[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 64, 64, 128)  65664       activation_213[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_214 (Activation)     (None, 64, 64, 128)  0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_214[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_215 (Activation)     (None, 64, 64, 128)  0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_215[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_69 (Add)                    (None, 64, 64, 512)  0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_213[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_216 (Activation)     (None, 64, 64, 512)  0           add_69[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 64, 64, 128)  65664       activation_216[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_217 (Activation)     (None, 64, 64, 128)  0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_217[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_218 (Activation)     (None, 64, 64, 128)  0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_218[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_70 (Add)                    (None, 64, 64, 512)  0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_216[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_219 (Activation)     (None, 64, 64, 512)  0           add_70[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 64, 64, 128)  65664       activation_219[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 64, 64, 128)  512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_220 (Activation)     (None, 64, 64, 128)  0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 64, 64, 128)  147584      activation_220[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 64, 64, 128)  512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_221 (Activation)     (None, 64, 64, 128)  0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 64, 64, 512)  66048       activation_221[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 64, 64, 512)  2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_71 (Add)                    (None, 64, 64, 512)  0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_219[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_222 (Activation)     (None, 64, 64, 512)  0           add_71[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 32, 32, 256)  131328      activation_222[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_223 (Activation)     (None, 32, 32, 256)  0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_223[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_224 (Activation)     (None, 32, 32, 256)  0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_224[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 32, 32, 1024) 525312      activation_222[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 32, 32, 1024) 4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_72 (Add)                    (None, 32, 32, 1024) 0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_225 (Activation)     (None, 32, 32, 1024) 0           add_72[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_225[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_226 (Activation)     (None, 32, 32, 256)  0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_226[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_227 (Activation)     (None, 32, 32, 256)  0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_227[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_73 (Add)                    (None, 32, 32, 1024) 0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_225[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_228 (Activation)     (None, 32, 32, 1024) 0           add_73[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_228[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_229 (Activation)     (None, 32, 32, 256)  0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_229[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_230 (Activation)     (None, 32, 32, 256)  0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_230[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_74 (Add)                    (None, 32, 32, 1024) 0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_228[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_231 (Activation)     (None, 32, 32, 1024) 0           add_74[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_231[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_232 (Activation)     (None, 32, 32, 256)  0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_232[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_233 (Activation)     (None, 32, 32, 256)  0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_233[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_75 (Add)                    (None, 32, 32, 1024) 0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_231[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_234 (Activation)     (None, 32, 32, 1024) 0           add_75[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_234[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_235 (Activation)     (None, 32, 32, 256)  0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_235[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_236 (Activation)     (None, 32, 32, 256)  0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_236[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_76 (Add)                    (None, 32, 32, 1024) 0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_234[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_237 (Activation)     (None, 32, 32, 1024) 0           add_76[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 32, 32, 256)  262400      activation_237[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 32, 32, 256)  1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_238 (Activation)     (None, 32, 32, 256)  0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 32, 32, 256)  590080      activation_238[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 32, 32, 256)  1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_239 (Activation)     (None, 32, 32, 256)  0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 32, 32, 1024) 263168      activation_239[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, 32, 32, 1024) 4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_77 (Add)                    (None, 32, 32, 1024) 0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_237[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_240 (Activation)     (None, 32, 32, 1024) 0           add_77[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 16, 16, 512)  524800      activation_240[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 16, 16, 512)  2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_241 (Activation)     (None, 16, 16, 512)  0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 16, 16, 512)  2359808     activation_241[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 16, 16, 512)  2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_242 (Activation)     (None, 16, 16, 512)  0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 16, 16, 2048) 1050624     activation_242[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 16, 16, 2048) 2099200     activation_240[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, 16, 16, 2048) 8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 16, 16, 2048) 8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_78 (Add)                    (None, 16, 16, 2048) 0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_243 (Activation)     (None, 16, 16, 2048) 0           add_78[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 16, 16, 512)  1049088     activation_243[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 16, 16, 512)  2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_244 (Activation)     (None, 16, 16, 512)  0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 16, 16, 512)  2359808     activation_244[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 16, 16, 512)  2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_245 (Activation)     (None, 16, 16, 512)  0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 16, 16, 2048) 1050624     activation_245[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, 16, 16, 2048) 8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_79 (Add)                    (None, 16, 16, 2048) 0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_243[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_246 (Activation)     (None, 16, 16, 2048) 0           add_79[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 16, 16, 512)  1049088     activation_246[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 16, 16, 512)  2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_247 (Activation)     (None, 16, 16, 512)  0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 16, 16, 512)  2359808     activation_247[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 16, 16, 512)  2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_248 (Activation)     (None, 16, 16, 512)  0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, 16, 16, 2048) 1050624     activation_248[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, 16, 16, 2048) 8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_80 (Add)                    (None, 16, 16, 2048) 0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_246[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_249 (Activation)     (None, 16, 16, 2048) 0           add_80[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           activation_249[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sequential_5 (Sequential)       (None, 1)            2049        avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 23,589,761\n",
      "Trainable params: 23,536,641\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n",
      "1570979349.0078342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER:: input_5\n",
      "LAYER:: conv1_pad\n",
      "LAYER:: conv1\n",
      "reinitializing layer conv1.kernel\n",
      "reinitializing layer conv1.bias\n",
      "LAYER:: bn_conv1\n",
      "reinitializing layer bn_conv1.gamma\n",
      "reinitializing layer bn_conv1.beta\n",
      "reinitializing layer bn_conv1.moving_mean\n",
      "reinitializing layer bn_conv1.moving_variance\n",
      "LAYER:: activation_201\n",
      "LAYER:: pool1_pad\n",
      "LAYER:: max_pooling2d_5\n",
      "LAYER:: res2a_branch2a\n",
      "reinitializing layer res2a_branch2a.kernel\n",
      "reinitializing layer res2a_branch2a.bias\n",
      "LAYER:: bn2a_branch2a\n",
      "reinitializing layer bn2a_branch2a.gamma\n",
      "reinitializing layer bn2a_branch2a.beta\n",
      "reinitializing layer bn2a_branch2a.moving_mean\n",
      "reinitializing layer bn2a_branch2a.moving_variance\n",
      "LAYER:: activation_202\n",
      "LAYER:: res2a_branch2b\n",
      "reinitializing layer res2a_branch2b.kernel\n",
      "reinitializing layer res2a_branch2b.bias\n",
      "LAYER:: bn2a_branch2b\n",
      "reinitializing layer bn2a_branch2b.gamma\n",
      "reinitializing layer bn2a_branch2b.beta\n",
      "reinitializing layer bn2a_branch2b.moving_mean\n",
      "reinitializing layer bn2a_branch2b.moving_variance\n",
      "LAYER:: activation_203\n",
      "LAYER:: res2a_branch2c\n",
      "reinitializing layer res2a_branch2c.kernel\n",
      "reinitializing layer res2a_branch2c.bias\n",
      "LAYER:: res2a_branch1\n",
      "reinitializing layer res2a_branch1.kernel\n",
      "reinitializing layer res2a_branch1.bias\n",
      "LAYER:: bn2a_branch2c\n",
      "reinitializing layer bn2a_branch2c.gamma\n",
      "reinitializing layer bn2a_branch2c.beta\n",
      "reinitializing layer bn2a_branch2c.moving_mean\n",
      "reinitializing layer bn2a_branch2c.moving_variance\n",
      "LAYER:: bn2a_branch1\n",
      "reinitializing layer bn2a_branch1.gamma\n",
      "reinitializing layer bn2a_branch1.beta\n",
      "reinitializing layer bn2a_branch1.moving_mean\n",
      "reinitializing layer bn2a_branch1.moving_variance\n",
      "LAYER:: add_65\n",
      "LAYER:: activation_204\n",
      "LAYER:: res2b_branch2a\n",
      "reinitializing layer res2b_branch2a.kernel\n",
      "reinitializing layer res2b_branch2a.bias\n",
      "LAYER:: bn2b_branch2a\n",
      "reinitializing layer bn2b_branch2a.gamma\n",
      "reinitializing layer bn2b_branch2a.beta\n",
      "reinitializing layer bn2b_branch2a.moving_mean\n",
      "reinitializing layer bn2b_branch2a.moving_variance\n",
      "LAYER:: activation_205\n",
      "LAYER:: res2b_branch2b\n",
      "reinitializing layer res2b_branch2b.kernel\n",
      "reinitializing layer res2b_branch2b.bias\n",
      "LAYER:: bn2b_branch2b\n",
      "reinitializing layer bn2b_branch2b.gamma\n",
      "reinitializing layer bn2b_branch2b.beta\n",
      "reinitializing layer bn2b_branch2b.moving_mean\n",
      "reinitializing layer bn2b_branch2b.moving_variance\n",
      "LAYER:: activation_206\n",
      "LAYER:: res2b_branch2c\n",
      "reinitializing layer res2b_branch2c.kernel\n",
      "reinitializing layer res2b_branch2c.bias\n",
      "LAYER:: bn2b_branch2c\n",
      "reinitializing layer bn2b_branch2c.gamma\n",
      "reinitializing layer bn2b_branch2c.beta\n",
      "reinitializing layer bn2b_branch2c.moving_mean\n",
      "reinitializing layer bn2b_branch2c.moving_variance\n",
      "LAYER:: add_66\n",
      "LAYER:: activation_207\n",
      "LAYER:: res2c_branch2a\n",
      "reinitializing layer res2c_branch2a.kernel\n",
      "reinitializing layer res2c_branch2a.bias\n",
      "LAYER:: bn2c_branch2a\n",
      "reinitializing layer bn2c_branch2a.gamma\n",
      "reinitializing layer bn2c_branch2a.beta\n",
      "reinitializing layer bn2c_branch2a.moving_mean\n",
      "reinitializing layer bn2c_branch2a.moving_variance\n",
      "LAYER:: activation_208\n",
      "LAYER:: res2c_branch2b\n",
      "reinitializing layer res2c_branch2b.kernel\n",
      "reinitializing layer res2c_branch2b.bias\n",
      "LAYER:: bn2c_branch2b\n",
      "reinitializing layer bn2c_branch2b.gamma\n",
      "reinitializing layer bn2c_branch2b.beta\n",
      "reinitializing layer bn2c_branch2b.moving_mean\n",
      "reinitializing layer bn2c_branch2b.moving_variance\n",
      "LAYER:: activation_209\n",
      "LAYER:: res2c_branch2c\n",
      "reinitializing layer res2c_branch2c.kernel\n",
      "reinitializing layer res2c_branch2c.bias\n",
      "LAYER:: bn2c_branch2c\n",
      "reinitializing layer bn2c_branch2c.gamma\n",
      "reinitializing layer bn2c_branch2c.beta\n",
      "reinitializing layer bn2c_branch2c.moving_mean\n",
      "reinitializing layer bn2c_branch2c.moving_variance\n",
      "LAYER:: add_67\n",
      "LAYER:: activation_210\n",
      "LAYER:: res3a_branch2a\n",
      "reinitializing layer res3a_branch2a.kernel\n",
      "reinitializing layer res3a_branch2a.bias\n",
      "LAYER:: bn3a_branch2a\n",
      "reinitializing layer bn3a_branch2a.gamma\n",
      "reinitializing layer bn3a_branch2a.beta\n",
      "reinitializing layer bn3a_branch2a.moving_mean\n",
      "reinitializing layer bn3a_branch2a.moving_variance\n",
      "LAYER:: activation_211\n",
      "LAYER:: res3a_branch2b\n",
      "reinitializing layer res3a_branch2b.kernel\n",
      "reinitializing layer res3a_branch2b.bias\n",
      "LAYER:: bn3a_branch2b\n",
      "reinitializing layer bn3a_branch2b.gamma\n",
      "reinitializing layer bn3a_branch2b.beta\n",
      "reinitializing layer bn3a_branch2b.moving_mean\n",
      "reinitializing layer bn3a_branch2b.moving_variance\n",
      "LAYER:: activation_212\n",
      "LAYER:: res3a_branch2c\n",
      "reinitializing layer res3a_branch2c.kernel\n",
      "reinitializing layer res3a_branch2c.bias\n",
      "LAYER:: res3a_branch1\n",
      "reinitializing layer res3a_branch1.kernel\n",
      "reinitializing layer res3a_branch1.bias\n",
      "LAYER:: bn3a_branch2c\n",
      "reinitializing layer bn3a_branch2c.gamma\n",
      "reinitializing layer bn3a_branch2c.beta\n",
      "reinitializing layer bn3a_branch2c.moving_mean\n",
      "reinitializing layer bn3a_branch2c.moving_variance\n",
      "LAYER:: bn3a_branch1\n",
      "reinitializing layer bn3a_branch1.gamma\n",
      "reinitializing layer bn3a_branch1.beta\n",
      "reinitializing layer bn3a_branch1.moving_mean\n",
      "reinitializing layer bn3a_branch1.moving_variance\n",
      "LAYER:: add_68\n",
      "LAYER:: activation_213\n",
      "LAYER:: res3b_branch2a\n",
      "reinitializing layer res3b_branch2a.kernel\n",
      "reinitializing layer res3b_branch2a.bias\n",
      "LAYER:: bn3b_branch2a\n",
      "reinitializing layer bn3b_branch2a.gamma\n",
      "reinitializing layer bn3b_branch2a.beta\n",
      "reinitializing layer bn3b_branch2a.moving_mean\n",
      "reinitializing layer bn3b_branch2a.moving_variance\n",
      "LAYER:: activation_214\n",
      "LAYER:: res3b_branch2b\n",
      "reinitializing layer res3b_branch2b.kernel\n",
      "reinitializing layer res3b_branch2b.bias\n",
      "LAYER:: bn3b_branch2b\n",
      "reinitializing layer bn3b_branch2b.gamma\n",
      "reinitializing layer bn3b_branch2b.beta\n",
      "reinitializing layer bn3b_branch2b.moving_mean\n",
      "reinitializing layer bn3b_branch2b.moving_variance\n",
      "LAYER:: activation_215\n",
      "LAYER:: res3b_branch2c\n",
      "reinitializing layer res3b_branch2c.kernel\n",
      "reinitializing layer res3b_branch2c.bias\n",
      "LAYER:: bn3b_branch2c\n",
      "reinitializing layer bn3b_branch2c.gamma\n",
      "reinitializing layer bn3b_branch2c.beta\n",
      "reinitializing layer bn3b_branch2c.moving_mean\n",
      "reinitializing layer bn3b_branch2c.moving_variance\n",
      "LAYER:: add_69\n",
      "LAYER:: activation_216\n",
      "LAYER:: res3c_branch2a\n",
      "reinitializing layer res3c_branch2a.kernel\n",
      "reinitializing layer res3c_branch2a.bias\n",
      "LAYER:: bn3c_branch2a\n",
      "reinitializing layer bn3c_branch2a.gamma\n",
      "reinitializing layer bn3c_branch2a.beta\n",
      "reinitializing layer bn3c_branch2a.moving_mean\n",
      "reinitializing layer bn3c_branch2a.moving_variance\n",
      "LAYER:: activation_217\n",
      "LAYER:: res3c_branch2b\n",
      "reinitializing layer res3c_branch2b.kernel\n",
      "reinitializing layer res3c_branch2b.bias\n",
      "LAYER:: bn3c_branch2b\n",
      "reinitializing layer bn3c_branch2b.gamma\n",
      "reinitializing layer bn3c_branch2b.beta\n",
      "reinitializing layer bn3c_branch2b.moving_mean\n",
      "reinitializing layer bn3c_branch2b.moving_variance\n",
      "LAYER:: activation_218\n",
      "LAYER:: res3c_branch2c\n",
      "reinitializing layer res3c_branch2c.kernel\n",
      "reinitializing layer res3c_branch2c.bias\n",
      "LAYER:: bn3c_branch2c\n",
      "reinitializing layer bn3c_branch2c.gamma\n",
      "reinitializing layer bn3c_branch2c.beta\n",
      "reinitializing layer bn3c_branch2c.moving_mean\n",
      "reinitializing layer bn3c_branch2c.moving_variance\n",
      "LAYER:: add_70\n",
      "LAYER:: activation_219\n",
      "LAYER:: res3d_branch2a\n",
      "reinitializing layer res3d_branch2a.kernel\n",
      "reinitializing layer res3d_branch2a.bias\n",
      "LAYER:: bn3d_branch2a\n",
      "reinitializing layer bn3d_branch2a.gamma\n",
      "reinitializing layer bn3d_branch2a.beta\n",
      "reinitializing layer bn3d_branch2a.moving_mean\n",
      "reinitializing layer bn3d_branch2a.moving_variance\n",
      "LAYER:: activation_220\n",
      "LAYER:: res3d_branch2b\n",
      "reinitializing layer res3d_branch2b.kernel\n",
      "reinitializing layer res3d_branch2b.bias\n",
      "LAYER:: bn3d_branch2b\n",
      "reinitializing layer bn3d_branch2b.gamma\n",
      "reinitializing layer bn3d_branch2b.beta\n",
      "reinitializing layer bn3d_branch2b.moving_mean\n",
      "reinitializing layer bn3d_branch2b.moving_variance\n",
      "LAYER:: activation_221\n",
      "LAYER:: res3d_branch2c\n",
      "reinitializing layer res3d_branch2c.kernel\n",
      "reinitializing layer res3d_branch2c.bias\n",
      "LAYER:: bn3d_branch2c\n",
      "reinitializing layer bn3d_branch2c.gamma\n",
      "reinitializing layer bn3d_branch2c.beta\n",
      "reinitializing layer bn3d_branch2c.moving_mean\n",
      "reinitializing layer bn3d_branch2c.moving_variance\n",
      "LAYER:: add_71\n",
      "LAYER:: activation_222\n",
      "LAYER:: res4a_branch2a\n",
      "reinitializing layer res4a_branch2a.kernel\n",
      "reinitializing layer res4a_branch2a.bias\n",
      "LAYER:: bn4a_branch2a\n",
      "reinitializing layer bn4a_branch2a.gamma\n",
      "reinitializing layer bn4a_branch2a.beta\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reinitializing layer bn4a_branch2a.moving_mean\n",
      "reinitializing layer bn4a_branch2a.moving_variance\n",
      "LAYER:: activation_223\n",
      "LAYER:: res4a_branch2b\n",
      "reinitializing layer res4a_branch2b.kernel\n",
      "reinitializing layer res4a_branch2b.bias\n",
      "LAYER:: bn4a_branch2b\n",
      "reinitializing layer bn4a_branch2b.gamma\n",
      "reinitializing layer bn4a_branch2b.beta\n",
      "reinitializing layer bn4a_branch2b.moving_mean\n",
      "reinitializing layer bn4a_branch2b.moving_variance\n",
      "LAYER:: activation_224\n",
      "LAYER:: res4a_branch2c\n",
      "reinitializing layer res4a_branch2c.kernel\n",
      "reinitializing layer res4a_branch2c.bias\n",
      "LAYER:: res4a_branch1\n",
      "reinitializing layer res4a_branch1.kernel\n",
      "reinitializing layer res4a_branch1.bias\n",
      "LAYER:: bn4a_branch2c\n",
      "reinitializing layer bn4a_branch2c.gamma\n",
      "reinitializing layer bn4a_branch2c.beta\n",
      "reinitializing layer bn4a_branch2c.moving_mean\n",
      "reinitializing layer bn4a_branch2c.moving_variance\n",
      "LAYER:: bn4a_branch1\n",
      "reinitializing layer bn4a_branch1.gamma\n",
      "reinitializing layer bn4a_branch1.beta\n",
      "reinitializing layer bn4a_branch1.moving_mean\n",
      "reinitializing layer bn4a_branch1.moving_variance\n",
      "LAYER:: add_72\n",
      "LAYER:: activation_225\n",
      "LAYER:: res4b_branch2a\n",
      "reinitializing layer res4b_branch2a.kernel\n",
      "reinitializing layer res4b_branch2a.bias\n",
      "LAYER:: bn4b_branch2a\n",
      "reinitializing layer bn4b_branch2a.gamma\n",
      "reinitializing layer bn4b_branch2a.beta\n",
      "reinitializing layer bn4b_branch2a.moving_mean\n",
      "reinitializing layer bn4b_branch2a.moving_variance\n",
      "LAYER:: activation_226\n",
      "LAYER:: res4b_branch2b\n",
      "reinitializing layer res4b_branch2b.kernel\n",
      "reinitializing layer res4b_branch2b.bias\n",
      "LAYER:: bn4b_branch2b\n",
      "reinitializing layer bn4b_branch2b.gamma\n",
      "reinitializing layer bn4b_branch2b.beta\n",
      "reinitializing layer bn4b_branch2b.moving_mean\n",
      "reinitializing layer bn4b_branch2b.moving_variance\n",
      "LAYER:: activation_227\n",
      "LAYER:: res4b_branch2c\n",
      "reinitializing layer res4b_branch2c.kernel\n",
      "reinitializing layer res4b_branch2c.bias\n",
      "LAYER:: bn4b_branch2c\n",
      "reinitializing layer bn4b_branch2c.gamma\n",
      "reinitializing layer bn4b_branch2c.beta\n",
      "reinitializing layer bn4b_branch2c.moving_mean\n",
      "reinitializing layer bn4b_branch2c.moving_variance\n",
      "LAYER:: add_73\n",
      "LAYER:: activation_228\n",
      "LAYER:: res4c_branch2a\n",
      "reinitializing layer res4c_branch2a.kernel\n",
      "reinitializing layer res4c_branch2a.bias\n",
      "LAYER:: bn4c_branch2a\n",
      "reinitializing layer bn4c_branch2a.gamma\n",
      "reinitializing layer bn4c_branch2a.beta\n",
      "reinitializing layer bn4c_branch2a.moving_mean\n",
      "reinitializing layer bn4c_branch2a.moving_variance\n",
      "LAYER:: activation_229\n",
      "LAYER:: res4c_branch2b\n",
      "reinitializing layer res4c_branch2b.kernel\n",
      "reinitializing layer res4c_branch2b.bias\n",
      "LAYER:: bn4c_branch2b\n",
      "reinitializing layer bn4c_branch2b.gamma\n",
      "reinitializing layer bn4c_branch2b.beta\n",
      "reinitializing layer bn4c_branch2b.moving_mean\n",
      "reinitializing layer bn4c_branch2b.moving_variance\n",
      "LAYER:: activation_230\n",
      "LAYER:: res4c_branch2c\n",
      "reinitializing layer res4c_branch2c.kernel\n",
      "reinitializing layer res4c_branch2c.bias\n",
      "LAYER:: bn4c_branch2c\n",
      "reinitializing layer bn4c_branch2c.gamma\n",
      "reinitializing layer bn4c_branch2c.beta\n",
      "reinitializing layer bn4c_branch2c.moving_mean\n",
      "reinitializing layer bn4c_branch2c.moving_variance\n",
      "LAYER:: add_74\n",
      "LAYER:: activation_231\n",
      "LAYER:: res4d_branch2a\n",
      "reinitializing layer res4d_branch2a.kernel\n",
      "reinitializing layer res4d_branch2a.bias\n",
      "LAYER:: bn4d_branch2a\n",
      "reinitializing layer bn4d_branch2a.gamma\n",
      "reinitializing layer bn4d_branch2a.beta\n",
      "reinitializing layer bn4d_branch2a.moving_mean\n",
      "reinitializing layer bn4d_branch2a.moving_variance\n",
      "LAYER:: activation_232\n",
      "LAYER:: res4d_branch2b\n",
      "reinitializing layer res4d_branch2b.kernel\n",
      "reinitializing layer res4d_branch2b.bias\n",
      "LAYER:: bn4d_branch2b\n",
      "reinitializing layer bn4d_branch2b.gamma\n",
      "reinitializing layer bn4d_branch2b.beta\n",
      "reinitializing layer bn4d_branch2b.moving_mean\n",
      "reinitializing layer bn4d_branch2b.moving_variance\n",
      "LAYER:: activation_233\n",
      "LAYER:: res4d_branch2c\n",
      "reinitializing layer res4d_branch2c.kernel\n",
      "reinitializing layer res4d_branch2c.bias\n",
      "LAYER:: bn4d_branch2c\n",
      "reinitializing layer bn4d_branch2c.gamma\n",
      "reinitializing layer bn4d_branch2c.beta\n",
      "reinitializing layer bn4d_branch2c.moving_mean\n",
      "reinitializing layer bn4d_branch2c.moving_variance\n",
      "LAYER:: add_75\n",
      "LAYER:: activation_234\n",
      "LAYER:: res4e_branch2a\n",
      "reinitializing layer res4e_branch2a.kernel\n",
      "reinitializing layer res4e_branch2a.bias\n",
      "LAYER:: bn4e_branch2a\n",
      "reinitializing layer bn4e_branch2a.gamma\n",
      "reinitializing layer bn4e_branch2a.beta\n",
      "reinitializing layer bn4e_branch2a.moving_mean\n",
      "reinitializing layer bn4e_branch2a.moving_variance\n",
      "LAYER:: activation_235\n",
      "LAYER:: res4e_branch2b\n",
      "reinitializing layer res4e_branch2b.kernel\n",
      "reinitializing layer res4e_branch2b.bias\n",
      "LAYER:: bn4e_branch2b\n",
      "reinitializing layer bn4e_branch2b.gamma\n",
      "reinitializing layer bn4e_branch2b.beta\n",
      "reinitializing layer bn4e_branch2b.moving_mean\n",
      "reinitializing layer bn4e_branch2b.moving_variance\n",
      "LAYER:: activation_236\n",
      "LAYER:: res4e_branch2c\n",
      "reinitializing layer res4e_branch2c.kernel\n",
      "reinitializing layer res4e_branch2c.bias\n",
      "LAYER:: bn4e_branch2c\n",
      "reinitializing layer bn4e_branch2c.gamma\n",
      "reinitializing layer bn4e_branch2c.beta\n",
      "reinitializing layer bn4e_branch2c.moving_mean\n",
      "reinitializing layer bn4e_branch2c.moving_variance\n",
      "LAYER:: add_76\n",
      "LAYER:: activation_237\n",
      "LAYER:: res4f_branch2a\n",
      "reinitializing layer res4f_branch2a.kernel\n",
      "reinitializing layer res4f_branch2a.bias\n",
      "LAYER:: bn4f_branch2a\n",
      "reinitializing layer bn4f_branch2a.gamma\n",
      "reinitializing layer bn4f_branch2a.beta\n",
      "reinitializing layer bn4f_branch2a.moving_mean\n",
      "reinitializing layer bn4f_branch2a.moving_variance\n",
      "LAYER:: activation_238\n",
      "LAYER:: res4f_branch2b\n",
      "reinitializing layer res4f_branch2b.kernel\n",
      "reinitializing layer res4f_branch2b.bias\n",
      "LAYER:: bn4f_branch2b\n",
      "reinitializing layer bn4f_branch2b.gamma\n",
      "reinitializing layer bn4f_branch2b.beta\n",
      "reinitializing layer bn4f_branch2b.moving_mean\n",
      "reinitializing layer bn4f_branch2b.moving_variance\n",
      "LAYER:: activation_239\n",
      "LAYER:: res4f_branch2c\n",
      "reinitializing layer res4f_branch2c.kernel\n",
      "reinitializing layer res4f_branch2c.bias\n",
      "LAYER:: bn4f_branch2c\n",
      "reinitializing layer bn4f_branch2c.gamma\n",
      "reinitializing layer bn4f_branch2c.beta\n",
      "reinitializing layer bn4f_branch2c.moving_mean\n",
      "reinitializing layer bn4f_branch2c.moving_variance\n",
      "LAYER:: add_77\n",
      "LAYER:: activation_240\n",
      "LAYER:: res5a_branch2a\n",
      "reinitializing layer res5a_branch2a.kernel\n",
      "reinitializing layer res5a_branch2a.bias\n",
      "LAYER:: bn5a_branch2a\n",
      "reinitializing layer bn5a_branch2a.gamma\n",
      "reinitializing layer bn5a_branch2a.beta\n",
      "reinitializing layer bn5a_branch2a.moving_mean\n",
      "reinitializing layer bn5a_branch2a.moving_variance\n",
      "LAYER:: activation_241\n",
      "LAYER:: res5a_branch2b\n",
      "reinitializing layer res5a_branch2b.kernel\n",
      "reinitializing layer res5a_branch2b.bias\n",
      "LAYER:: bn5a_branch2b\n",
      "reinitializing layer bn5a_branch2b.gamma\n",
      "reinitializing layer bn5a_branch2b.beta\n",
      "reinitializing layer bn5a_branch2b.moving_mean\n",
      "reinitializing layer bn5a_branch2b.moving_variance\n",
      "LAYER:: activation_242\n",
      "LAYER:: res5a_branch2c\n",
      "reinitializing layer res5a_branch2c.kernel\n",
      "reinitializing layer res5a_branch2c.bias\n",
      "LAYER:: res5a_branch1\n",
      "reinitializing layer res5a_branch1.kernel\n",
      "reinitializing layer res5a_branch1.bias\n",
      "LAYER:: bn5a_branch2c\n",
      "reinitializing layer bn5a_branch2c.gamma\n",
      "reinitializing layer bn5a_branch2c.beta\n",
      "reinitializing layer bn5a_branch2c.moving_mean\n",
      "reinitializing layer bn5a_branch2c.moving_variance\n",
      "LAYER:: bn5a_branch1\n",
      "reinitializing layer bn5a_branch1.gamma\n",
      "reinitializing layer bn5a_branch1.beta\n",
      "reinitializing layer bn5a_branch1.moving_mean\n",
      "reinitializing layer bn5a_branch1.moving_variance\n",
      "LAYER:: add_78\n",
      "LAYER:: activation_243\n",
      "LAYER:: res5b_branch2a\n",
      "reinitializing layer res5b_branch2a.kernel\n",
      "reinitializing layer res5b_branch2a.bias\n",
      "LAYER:: bn5b_branch2a\n",
      "reinitializing layer bn5b_branch2a.gamma\n",
      "reinitializing layer bn5b_branch2a.beta\n",
      "reinitializing layer bn5b_branch2a.moving_mean\n",
      "reinitializing layer bn5b_branch2a.moving_variance\n",
      "LAYER:: activation_244\n",
      "LAYER:: res5b_branch2b\n",
      "reinitializing layer res5b_branch2b.kernel\n",
      "reinitializing layer res5b_branch2b.bias\n",
      "LAYER:: bn5b_branch2b\n",
      "reinitializing layer bn5b_branch2b.gamma\n",
      "reinitializing layer bn5b_branch2b.beta\n",
      "reinitializing layer bn5b_branch2b.moving_mean\n",
      "reinitializing layer bn5b_branch2b.moving_variance\n",
      "LAYER:: activation_245\n",
      "LAYER:: res5b_branch2c\n",
      "reinitializing layer res5b_branch2c.kernel\n",
      "reinitializing layer res5b_branch2c.bias\n",
      "LAYER:: bn5b_branch2c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reinitializing layer bn5b_branch2c.gamma\n",
      "reinitializing layer bn5b_branch2c.beta\n",
      "reinitializing layer bn5b_branch2c.moving_mean\n",
      "reinitializing layer bn5b_branch2c.moving_variance\n",
      "LAYER:: add_79\n",
      "LAYER:: activation_246\n",
      "LAYER:: res5c_branch2a\n",
      "reinitializing layer res5c_branch2a.kernel\n",
      "reinitializing layer res5c_branch2a.bias\n",
      "LAYER:: bn5c_branch2a\n",
      "reinitializing layer bn5c_branch2a.gamma\n",
      "reinitializing layer bn5c_branch2a.beta\n",
      "reinitializing layer bn5c_branch2a.moving_mean\n",
      "reinitializing layer bn5c_branch2a.moving_variance\n",
      "LAYER:: activation_247\n",
      "LAYER:: res5c_branch2b\n",
      "reinitializing layer res5c_branch2b.kernel\n",
      "reinitializing layer res5c_branch2b.bias\n",
      "LAYER:: bn5c_branch2b\n",
      "reinitializing layer bn5c_branch2b.gamma\n",
      "reinitializing layer bn5c_branch2b.beta\n",
      "reinitializing layer bn5c_branch2b.moving_mean\n",
      "reinitializing layer bn5c_branch2b.moving_variance\n",
      "LAYER:: activation_248\n",
      "LAYER:: res5c_branch2c\n",
      "reinitializing layer res5c_branch2c.kernel\n",
      "reinitializing layer res5c_branch2c.bias\n",
      "LAYER:: bn5c_branch2c\n",
      "reinitializing layer bn5c_branch2c.gamma\n",
      "reinitializing layer bn5c_branch2c.beta\n",
      "reinitializing layer bn5c_branch2c.moving_mean\n",
      "reinitializing layer bn5c_branch2c.moving_variance\n",
      "LAYER:: add_80\n",
      "LAYER:: activation_249\n",
      "LAYER:: avg_pool\n",
      "LAYER:: dense_5\n",
      "reinitializing layer dense_5.kernel\n",
      "reinitializing layer dense_5.bias\n",
      "LAYER:: activation_250\n",
      "Train on 1053 samples, validate on 264 samples\n",
      "Epoch 1/100\n",
      "1053/1053 [==============================] - 58s 55ms/step - loss: 0.9209 - mse: 0.9209 - val_loss: 0.1133 - val_mse: 0.1133\n",
      "Epoch 2/100\n",
      "1053/1053 [==============================] - 50s 47ms/step - loss: 0.0361 - mse: 0.0361 - val_loss: 0.1065 - val_mse: 0.1065\n",
      "Epoch 3/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0265 - mse: 0.0265 - val_loss: 0.0784 - val_mse: 0.0784\n",
      "Epoch 4/100\n",
      "1053/1053 [==============================] - 50s 48ms/step - loss: 0.0319 - mse: 0.0319 - val_loss: 0.0308 - val_mse: 0.0308\n",
      "Epoch 5/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0180 - val_mse: 0.0180\n",
      "Epoch 6/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 0.0293 - val_mse: 0.0293\n",
      "Epoch 7/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 8/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0131 - val_mse: 0.0131\n",
      "Epoch 9/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0124 - val_mse: 0.0124\n",
      "Epoch 10/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 11/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0220 - val_mse: 0.0220\n",
      "Epoch 12/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 13/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0280 - val_mse: 0.0280\n",
      "Epoch 14/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0153 - val_mse: 0.0153\n",
      "Epoch 15/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0315 - val_mse: 0.0315\n",
      "Epoch 16/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 17/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0148 - val_mse: 0.0148\n",
      "Epoch 18/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 19/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0182 - val_mse: 0.0182\n",
      "Epoch 20/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0411 - val_mse: 0.0411\n",
      "Epoch 21/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0426 - val_mse: 0.0426\n",
      "Epoch 22/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0202 - val_mse: 0.0202\n",
      "Epoch 23/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 24/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0134 - val_mse: 0.0134\n",
      "Epoch 25/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0124 - val_mse: 0.0124\n",
      "Epoch 26/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.4240 - val_mse: 0.4240\n",
      "Epoch 27/100\n",
      "1053/1053 [==============================] - 51s 49ms/step - loss: 0.0188 - mse: 0.0188 - val_loss: 0.0121 - val_mse: 0.0121\n",
      "Epoch 28/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0229 - val_mse: 0.0229\n",
      "Epoch 29/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0079 - mse: 0.0079 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 30/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 31/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0122 - val_mse: 0.0122\n",
      "Epoch 32/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 33/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 34/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.0297 - val_mse: 0.0297\n",
      "Epoch 35/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0181 - val_mse: 0.0181\n",
      "Epoch 36/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0198 - val_mse: 0.0198\n",
      "Epoch 37/100\n",
      "1053/1053 [==============================] - 51s 49ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0065 - val_mse: 0.0065\n",
      "Epoch 38/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0058 - val_mse: 0.0058\n",
      "Epoch 39/100\n",
      "1053/1053 [==============================] - 51s 49ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 40/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 41/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0195 - val_mse: 0.0195\n",
      "Epoch 42/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0049 - val_mse: 0.0049\n",
      "Epoch 43/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 44/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0143 - val_mse: 0.0143\n",
      "Epoch 45/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0078 - val_mse: 0.0078\n",
      "Epoch 46/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 47/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 48/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0261 - val_mse: 0.0261\n",
      "Epoch 49/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 50/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 51/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0236 - val_mse: 0.0236\n",
      "Epoch 52/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 53/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0210 - val_mse: 0.0210\n",
      "Epoch 54/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0177 - val_mse: 0.0177\n",
      "Epoch 55/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0203 - val_mse: 0.0203\n",
      "Epoch 56/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 57/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0402 - val_mse: 0.0402\n",
      "Epoch 58/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 59/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 60/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 61/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 62/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 63/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 64/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0206 - val_mse: 0.0206\n",
      "Epoch 65/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0127 - val_mse: 0.0127\n",
      "Epoch 66/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0056 - val_mse: 0.0056\n",
      "Epoch 67/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 68/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 69/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 70/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 71/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 72/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 73/100\n",
      "1053/1053 [==============================] - 51s 49ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 74/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 75/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0252 - val_mse: 0.0252\n",
      "Epoch 76/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 77/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 78/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0124 - val_mse: 0.0124\n",
      "Epoch 79/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 80/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 81/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 82/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 83/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 84/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0071 - val_mse: 0.0071\n",
      "Epoch 85/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 86/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 87/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 88/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0057 - val_mse: 0.0057\n",
      "Epoch 89/100\n",
      "1053/1053 [==============================] - 51s 49ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 90/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 91/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 92/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 93/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 94/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 95/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 96/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 97/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 98/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 99/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0102 - val_mse: 0.0102\n",
      "Epoch 100/100\n",
      "1053/1053 [==============================] - 51s 48ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "330/330 [==============================] - 11s 34ms/step\n",
      "(330,)\n",
      "(330,)\n",
      "[ 4.56776476e+00  4.38754120e+01  2.39463699e+02  2.38327408e+02\n",
      "  4.15676483e+02  1.18614433e+02  2.29719940e+02  6.69000015e+01\n",
      "  1.45756531e+02  1.28520065e+02  2.51629181e+02  4.61180801e+01\n",
      "  1.41413330e+02  1.06461227e+02  9.82179337e+01  3.91938782e+01\n",
      "  1.20602890e+02  1.30280594e+02  2.47006226e+02  1.85576599e+02\n",
      "  4.68911896e+02  2.35331985e+02  2.83038422e+02  5.41163864e+01\n",
      "  5.98605423e+01  9.72471237e+01  1.27871674e+02 -3.66026268e+01\n",
      "  1.49827316e+02  1.33766754e+02  1.61886795e+02  1.19478104e+02\n",
      "  9.18493881e+01  3.70340271e+01  1.12116081e+02  1.59382812e+02\n",
      "  2.86306305e+02  6.42449045e+00  4.58948555e+01  3.94672241e+02\n",
      "  2.52194351e+02  2.76252106e+02  1.47395325e+02  2.18787476e+02\n",
      "  2.74984680e+02  2.92011383e+02  3.35939598e+01  5.32920265e+00\n",
      "  7.16215210e+01  1.34765854e+02 -6.63474178e+00  8.00128784e+01\n",
      "  1.62260773e+02  4.37339973e+01  1.67986877e+02  1.69413269e+02\n",
      "  1.06954964e+02  2.55768143e+02  2.42129745e+02  6.30892420e+00\n",
      "  1.81808868e+02  2.80484772e+02  4.45124435e+01  3.92151459e+02\n",
      "  2.14342606e+02  2.81605316e+02  2.39812653e+02  1.94469162e+02\n",
      "  2.43268509e+02  6.72224579e+01  2.17681808e+02 -3.60436707e+01\n",
      "  1.71380356e+02  1.51641251e+02  6.28414307e+01  1.81264267e+02\n",
      "  1.39909500e+02  1.21124443e+02  1.09063400e+02 -7.35764599e+00\n",
      "  1.08779732e+02  3.79605438e+02  6.11974640e+01 -2.11587658e+01\n",
      "  7.67850113e+00  7.92880630e+01  8.48153992e+01  8.11566162e+01\n",
      "  6.36940155e+01  6.18230553e+01  2.38525436e+02  1.41960497e+01\n",
      "  1.79364792e+02  1.27732765e+02  2.48427582e+02  2.08249557e+02\n",
      "  9.27202988e+01  1.81264267e+02  3.86832542e+01  6.66015091e+01\n",
      "  1.63434555e+02  8.16361084e+01  5.60996780e+01  1.21789050e+00\n",
      "  1.15641470e+01  4.11196632e+01  1.27175613e+02  1.72820007e+02\n",
      "  7.34095001e+01  1.08798294e+01  1.70455875e+01  1.50052521e+02\n",
      "  1.28558212e+02  6.00236931e+01  8.71620941e+01  3.75927429e+02\n",
      "  2.19708805e+01  2.61934723e+02  4.14214668e+01  2.57637299e+02\n",
      "  2.53194122e+02  1.38873596e+02  1.34398468e+02  2.39585068e+02\n",
      "  2.14994370e+02  1.30310608e+02  2.50027084e+02  8.03807373e+01\n",
      " -2.41183529e+01  1.15808113e+02  2.23786392e+02  3.00465515e+02\n",
      "  4.14771484e+02  5.19422722e+01  5.23251419e+01  2.06425415e+02\n",
      "  5.94478073e+01  1.02409256e+02  2.54530640e+01  4.41996613e+02\n",
      "  4.05852509e+00  4.79868546e+01  2.58389339e+01  4.62416046e+02\n",
      " -4.69126940e+00  1.08335999e+02  1.02273643e+02  6.24981575e+01\n",
      "  1.10998154e+02  1.81043701e+02  2.71346283e+02  6.06924438e+01\n",
      "  4.91166420e+01  2.78375034e+01  3.99004913e+02 -1.48481913e+01\n",
      "  1.24575943e+02  1.01506453e+01  8.04520798e+00  1.36724136e+02\n",
      "  4.49119034e+01 -6.07871521e+02  6.85978851e+01  1.01298386e+02\n",
      "  3.12581959e+01  4.62271347e+01  2.60006958e+02  9.55180893e+01\n",
      "  1.82692474e+02  3.42066383e+01  2.10048523e+01  8.43525391e+01\n",
      "  7.46012831e+00  1.22200134e+02  6.12107086e+01  2.00383972e+02\n",
      "  2.89181580e+02  1.26665474e+02  2.43349503e+02  5.71716766e+01\n",
      "  4.15773499e+02  1.01156311e+02  1.37992157e+02  1.32301361e+02\n",
      "  6.59753113e+01  1.07227386e+02  1.40101089e+02  1.85559143e+02\n",
      "  1.42006241e+02  3.02880001e+01  1.38426529e+02  9.05930557e+01\n",
      "  2.04297962e+01  1.32193665e+02  3.64438873e+02  1.64113939e-01\n",
      "  1.05850967e+02  5.47961884e+01  1.06282097e+02  4.70454636e+01\n",
      "  8.95488052e+01  1.59232925e+02  1.72625534e+02  1.09507736e+02\n",
      "  4.82977867e+01  7.19935608e+01  1.29660660e+02  1.70984650e+02\n",
      "  5.63850555e+01  1.04503098e+02  2.07493896e+02  1.63195938e+02\n",
      "  1.27171127e+02  1.05429482e+02  1.20123131e+02  6.63999100e+01\n",
      "  1.95474228e+02  2.54850510e+02  5.84784241e+01  2.65102661e+02\n",
      "  1.18663712e+02  1.08923538e+02  3.85634575e+01  1.25655548e+02\n",
      "  2.66012390e+02  2.99196968e+01 -1.19004812e+01  1.04139069e+02\n",
      "  1.28774124e+02  7.86704102e+01  2.18509827e+02  4.85105286e+01\n",
      "  2.22015015e+02  2.30988586e+02  5.54943504e+01  1.01535233e+02\n",
      "  1.55078522e+02  4.95564690e+01  9.96380157e+01 -1.22983122e+01\n",
      "  2.00208984e+02  1.04329659e+02  1.22232002e+02  1.08094139e+02\n",
      "  2.88829987e+02  5.01387825e+01  6.95814209e+01  1.25296310e+02\n",
      " -1.09320879e+00  1.40205353e+02  1.46503815e+02  2.03031860e+02\n",
      "  1.30256409e+02  5.56304474e+01  3.31698090e+02  2.74143982e+02\n",
      "  9.16558380e+01 -1.71126976e+01  5.24475136e+01  6.65173950e+01\n",
      "  6.45901794e+01  1.32298950e+02  1.35048599e+02  4.41477203e+02\n",
      "  2.12310364e+02  1.92509861e+01  8.06320419e+01  2.75128845e+02\n",
      "  1.08820427e+02  1.11388832e+02  1.14706665e+02  1.07980751e+02\n",
      "  2.48932556e+02  2.44545273e+02  3.21926460e+01  1.38019211e+02\n",
      "  1.63413483e+02  2.39585068e+02  1.46890961e+02  1.20184822e+02\n",
      "  1.25216705e+02  2.11693771e+02  2.52517548e+02  2.09410965e+02\n",
      "  6.06069336e+01  2.16826286e+01  1.19103401e+02 -2.29899955e+00\n",
      "  2.76402069e+02  2.45336151e+02  1.31472931e+02  1.26837090e+02\n",
      "  5.17646523e+01  1.65335693e+01  2.97468140e+02  2.33888260e+02\n",
      "  8.81639099e+01  1.14419121e+02  1.38687897e+02  2.47191498e+02\n",
      "  1.91472153e+02  4.14306580e+02  1.24904381e+02  8.86590500e+01\n",
      "  1.16290215e+02  6.16631126e+01  1.74695236e+02  8.43070526e+01\n",
      "  4.24246582e+02  5.78292503e+01  6.53640976e+01  9.21237087e+00\n",
      "  1.34813049e+02  2.45601940e+01  2.18627579e+02  4.14039215e+02\n",
      "  3.34547729e+02  1.38400711e+02  4.52661209e+01  3.07778809e+02\n",
      "  2.31445221e+02  1.21859413e+02  2.78111542e+02  2.56263062e+02\n",
      "  2.08055973e+00  1.39347977e+02  5.21336212e+01  1.59947342e+02\n",
      "  1.31295456e+02  4.25802765e+01]\n",
      "[  6.  54. 275. 240. 450. 108. 225.  58. 150. 162. 250.  80. 162.  64.\n",
      " 100.  46. 174. 128. 250. 184. 500. 300. 285.  68.  88.  30. 120.   8.\n",
      " 128. 154. 144. 126.  80.  52. 112. 158. 300.  18.  66. 450. 265. 290.\n",
      " 168. 255. 295. 300.  32.  24.  96. 132.   6.  92. 186.   8. 180. 196.\n",
      " 108. 230. 225.   8. 215. 290.  54. 450. 250. 270. 230. 200. 225.  80.\n",
      " 245.   4. 190. 168.  44. 225. 182. 122. 120.   4. 124. 350.  56.   2.\n",
      "  12.  84.  84.  78.  84.  82. 225.  18. 176. 174. 198. 225. 100. 225.\n",
      "  46.  60. 188.  88.  70.  10.  24.  48. 172. 186.  52.  10.   2. 188.\n",
      " 156.  48. 116. 400.  30. 275.  40. 275. 215. 156. 190. 225. 245. 172.\n",
      " 235. 102.   6. 122. 265. 295. 450.  46.  86. 196.  84. 112.  10. 500.\n",
      "  20.  58.  34. 500.  14.  82. 104.  92. 154. 188. 285. 100. 120.  34.\n",
      " 500.  16. 132.  46.  12. 136.  22.   1. 100.  90.  58.  38. 270. 112.\n",
      " 215.  32.  26. 124.  16. 144.  90. 182. 300. 134. 215. 102. 450. 128.\n",
      "  94. 138.  72. 146. 158.   0. 112.  40. 178.  98.  32. 136. 400.   4.\n",
      " 112.  28. 106.  28.  74. 200. 192.  96.  58.  32. 140. 172.  72.  40.\n",
      " 200. 156. 162. 142. 134.  76. 255. 240.  58. 280. 124. 128.  28. 136.\n",
      " 275.  26.  16. 128. 134.  64. 225.  32. 186. 220.  52. 154. 184.  54.\n",
      " 114.  18. 220.  94. 130.  60. 295.  74.  60. 164.  10. 198. 138. 200.\n",
      " 128.  74. 350. 285. 102.   2.   4.  92.  86. 154. 190. 500. 186.  42.\n",
      "  62. 275. 110. 124. 116.  98. 275. 265.  28. 182. 198. 225. 170. 154.\n",
      "  86. 235. 300. 240.  84.  36. 150.  14. 300. 250. 164. 160.  78.  22.\n",
      " 300. 230. 112. 118. 118. 250. 178. 500.   0.  90. 120.  80. 186.  90.\n",
      " 450.  74.  64.  24. 140.  36. 215. 450. 350. 136.  68. 290. 220. 116.\n",
      " 300. 290.   8. 120.  48. 188. 164.  86.]\n",
      "mse =  775.8554701296968\n"
     ]
    }
   ],
   "source": [
    "pic = X_train[1]\n",
    "model = Sequential()\n",
    "#model.add(Input(shape=pic.shape))\n",
    "# model.add(Conv2D(filters=4, kernel_size=(7,7), strides=1, input_shape=pic.shape))\n",
    "Kerasmodel = keras.applications.resnet50.ResNet50(include_top=True\n",
    "                , weights=None#'imagenet'\n",
    "                # , input_tensor=inputs\n",
    "                , input_shape=pic.shape\n",
    "                )\n",
    "# Kerasmodel._layers.pop()\n",
    "# Kerasmodel.layers.pop()\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(1,kernel_initializer='he_normal'))\n",
    "model.add(Activation('linear')) #softmax\n",
    "model = Model( inputs=Kerasmodel.input , outputs=model(Kerasmodel.layers[-2].output))\n",
    "model.compile(loss='mean_squared_error', #mse binary_crossentropy Dice-coefficient loss function vs cross-entropy\n",
    "                optimizer=optimizers.Adam(lr=5e-3),\n",
    "                metrics=['mse'])\n",
    "\n",
    "model.summary()  \n",
    "time_str = str(time.time())\n",
    "print(time_str)\n",
    "reinitLayers(model)\n",
    "early_stop = EarlyStopping(monitor='val_mean_squared_error', patience=30)\n",
    "# model.load_weights('./modelWights/regression_model'+model_fn.__name__+'.h5')\n",
    "model_checkpoint = ModelCheckpoint('./modelWights/weights_'+'resnet50_without_pre'+time_str+'_gray_scale.h5', monitor='val_loss', save_best_only=True)\n",
    "tensorboard = keras.callbacks.tensorboard_v1.TensorBoard(log_dir=\"~/data/projects_logs/water_logs/resnet50_without_pre_color_{}\".format(time_str), histogram_freq=2,write_grads=True)\n",
    "\n",
    "history = model.fit(X_train_g, y_train, batch_size=BATCH_SIZE, epochs=100, validation_data=(X_val_g, y_val), callbacks=[model_checkpoint,])\n",
    "# print(history.history)\n",
    "\n",
    "y_pred_ = model.predict(X_test_g, batch_size=1, verbose=1)\n",
    "\n",
    "time_str = str(time.time())\n",
    "y_pred_, y_test_ = post_training_data(y_test, y_pred_)\n",
    "predicted_file = 'regress_'+\"resnet50_without_pre\"+\"resnet50\"+time_str+'_without_pretrain_gray_scale.csv'\n",
    "save_result(\"./result/\"+predicted_file,y_pred_,y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mobilenet without pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 512, 512, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)    (None, 513, 513, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 256, 256, 32)      864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 256, 256, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (ReLU)            (None, 256, 256, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 256, 256, 32)      288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 256, 256, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (ReLU)        (None, 256, 256, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 256, 256, 64)      2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 256, 256, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (ReLU)        (None, 256, 256, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_pad_2 (ZeroPadding2D)   (None, 257, 257, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 128, 128, 64)      576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 128, 128, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (ReLU)        (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 128, 128, 128)     8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 128, 128, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (ReLU)        (None, 128, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 128, 128, 128)     1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 128, 128, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (ReLU)        (None, 128, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 128, 128, 128)     16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 128, 128, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (ReLU)        (None, 128, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv_pad_4 (ZeroPadding2D)   (None, 129, 129, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 64, 64, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 64, 64, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (ReLU)        (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 64, 64, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 64, 64, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (ReLU)        (None, 64, 64, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 64, 64, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 64, 64, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (ReLU)        (None, 64, 64, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 64, 64, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 64, 64, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (ReLU)        (None, 64, 64, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_6 (ZeroPadding2D)   (None, 65, 65, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 32, 32, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 32, 32, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (ReLU)        (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 32, 32, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (ReLU)        (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 32, 32, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (ReLU)        (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 32, 32, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (ReLU)        (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 32, 32, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (ReLU)        (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 32, 32, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (ReLU)        (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 32, 32, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (ReLU)        (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 32, 32, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (ReLU)        (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 32, 32, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (ReLU)       (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 32, 32, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (ReLU)       (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 32, 32, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (ReLU)       (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 32, 32, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (ReLU)       (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)  (None, 33, 33, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 16, 16, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 16, 16, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (ReLU)       (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 16, 16, 1024)      524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 16, 16, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (ReLU)       (None, 16, 16, 1024)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 16, 16, 1024)      9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 16, 16, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (ReLU)       (None, 16, 16, 1024)      0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 16, 16, 1024)      1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 16, 16, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (ReLU)       (None, 16, 16, 1024)      0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_preds (Conv2D)          (None, 1, 1, 1000)        1025000   \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "sequential_3 (Sequential)    (None, 1)                 1001      \n",
      "=================================================================\n",
      "Total params: 4,254,865\n",
      "Trainable params: 4,232,977\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pic = X_train_g[1]\n",
    "model = Sequential()\n",
    "#model.add(Input(shape=pic.shape))\n",
    "# model.add(Conv2D(filters=4, kernel_size=(7,7), strides=1, input_shape=pic.shape))\n",
    "Kerasmodel = keras.applications.mobilenet.MobileNet(include_top=True\n",
    "                , weights=None#'imagenet'\n",
    "                # , input_tensor=inputs\n",
    "                , input_shape=pic.shape\n",
    "                )\n",
    "# Kerasmodel._layers.pop()\n",
    "# Kerasmodel.layers.pop()\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(1,kernel_initializer='he_normal'))\n",
    "model.add(Activation('linear')) #softmax\n",
    "model = Model( inputs=Kerasmodel.input , outputs=model(Kerasmodel.layers[-2].output))\n",
    "model.compile(loss='mean_squared_error', #mse binary_crossentropy Dice-coefficient loss function vs cross-entropy\n",
    "                optimizer=optimizers.Adam(lr=5e-3),\n",
    "                metrics=['mse'])\n",
    "\n",
    "model.summary()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mobilenet gray scale without pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER:: input_3\n",
      "LAYER:: conv1_pad\n",
      "LAYER:: conv1\n",
      "reinitializing layer conv1.kernel\n",
      "LAYER:: conv1_bn\n",
      "reinitializing layer conv1_bn.gamma\n",
      "reinitializing layer conv1_bn.beta\n",
      "reinitializing layer conv1_bn.moving_mean\n",
      "reinitializing layer conv1_bn.moving_variance\n",
      "LAYER:: conv1_relu\n",
      "LAYER:: conv_dw_1\n",
      "reinitializing layer conv_dw_1.depthwise_kernel\n",
      "LAYER:: conv_dw_1_bn\n",
      "reinitializing layer conv_dw_1_bn.gamma\n",
      "reinitializing layer conv_dw_1_bn.beta\n",
      "reinitializing layer conv_dw_1_bn.moving_mean\n",
      "reinitializing layer conv_dw_1_bn.moving_variance\n",
      "LAYER:: conv_dw_1_relu\n",
      "LAYER:: conv_pw_1\n",
      "reinitializing layer conv_pw_1.kernel\n",
      "LAYER:: conv_pw_1_bn\n",
      "reinitializing layer conv_pw_1_bn.gamma\n",
      "reinitializing layer conv_pw_1_bn.beta\n",
      "reinitializing layer conv_pw_1_bn.moving_mean\n",
      "reinitializing layer conv_pw_1_bn.moving_variance\n",
      "LAYER:: conv_pw_1_relu\n",
      "LAYER:: conv_pad_2\n",
      "LAYER:: conv_dw_2\n",
      "reinitializing layer conv_dw_2.depthwise_kernel\n",
      "LAYER:: conv_dw_2_bn\n",
      "reinitializing layer conv_dw_2_bn.gamma\n",
      "reinitializing layer conv_dw_2_bn.beta\n",
      "reinitializing layer conv_dw_2_bn.moving_mean\n",
      "reinitializing layer conv_dw_2_bn.moving_variance\n",
      "LAYER:: conv_dw_2_relu\n",
      "LAYER:: conv_pw_2\n",
      "reinitializing layer conv_pw_2.kernel\n",
      "LAYER:: conv_pw_2_bn\n",
      "reinitializing layer conv_pw_2_bn.gamma\n",
      "reinitializing layer conv_pw_2_bn.beta\n",
      "reinitializing layer conv_pw_2_bn.moving_mean\n",
      "reinitializing layer conv_pw_2_bn.moving_variance\n",
      "LAYER:: conv_pw_2_relu\n",
      "LAYER:: conv_dw_3\n",
      "reinitializing layer conv_dw_3.depthwise_kernel\n",
      "LAYER:: conv_dw_3_bn\n",
      "reinitializing layer conv_dw_3_bn.gamma\n",
      "reinitializing layer conv_dw_3_bn.beta\n",
      "reinitializing layer conv_dw_3_bn.moving_mean\n",
      "reinitializing layer conv_dw_3_bn.moving_variance\n",
      "LAYER:: conv_dw_3_relu\n",
      "LAYER:: conv_pw_3\n",
      "reinitializing layer conv_pw_3.kernel\n",
      "LAYER:: conv_pw_3_bn\n",
      "reinitializing layer conv_pw_3_bn.gamma\n",
      "reinitializing layer conv_pw_3_bn.beta\n",
      "reinitializing layer conv_pw_3_bn.moving_mean\n",
      "reinitializing layer conv_pw_3_bn.moving_variance\n",
      "LAYER:: conv_pw_3_relu\n",
      "LAYER:: conv_pad_4\n",
      "LAYER:: conv_dw_4\n",
      "reinitializing layer conv_dw_4.depthwise_kernel\n",
      "LAYER:: conv_dw_4_bn\n",
      "reinitializing layer conv_dw_4_bn.gamma\n",
      "reinitializing layer conv_dw_4_bn.beta\n",
      "reinitializing layer conv_dw_4_bn.moving_mean\n",
      "reinitializing layer conv_dw_4_bn.moving_variance\n",
      "LAYER:: conv_dw_4_relu\n",
      "LAYER:: conv_pw_4\n",
      "reinitializing layer conv_pw_4.kernel\n",
      "LAYER:: conv_pw_4_bn\n",
      "reinitializing layer conv_pw_4_bn.gamma\n",
      "reinitializing layer conv_pw_4_bn.beta\n",
      "reinitializing layer conv_pw_4_bn.moving_mean\n",
      "reinitializing layer conv_pw_4_bn.moving_variance\n",
      "LAYER:: conv_pw_4_relu\n",
      "LAYER:: conv_dw_5\n",
      "reinitializing layer conv_dw_5.depthwise_kernel\n",
      "LAYER:: conv_dw_5_bn\n",
      "reinitializing layer conv_dw_5_bn.gamma\n",
      "reinitializing layer conv_dw_5_bn.beta\n",
      "reinitializing layer conv_dw_5_bn.moving_mean\n",
      "reinitializing layer conv_dw_5_bn.moving_variance\n",
      "LAYER:: conv_dw_5_relu\n",
      "LAYER:: conv_pw_5\n",
      "reinitializing layer conv_pw_5.kernel\n",
      "LAYER:: conv_pw_5_bn\n",
      "reinitializing layer conv_pw_5_bn.gamma\n",
      "reinitializing layer conv_pw_5_bn.beta\n",
      "reinitializing layer conv_pw_5_bn.moving_mean\n",
      "reinitializing layer conv_pw_5_bn.moving_variance\n",
      "LAYER:: conv_pw_5_relu\n",
      "LAYER:: conv_pad_6\n",
      "LAYER:: conv_dw_6\n",
      "reinitializing layer conv_dw_6.depthwise_kernel\n",
      "LAYER:: conv_dw_6_bn\n",
      "reinitializing layer conv_dw_6_bn.gamma\n",
      "reinitializing layer conv_dw_6_bn.beta\n",
      "reinitializing layer conv_dw_6_bn.moving_mean\n",
      "reinitializing layer conv_dw_6_bn.moving_variance\n",
      "LAYER:: conv_dw_6_relu\n",
      "LAYER:: conv_pw_6\n",
      "reinitializing layer conv_pw_6.kernel\n",
      "LAYER:: conv_pw_6_bn\n",
      "reinitializing layer conv_pw_6_bn.gamma\n",
      "reinitializing layer conv_pw_6_bn.beta\n",
      "reinitializing layer conv_pw_6_bn.moving_mean\n",
      "reinitializing layer conv_pw_6_bn.moving_variance\n",
      "LAYER:: conv_pw_6_relu\n",
      "LAYER:: conv_dw_7\n",
      "reinitializing layer conv_dw_7.depthwise_kernel\n",
      "LAYER:: conv_dw_7_bn\n",
      "reinitializing layer conv_dw_7_bn.gamma\n",
      "reinitializing layer conv_dw_7_bn.beta\n",
      "reinitializing layer conv_dw_7_bn.moving_mean\n",
      "reinitializing layer conv_dw_7_bn.moving_variance\n",
      "LAYER:: conv_dw_7_relu\n",
      "LAYER:: conv_pw_7\n",
      "reinitializing layer conv_pw_7.kernel\n",
      "LAYER:: conv_pw_7_bn\n",
      "reinitializing layer conv_pw_7_bn.gamma\n",
      "reinitializing layer conv_pw_7_bn.beta\n",
      "reinitializing layer conv_pw_7_bn.moving_mean\n",
      "reinitializing layer conv_pw_7_bn.moving_variance\n",
      "LAYER:: conv_pw_7_relu\n",
      "LAYER:: conv_dw_8\n",
      "reinitializing layer conv_dw_8.depthwise_kernel\n",
      "LAYER:: conv_dw_8_bn\n",
      "reinitializing layer conv_dw_8_bn.gamma\n",
      "reinitializing layer conv_dw_8_bn.beta\n",
      "reinitializing layer conv_dw_8_bn.moving_mean\n",
      "reinitializing layer conv_dw_8_bn.moving_variance\n",
      "LAYER:: conv_dw_8_relu\n",
      "LAYER:: conv_pw_8\n",
      "reinitializing layer conv_pw_8.kernel\n",
      "LAYER:: conv_pw_8_bn\n",
      "reinitializing layer conv_pw_8_bn.gamma\n",
      "reinitializing layer conv_pw_8_bn.beta\n",
      "reinitializing layer conv_pw_8_bn.moving_mean\n",
      "reinitializing layer conv_pw_8_bn.moving_variance\n",
      "LAYER:: conv_pw_8_relu\n",
      "LAYER:: conv_dw_9\n",
      "reinitializing layer conv_dw_9.depthwise_kernel\n",
      "LAYER:: conv_dw_9_bn\n",
      "reinitializing layer conv_dw_9_bn.gamma\n",
      "reinitializing layer conv_dw_9_bn.beta\n",
      "reinitializing layer conv_dw_9_bn.moving_mean\n",
      "reinitializing layer conv_dw_9_bn.moving_variance\n",
      "LAYER:: conv_dw_9_relu\n",
      "LAYER:: conv_pw_9\n",
      "reinitializing layer conv_pw_9.kernel\n",
      "LAYER:: conv_pw_9_bn\n",
      "reinitializing layer conv_pw_9_bn.gamma\n",
      "reinitializing layer conv_pw_9_bn.beta\n",
      "reinitializing layer conv_pw_9_bn.moving_mean\n",
      "reinitializing layer conv_pw_9_bn.moving_variance\n",
      "LAYER:: conv_pw_9_relu\n",
      "LAYER:: conv_dw_10\n",
      "reinitializing layer conv_dw_10.depthwise_kernel\n",
      "LAYER:: conv_dw_10_bn\n",
      "reinitializing layer conv_dw_10_bn.gamma\n",
      "reinitializing layer conv_dw_10_bn.beta\n",
      "reinitializing layer conv_dw_10_bn.moving_mean\n",
      "reinitializing layer conv_dw_10_bn.moving_variance\n",
      "LAYER:: conv_dw_10_relu\n",
      "LAYER:: conv_pw_10\n",
      "reinitializing layer conv_pw_10.kernel\n",
      "LAYER:: conv_pw_10_bn\n",
      "reinitializing layer conv_pw_10_bn.gamma\n",
      "reinitializing layer conv_pw_10_bn.beta\n",
      "reinitializing layer conv_pw_10_bn.moving_mean\n",
      "reinitializing layer conv_pw_10_bn.moving_variance\n",
      "LAYER:: conv_pw_10_relu\n",
      "LAYER:: conv_dw_11\n",
      "reinitializing layer conv_dw_11.depthwise_kernel\n",
      "LAYER:: conv_dw_11_bn\n",
      "reinitializing layer conv_dw_11_bn.gamma\n",
      "reinitializing layer conv_dw_11_bn.beta\n",
      "reinitializing layer conv_dw_11_bn.moving_mean\n",
      "reinitializing layer conv_dw_11_bn.moving_variance\n",
      "LAYER:: conv_dw_11_relu\n",
      "LAYER:: conv_pw_11\n",
      "reinitializing layer conv_pw_11.kernel\n",
      "LAYER:: conv_pw_11_bn\n",
      "reinitializing layer conv_pw_11_bn.gamma\n",
      "reinitializing layer conv_pw_11_bn.beta\n",
      "reinitializing layer conv_pw_11_bn.moving_mean\n",
      "reinitializing layer conv_pw_11_bn.moving_variance\n",
      "LAYER:: conv_pw_11_relu\n",
      "LAYER:: conv_pad_12\n",
      "LAYER:: conv_dw_12\n",
      "reinitializing layer conv_dw_12.depthwise_kernel\n",
      "LAYER:: conv_dw_12_bn\n",
      "reinitializing layer conv_dw_12_bn.gamma\n",
      "reinitializing layer conv_dw_12_bn.beta\n",
      "reinitializing layer conv_dw_12_bn.moving_mean\n",
      "reinitializing layer conv_dw_12_bn.moving_variance\n",
      "LAYER:: conv_dw_12_relu\n",
      "LAYER:: conv_pw_12\n",
      "reinitializing layer conv_pw_12.kernel\n",
      "LAYER:: conv_pw_12_bn\n",
      "reinitializing layer conv_pw_12_bn.gamma\n",
      "reinitializing layer conv_pw_12_bn.beta\n",
      "reinitializing layer conv_pw_12_bn.moving_mean\n",
      "reinitializing layer conv_pw_12_bn.moving_variance\n",
      "LAYER:: conv_pw_12_relu\n",
      "LAYER:: conv_dw_13\n",
      "reinitializing layer conv_dw_13.depthwise_kernel\n",
      "LAYER:: conv_dw_13_bn\n",
      "reinitializing layer conv_dw_13_bn.gamma\n",
      "reinitializing layer conv_dw_13_bn.beta\n",
      "reinitializing layer conv_dw_13_bn.moving_mean\n",
      "reinitializing layer conv_dw_13_bn.moving_variance\n",
      "LAYER:: conv_dw_13_relu\n",
      "LAYER:: conv_pw_13\n",
      "reinitializing layer conv_pw_13.kernel\n",
      "LAYER:: conv_pw_13_bn\n",
      "reinitializing layer conv_pw_13_bn.gamma\n",
      "reinitializing layer conv_pw_13_bn.beta\n",
      "reinitializing layer conv_pw_13_bn.moving_mean\n",
      "reinitializing layer conv_pw_13_bn.moving_variance\n",
      "LAYER:: conv_pw_13_relu\n",
      "LAYER:: global_average_pooling2d_2\n",
      "LAYER:: reshape_1\n",
      "LAYER:: dropout\n",
      "LAYER:: conv_preds\n",
      "reinitializing layer conv_preds.kernel\n",
      "reinitializing layer conv_preds.bias\n",
      "LAYER:: reshape_2\n",
      "LAYER:: dense_3\n",
      "reinitializing layer dense_3.kernel\n",
      "reinitializing layer dense_3.bias\n",
      "LAYER:: activation_52\n",
      "Train on 1053 samples, validate on 264 samples\n",
      "Epoch 1/200\n",
      "1053/1053 [==============================] - 33s 31ms/step - loss: 34.7882 - mse: 34.7882 - val_loss: 0.0669 - val_mse: 0.0669\n",
      "Epoch 2/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0293 - mse: 0.0293 - val_loss: 0.0556 - val_mse: 0.0556\n",
      "Epoch 3/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0287 - mse: 0.0287 - val_loss: 0.0155 - val_mse: 0.0155\n",
      "Epoch 4/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0178 - mse: 0.0178 - val_loss: 0.0188 - val_mse: 0.0188\n",
      "Epoch 5/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 6/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0142 - mse: 0.0142 - val_loss: 0.0271 - val_mse: 0.0271\n",
      "Epoch 7/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0048 - val_mse: 0.0048\n",
      "Epoch 8/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.0189 - val_mse: 0.0189\n",
      "Epoch 9/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 10/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 11/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 12/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 13/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0218 - val_mse: 0.0218\n",
      "Epoch 14/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 15/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 16/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0288 - val_mse: 0.0288\n",
      "Epoch 17/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 18/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 19/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 20/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 21/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0049 - val_mse: 0.0049\n",
      "Epoch 22/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 23/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 24/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 25/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 26/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 27/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0153 - val_mse: 0.0153\n",
      "Epoch 28/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 29/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 30/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 31/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 32/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.1040 - val_mse: 0.1040\n",
      "Epoch 33/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0053 - val_mse: 0.0053\n",
      "Epoch 34/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0050 - val_mse: 0.0050\n",
      "Epoch 35/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 36/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0071 - val_mse: 0.0071\n",
      "Epoch 37/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0058 - val_mse: 0.0058\n",
      "Epoch 38/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 39/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 40/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 41/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 42/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 43/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 44/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 45/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0158 - val_mse: 0.0158\n",
      "Epoch 46/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 47/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 48/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.7866 - mse: 0.7866 - val_loss: 0.2906 - val_mse: 0.2906\n",
      "Epoch 49/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0485 - mse: 0.0485 - val_loss: 0.0575 - val_mse: 0.0575\n",
      "Epoch 50/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.0274 - val_mse: 0.0274\n",
      "Epoch 51/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0235 - mse: 0.0235 - val_loss: 0.1018 - val_mse: 0.1018\n",
      "Epoch 52/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.0225 - val_mse: 0.0225\n",
      "Epoch 53/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 54/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0237 - val_mse: 0.0237\n",
      "Epoch 55/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0154 - mse: 0.0154 - val_loss: 0.0182 - val_mse: 0.0182\n",
      "Epoch 56/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 57/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 58/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.1313 - val_mse: 0.1313\n",
      "Epoch 59/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 60/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.0150 - val_mse: 0.0150\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 62/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 63/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0057 - val_mse: 0.0057\n",
      "Epoch 64/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 65/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 66/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 67/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 68/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0177 - val_mse: 0.0177\n",
      "Epoch 69/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 70/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0159 - mse: 0.0159 - val_loss: 0.0220 - val_mse: 0.0220\n",
      "Epoch 71/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0148 - val_mse: 0.0148\n",
      "Epoch 72/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 73/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 74/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 75/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 76/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 77/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0208 - val_mse: 0.0208\n",
      "Epoch 78/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0061 - mse: 0.0061 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 79/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0101 - val_mse: 0.0101\n",
      "Epoch 80/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0069 - val_mse: 0.0069\n",
      "Epoch 81/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 82/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0208 - val_mse: 0.0208\n",
      "Epoch 83/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 84/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 85/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 86/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 87/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0247 - val_mse: 0.0247\n",
      "Epoch 88/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0299 - val_mse: 0.0299\n",
      "Epoch 89/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 90/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0194 - val_mse: 0.0194\n",
      "Epoch 91/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 92/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0301 - val_mse: 0.0301\n",
      "Epoch 93/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 94/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 95/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0125 - val_mse: 0.0125\n",
      "Epoch 96/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0061 - mse: 0.0061 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 97/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0054 - val_mse: 0.0054\n",
      "Epoch 98/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 99/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0317 - val_mse: 0.0317\n",
      "Epoch 100/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0176 - val_mse: 0.0176\n",
      "Epoch 101/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0070 - val_mse: 0.0070\n",
      "Epoch 102/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 103/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0061 - mse: 0.0061 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 104/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 105/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0134 - val_mse: 0.0134\n",
      "Epoch 106/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 107/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0068 - val_mse: 0.0068\n",
      "Epoch 108/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0049 - val_mse: 0.0049\n",
      "Epoch 109/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0071 - val_mse: 0.0071\n",
      "Epoch 110/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 111/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 112/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0050 - val_mse: 0.0050\n",
      "Epoch 113/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 114/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0056 - val_mse: 0.0056\n",
      "Epoch 115/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 116/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0188 - val_mse: 0.0188\n",
      "Epoch 117/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 118/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 119/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0047 - val_mse: 0.0047\n",
      "Epoch 120/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 121/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 122/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 123/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 124/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 125/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 126/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0050 - val_mse: 0.0050\n",
      "Epoch 127/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 128/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0165 - val_mse: 0.0165\n",
      "Epoch 129/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 130/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0284 - val_mse: 0.0284\n",
      "Epoch 131/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0183 - val_mse: 0.0183\n",
      "Epoch 132/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 133/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 134/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 135/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 136/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 137/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 138/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 139/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 140/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 141/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 142/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 143/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0102 - val_mse: 0.0102\n",
      "Epoch 144/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 145/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0124 - val_mse: 0.0124\n",
      "Epoch 146/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0170 - val_mse: 0.0170\n",
      "Epoch 147/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 148/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 149/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 150/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 151/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 152/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 153/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 154/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 155/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 156/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 157/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 158/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 159/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 160/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 161/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 162/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 163/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 164/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 165/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 166/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 167/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 168/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 169/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0049 - val_mse: 0.0049\n",
      "Epoch 170/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0133 - val_mse: 0.0133\n",
      "Epoch 171/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 172/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 173/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 174/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 175/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 176/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 177/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 178/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 180/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 181/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 182/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0204 - val_mse: 0.0204\n",
      "Epoch 183/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 184/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0047 - val_mse: 0.0047\n",
      "Epoch 185/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0064 - val_mse: 0.0064\n",
      "Epoch 186/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 187/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 188/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 189/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 190/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 191/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 192/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 193/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 194/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 195/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0101 - val_mse: 0.0101\n",
      "Epoch 196/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 197/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 198/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0057 - val_mse: 0.0057\n",
      "Epoch 199/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0238 - val_mse: 0.0238\n",
      "Epoch 200/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "330/330 [==============================] - 4s 12ms/step\n",
      "(330,)\n",
      "(330,)\n",
      "[ 13.716325    40.376575   215.48906    212.32874    439.97815\n",
      "  99.81547    220.99689     62.856316    73.7086     147.71582\n",
      " 212.6044      61.669075    88.15581     51.826275    72.03511\n",
      "  40.3926     118.62931    119.48449    238.01634    146.70683\n",
      " 448.55984    208.18309    217.86972     81.07273     88.44508\n",
      "  50.885544    72.58536     -1.894325   103.49889    126.19486\n",
      " 177.61856     75.35821     62.36569     37.845284    88.76503\n",
      " 144.69759    239.49113     12.8586445   45.399227   394.21295\n",
      " 221.78996    247.39859    106.611404   199.62567    253.97473\n",
      " 267.12094     32.35627     22.815317    83.76322    129.02866\n",
      " -11.411473    90.5744     122.0201      11.173889   143.26003\n",
      " 160.29948     98.52572    200.3572     151.74466     -3.0626059\n",
      " 196.82611    266.87378     56.427307   430.61447    204.38934\n",
      " 195.29323    184.51625    178.44014    183.80269     49.933598\n",
      " 190.0368      -4.443124   134.77316    119.68492     34.576683\n",
      " 167.82387     95.66267    110.119545    75.33733     -2.4320483\n",
      " 100.54704    327.45386     35.715244     2.2681952    1.3414323\n",
      "  52.771107    60.911842    76.859375   100.83557     79.48233\n",
      " 207.90686     16.972065   149.48341    105.942665   215.85887\n",
      " 199.55013     97.696396   167.82387     24.028196    67.402695\n",
      " 143.14557     63.53286     35.808952    -3.2333434   45.194374\n",
      "  59.49445     89.07733    164.7054      87.91457     -4.0684195\n",
      "  -0.75797737 110.264145   115.66376     20.519077    82.455536\n",
      " 344.1866      44.677017   222.84055     49.528614   242.17462\n",
      " 202.10742    127.18579    137.68222    176.99663    195.64296\n",
      " 111.158936   196.22421     71.57695      3.0623822  109.86678\n",
      " 217.03317    211.64824    403.39465     62.44339     52.223885\n",
      " 170.3091      67.148926    99.92233     32.250492   437.22858\n",
      "  49.020626    58.926746    56.602463   424.69315      1.5218705\n",
      "  90.646324    69.10512     60.170128   116.12551    136.55371\n",
      " 231.19485     63.90409     71.15178     64.14427    421.98874\n",
      "  -1.3813078  124.42767     57.081333    10.656923   100.297035\n",
      "  15.322134    78.59175     63.570984    79.6187      42.15525\n",
      "  57.960606   186.98433    105.148155   191.4581      36.5054\n",
      "   5.792871   122.675804    30.217096    84.42783    102.67073\n",
      " 177.65233    238.54411     83.37132    226.93134     70.57252\n",
      " 420.01337    117.17199     79.4977     114.090324    95.91211\n",
      " 110.348656   134.3176      10.090649   118.62534     42.25953\n",
      " 172.57521     87.0246      17.879993   117.09026    357.54483\n",
      "   0.9238571   69.909096    44.41531     86.01456     23.098707\n",
      "  91.70775    136.63152    107.83471     98.79476     63.300453\n",
      "  46.050594   108.673615   137.06573     82.49592     57.38814\n",
      " 145.52214    104.82219    116.230804    61.128124    92.28396\n",
      "  99.99569    174.55705    208.19432     73.428276   225.70122\n",
      "  66.72681    121.82669     51.80333     89.79094    192.8286\n",
      "  37.452503   -11.695653    84.90897    112.67943     76.5265\n",
      " 189.3653      61.485188   147.64825    165.70218     61.08544\n",
      " 102.59962    129.01599     48.591873    89.318436     8.718327\n",
      " 154.92215     82.63903    114.26599     81.979416   233.0231\n",
      "  39.883553    77.67122    100.211716    -4.930541   129.96063\n",
      " 126.136955   140.66505     72.8794      56.662865   300.20047\n",
      " 230.4726      76.73545      2.6415734    1.6855001   71.82762\n",
      "  86.303055    99.7326     146.7851     441.5133     152.82236\n",
      "  60.985645    80.769325   252.49284    117.205025    84.57734\n",
      " 104.489876    66.59495    203.89397    218.25371     50.319115\n",
      " 127.451744   156.45123    176.99663    160.45294    120.08847\n",
      " 101.94572    174.45706    245.49196    186.91798     47.048084\n",
      "  38.90562     93.41861      5.865559   231.8193     223.09381\n",
      " 140.74966     95.588806    61.291435    23.882107   249.58151\n",
      " 186.15298     88.69635     59.90878    116.49711    201.10222\n",
      " 107.31396    440.7998       9.381384    85.28544    110.515465\n",
      "  76.54632    134.57036     74.40719    398.92722     63.253723\n",
      "  32.625744    19.442902   162.7903      29.35028    171.64194\n",
      " 436.6658     361.25543     96.12376     77.6285     209.18092\n",
      " 211.40245     76.505905   239.78172    250.39381     -5.803332\n",
      "  98.737816    17.067015   140.03847    165.06577     56.090473  ]\n",
      "[  6.  54. 275. 240. 450. 108. 225.  58. 150. 162. 250.  80. 162.  64.\n",
      " 100.  46. 174. 128. 250. 184. 500. 300. 285.  68.  88.  30. 120.   8.\n",
      " 128. 154. 144. 126.  80.  52. 112. 158. 300.  18.  66. 450. 265. 290.\n",
      " 168. 255. 295. 300.  32.  24.  96. 132.   6.  92. 186.   8. 180. 196.\n",
      " 108. 230. 225.   8. 215. 290.  54. 450. 250. 270. 230. 200. 225.  80.\n",
      " 245.   4. 190. 168.  44. 225. 182. 122. 120.   4. 124. 350.  56.   2.\n",
      "  12.  84.  84.  78.  84.  82. 225.  18. 176. 174. 198. 225. 100. 225.\n",
      "  46.  60. 188.  88.  70.  10.  24.  48. 172. 186.  52.  10.   2. 188.\n",
      " 156.  48. 116. 400.  30. 275.  40. 275. 215. 156. 190. 225. 245. 172.\n",
      " 235. 102.   6. 122. 265. 295. 450.  46.  86. 196.  84. 112.  10. 500.\n",
      "  20.  58.  34. 500.  14.  82. 104.  92. 154. 188. 285. 100. 120.  34.\n",
      " 500.  16. 132.  46.  12. 136.  22.   1. 100.  90.  58.  38. 270. 112.\n",
      " 215.  32.  26. 124.  16. 144.  90. 182. 300. 134. 215. 102. 450. 128.\n",
      "  94. 138.  72. 146. 158.   0. 112.  40. 178.  98.  32. 136. 400.   4.\n",
      " 112.  28. 106.  28.  74. 200. 192.  96.  58.  32. 140. 172.  72.  40.\n",
      " 200. 156. 162. 142. 134.  76. 255. 240.  58. 280. 124. 128.  28. 136.\n",
      " 275.  26.  16. 128. 134.  64. 225.  32. 186. 220.  52. 154. 184.  54.\n",
      " 114.  18. 220.  94. 130.  60. 295.  74.  60. 164.  10. 198. 138. 200.\n",
      " 128.  74. 350. 285. 102.   2.   4.  92.  86. 154. 190. 500. 186.  42.\n",
      "  62. 275. 110. 124. 116.  98. 275. 265.  28. 182. 198. 225. 170. 154.\n",
      "  86. 235. 300. 240.  84.  36. 150.  14. 300. 250. 164. 160.  78.  22.\n",
      " 300. 230. 112. 118. 118. 250. 178. 500.   0.  90. 120.  80. 186.  90.\n",
      " 450.  74.  64.  24. 140.  36. 215. 450. 350. 136.  68. 290. 220. 116.\n",
      " 300. 290.   8. 120.  48. 188. 164.  86.]\n",
      "mse =  1410.784387384384\n"
     ]
    }
   ],
   "source": [
    "time_str = str(time.time())\n",
    "reinitLayers(model)\n",
    "early_stop = EarlyStopping(monitor='val_mean_squared_error', patience=30)\n",
    "# model.load_weights('./modelWights/regression_model'+model_fn.__name__+'.h5')\n",
    "model_checkpoint = ModelCheckpoint('./modelWights/weights_'+'mobileNet_without_pre'+time_str+'_gray_scale.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train_g, y_train, batch_size=BATCH_SIZE, epochs=200, validation_data=(X_val_g, y_val), callbacks=[model_checkpoint])\n",
    "# print(history.history)\n",
    "\n",
    "y_pred_ = model.predict(X_test_g, batch_size=1, verbose=1)\n",
    "\n",
    "\n",
    "y_pred_, y_test_ = post_training_data(y_test, y_pred_)\n",
    "predicted_file = 'regress_'+\"mobileNet_without_pre\"+time_str+'_gray_scale.csv'\n",
    "save_result(\"./result/\"+predicted_file,y_pred_,y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8828690727800078"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test_, y_pred_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER:: input_3\n",
      "LAYER:: conv1_pad\n",
      "LAYER:: conv1\n",
      "reinitializing layer conv1.kernel\n",
      "LAYER:: conv1_bn\n",
      "reinitializing layer conv1_bn.gamma\n",
      "reinitializing layer conv1_bn.beta\n",
      "reinitializing layer conv1_bn.moving_mean\n",
      "reinitializing layer conv1_bn.moving_variance\n",
      "LAYER:: conv1_relu\n",
      "LAYER:: conv_dw_1\n",
      "reinitializing layer conv_dw_1.depthwise_kernel\n",
      "LAYER:: conv_dw_1_bn\n",
      "reinitializing layer conv_dw_1_bn.gamma\n",
      "reinitializing layer conv_dw_1_bn.beta\n",
      "reinitializing layer conv_dw_1_bn.moving_mean\n",
      "reinitializing layer conv_dw_1_bn.moving_variance\n",
      "LAYER:: conv_dw_1_relu\n",
      "LAYER:: conv_pw_1\n",
      "reinitializing layer conv_pw_1.kernel\n",
      "LAYER:: conv_pw_1_bn\n",
      "reinitializing layer conv_pw_1_bn.gamma\n",
      "reinitializing layer conv_pw_1_bn.beta\n",
      "reinitializing layer conv_pw_1_bn.moving_mean\n",
      "reinitializing layer conv_pw_1_bn.moving_variance\n",
      "LAYER:: conv_pw_1_relu\n",
      "LAYER:: conv_pad_2\n",
      "LAYER:: conv_dw_2\n",
      "reinitializing layer conv_dw_2.depthwise_kernel\n",
      "LAYER:: conv_dw_2_bn\n",
      "reinitializing layer conv_dw_2_bn.gamma\n",
      "reinitializing layer conv_dw_2_bn.beta\n",
      "reinitializing layer conv_dw_2_bn.moving_mean\n",
      "reinitializing layer conv_dw_2_bn.moving_variance\n",
      "LAYER:: conv_dw_2_relu\n",
      "LAYER:: conv_pw_2\n",
      "reinitializing layer conv_pw_2.kernel\n",
      "LAYER:: conv_pw_2_bn\n",
      "reinitializing layer conv_pw_2_bn.gamma\n",
      "reinitializing layer conv_pw_2_bn.beta\n",
      "reinitializing layer conv_pw_2_bn.moving_mean\n",
      "reinitializing layer conv_pw_2_bn.moving_variance\n",
      "LAYER:: conv_pw_2_relu\n",
      "LAYER:: conv_dw_3\n",
      "reinitializing layer conv_dw_3.depthwise_kernel\n",
      "LAYER:: conv_dw_3_bn\n",
      "reinitializing layer conv_dw_3_bn.gamma\n",
      "reinitializing layer conv_dw_3_bn.beta\n",
      "reinitializing layer conv_dw_3_bn.moving_mean\n",
      "reinitializing layer conv_dw_3_bn.moving_variance\n",
      "LAYER:: conv_dw_3_relu\n",
      "LAYER:: conv_pw_3\n",
      "reinitializing layer conv_pw_3.kernel\n",
      "LAYER:: conv_pw_3_bn\n",
      "reinitializing layer conv_pw_3_bn.gamma\n",
      "reinitializing layer conv_pw_3_bn.beta\n",
      "reinitializing layer conv_pw_3_bn.moving_mean\n",
      "reinitializing layer conv_pw_3_bn.moving_variance\n",
      "LAYER:: conv_pw_3_relu\n",
      "LAYER:: conv_pad_4\n",
      "LAYER:: conv_dw_4\n",
      "reinitializing layer conv_dw_4.depthwise_kernel\n",
      "LAYER:: conv_dw_4_bn\n",
      "reinitializing layer conv_dw_4_bn.gamma\n",
      "reinitializing layer conv_dw_4_bn.beta\n",
      "reinitializing layer conv_dw_4_bn.moving_mean\n",
      "reinitializing layer conv_dw_4_bn.moving_variance\n",
      "LAYER:: conv_dw_4_relu\n",
      "LAYER:: conv_pw_4\n",
      "reinitializing layer conv_pw_4.kernel\n",
      "LAYER:: conv_pw_4_bn\n",
      "reinitializing layer conv_pw_4_bn.gamma\n",
      "reinitializing layer conv_pw_4_bn.beta\n",
      "reinitializing layer conv_pw_4_bn.moving_mean\n",
      "reinitializing layer conv_pw_4_bn.moving_variance\n",
      "LAYER:: conv_pw_4_relu\n",
      "LAYER:: conv_dw_5\n",
      "reinitializing layer conv_dw_5.depthwise_kernel\n",
      "LAYER:: conv_dw_5_bn\n",
      "reinitializing layer conv_dw_5_bn.gamma\n",
      "reinitializing layer conv_dw_5_bn.beta\n",
      "reinitializing layer conv_dw_5_bn.moving_mean\n",
      "reinitializing layer conv_dw_5_bn.moving_variance\n",
      "LAYER:: conv_dw_5_relu\n",
      "LAYER:: conv_pw_5\n",
      "reinitializing layer conv_pw_5.kernel\n",
      "LAYER:: conv_pw_5_bn\n",
      "reinitializing layer conv_pw_5_bn.gamma\n",
      "reinitializing layer conv_pw_5_bn.beta\n",
      "reinitializing layer conv_pw_5_bn.moving_mean\n",
      "reinitializing layer conv_pw_5_bn.moving_variance\n",
      "LAYER:: conv_pw_5_relu\n",
      "LAYER:: conv_pad_6\n",
      "LAYER:: conv_dw_6\n",
      "reinitializing layer conv_dw_6.depthwise_kernel\n",
      "LAYER:: conv_dw_6_bn\n",
      "reinitializing layer conv_dw_6_bn.gamma\n",
      "reinitializing layer conv_dw_6_bn.beta\n",
      "reinitializing layer conv_dw_6_bn.moving_mean\n",
      "reinitializing layer conv_dw_6_bn.moving_variance\n",
      "LAYER:: conv_dw_6_relu\n",
      "LAYER:: conv_pw_6\n",
      "reinitializing layer conv_pw_6.kernel\n",
      "LAYER:: conv_pw_6_bn\n",
      "reinitializing layer conv_pw_6_bn.gamma\n",
      "reinitializing layer conv_pw_6_bn.beta\n",
      "reinitializing layer conv_pw_6_bn.moving_mean\n",
      "reinitializing layer conv_pw_6_bn.moving_variance\n",
      "LAYER:: conv_pw_6_relu\n",
      "LAYER:: conv_dw_7\n",
      "reinitializing layer conv_dw_7.depthwise_kernel\n",
      "LAYER:: conv_dw_7_bn\n",
      "reinitializing layer conv_dw_7_bn.gamma\n",
      "reinitializing layer conv_dw_7_bn.beta\n",
      "reinitializing layer conv_dw_7_bn.moving_mean\n",
      "reinitializing layer conv_dw_7_bn.moving_variance\n",
      "LAYER:: conv_dw_7_relu\n",
      "LAYER:: conv_pw_7\n",
      "reinitializing layer conv_pw_7.kernel\n",
      "LAYER:: conv_pw_7_bn\n",
      "reinitializing layer conv_pw_7_bn.gamma\n",
      "reinitializing layer conv_pw_7_bn.beta\n",
      "reinitializing layer conv_pw_7_bn.moving_mean\n",
      "reinitializing layer conv_pw_7_bn.moving_variance\n",
      "LAYER:: conv_pw_7_relu\n",
      "LAYER:: conv_dw_8\n",
      "reinitializing layer conv_dw_8.depthwise_kernel\n",
      "LAYER:: conv_dw_8_bn\n",
      "reinitializing layer conv_dw_8_bn.gamma\n",
      "reinitializing layer conv_dw_8_bn.beta\n",
      "reinitializing layer conv_dw_8_bn.moving_mean\n",
      "reinitializing layer conv_dw_8_bn.moving_variance\n",
      "LAYER:: conv_dw_8_relu\n",
      "LAYER:: conv_pw_8\n",
      "reinitializing layer conv_pw_8.kernel\n",
      "LAYER:: conv_pw_8_bn\n",
      "reinitializing layer conv_pw_8_bn.gamma\n",
      "reinitializing layer conv_pw_8_bn.beta\n",
      "reinitializing layer conv_pw_8_bn.moving_mean\n",
      "reinitializing layer conv_pw_8_bn.moving_variance\n",
      "LAYER:: conv_pw_8_relu\n",
      "LAYER:: conv_dw_9\n",
      "reinitializing layer conv_dw_9.depthwise_kernel\n",
      "LAYER:: conv_dw_9_bn\n",
      "reinitializing layer conv_dw_9_bn.gamma\n",
      "reinitializing layer conv_dw_9_bn.beta\n",
      "reinitializing layer conv_dw_9_bn.moving_mean\n",
      "reinitializing layer conv_dw_9_bn.moving_variance\n",
      "LAYER:: conv_dw_9_relu\n",
      "LAYER:: conv_pw_9\n",
      "reinitializing layer conv_pw_9.kernel\n",
      "LAYER:: conv_pw_9_bn\n",
      "reinitializing layer conv_pw_9_bn.gamma\n",
      "reinitializing layer conv_pw_9_bn.beta\n",
      "reinitializing layer conv_pw_9_bn.moving_mean\n",
      "reinitializing layer conv_pw_9_bn.moving_variance\n",
      "LAYER:: conv_pw_9_relu\n",
      "LAYER:: conv_dw_10\n",
      "reinitializing layer conv_dw_10.depthwise_kernel\n",
      "LAYER:: conv_dw_10_bn\n",
      "reinitializing layer conv_dw_10_bn.gamma\n",
      "reinitializing layer conv_dw_10_bn.beta\n",
      "reinitializing layer conv_dw_10_bn.moving_mean\n",
      "reinitializing layer conv_dw_10_bn.moving_variance\n",
      "LAYER:: conv_dw_10_relu\n",
      "LAYER:: conv_pw_10\n",
      "reinitializing layer conv_pw_10.kernel\n",
      "LAYER:: conv_pw_10_bn\n",
      "reinitializing layer conv_pw_10_bn.gamma\n",
      "reinitializing layer conv_pw_10_bn.beta\n",
      "reinitializing layer conv_pw_10_bn.moving_mean\n",
      "reinitializing layer conv_pw_10_bn.moving_variance\n",
      "LAYER:: conv_pw_10_relu\n",
      "LAYER:: conv_dw_11\n",
      "reinitializing layer conv_dw_11.depthwise_kernel\n",
      "LAYER:: conv_dw_11_bn\n",
      "reinitializing layer conv_dw_11_bn.gamma\n",
      "reinitializing layer conv_dw_11_bn.beta\n",
      "reinitializing layer conv_dw_11_bn.moving_mean\n",
      "reinitializing layer conv_dw_11_bn.moving_variance\n",
      "LAYER:: conv_dw_11_relu\n",
      "LAYER:: conv_pw_11\n",
      "reinitializing layer conv_pw_11.kernel\n",
      "LAYER:: conv_pw_11_bn\n",
      "reinitializing layer conv_pw_11_bn.gamma\n",
      "reinitializing layer conv_pw_11_bn.beta\n",
      "reinitializing layer conv_pw_11_bn.moving_mean\n",
      "reinitializing layer conv_pw_11_bn.moving_variance\n",
      "LAYER:: conv_pw_11_relu\n",
      "LAYER:: conv_pad_12\n",
      "LAYER:: conv_dw_12\n",
      "reinitializing layer conv_dw_12.depthwise_kernel\n",
      "LAYER:: conv_dw_12_bn\n",
      "reinitializing layer conv_dw_12_bn.gamma\n",
      "reinitializing layer conv_dw_12_bn.beta\n",
      "reinitializing layer conv_dw_12_bn.moving_mean\n",
      "reinitializing layer conv_dw_12_bn.moving_variance\n",
      "LAYER:: conv_dw_12_relu\n",
      "LAYER:: conv_pw_12\n",
      "reinitializing layer conv_pw_12.kernel\n",
      "LAYER:: conv_pw_12_bn\n",
      "reinitializing layer conv_pw_12_bn.gamma\n",
      "reinitializing layer conv_pw_12_bn.beta\n",
      "reinitializing layer conv_pw_12_bn.moving_mean\n",
      "reinitializing layer conv_pw_12_bn.moving_variance\n",
      "LAYER:: conv_pw_12_relu\n",
      "LAYER:: conv_dw_13\n",
      "reinitializing layer conv_dw_13.depthwise_kernel\n",
      "LAYER:: conv_dw_13_bn\n",
      "reinitializing layer conv_dw_13_bn.gamma\n",
      "reinitializing layer conv_dw_13_bn.beta\n",
      "reinitializing layer conv_dw_13_bn.moving_mean\n",
      "reinitializing layer conv_dw_13_bn.moving_variance\n",
      "LAYER:: conv_dw_13_relu\n",
      "LAYER:: conv_pw_13\n",
      "reinitializing layer conv_pw_13.kernel\n",
      "LAYER:: conv_pw_13_bn\n",
      "reinitializing layer conv_pw_13_bn.gamma\n",
      "reinitializing layer conv_pw_13_bn.beta\n",
      "reinitializing layer conv_pw_13_bn.moving_mean\n",
      "reinitializing layer conv_pw_13_bn.moving_variance\n",
      "LAYER:: conv_pw_13_relu\n",
      "LAYER:: global_average_pooling2d_2\n",
      "LAYER:: reshape_1\n",
      "LAYER:: dropout\n",
      "LAYER:: conv_preds\n",
      "reinitializing layer conv_preds.kernel\n",
      "reinitializing layer conv_preds.bias\n",
      "LAYER:: reshape_2\n",
      "LAYER:: dense_3\n",
      "reinitializing layer dense_3.kernel\n",
      "reinitializing layer dense_3.bias\n",
      "LAYER:: activation_52\n",
      "Train on 1053 samples, validate on 264 samples\n",
      "Epoch 1/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 308.7054 - mse: 308.7054 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 2/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0339 - mse: 0.0339 - val_loss: 0.0229 - val_mse: 0.0229\n",
      "Epoch 3/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0298 - mse: 0.0298 - val_loss: 0.0226 - val_mse: 0.0226\n",
      "Epoch 4/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0293 - mse: 0.0293 - val_loss: 0.0217 - val_mse: 0.0217\n",
      "Epoch 5/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0264 - mse: 0.0264 - val_loss: 0.0215 - val_mse: 0.0215\n",
      "Epoch 6/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0267 - mse: 0.0267 - val_loss: 0.0202 - val_mse: 0.0202\n",
      "Epoch 7/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0266 - mse: 0.0266 - val_loss: 0.0216 - val_mse: 0.0216\n",
      "Epoch 8/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0220 - val_mse: 0.0220\n",
      "Epoch 9/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.0255 - val_mse: 0.0255\n",
      "Epoch 10/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0239 - mse: 0.0239 - val_loss: 0.0257 - val_mse: 0.0257\n",
      "Epoch 11/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0254 - val_mse: 0.0254\n",
      "Epoch 12/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0274 - val_mse: 0.0274\n",
      "Epoch 13/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0237 - mse: 0.0237 - val_loss: 0.0248 - val_mse: 0.0248\n",
      "Epoch 14/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0229 - mse: 0.0229 - val_loss: 0.0232 - val_mse: 0.0232\n",
      "Epoch 15/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0230 - mse: 0.0230 - val_loss: 0.0241 - val_mse: 0.0241\n",
      "Epoch 16/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0233 - mse: 0.0233 - val_loss: 0.0226 - val_mse: 0.0226\n",
      "Epoch 17/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0212 - mse: 0.0212 - val_loss: 0.0273 - val_mse: 0.0273\n",
      "Epoch 18/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0217 - mse: 0.0217 - val_loss: 0.0260 - val_mse: 0.0260\n",
      "Epoch 19/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0221 - mse: 0.0221 - val_loss: 0.0217 - val_mse: 0.0217\n",
      "Epoch 20/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0224 - mse: 0.0224 - val_loss: 0.0222 - val_mse: 0.0222\n",
      "Epoch 21/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0217 - mse: 0.0217 - val_loss: 0.0289 - val_mse: 0.0289\n",
      "Epoch 22/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0225 - mse: 0.0225 - val_loss: 0.0243 - val_mse: 0.0243\n",
      "Epoch 23/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0221 - mse: 0.0221 - val_loss: 0.0191 - val_mse: 0.0191\n",
      "Epoch 24/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0233 - mse: 0.0233 - val_loss: 0.0202 - val_mse: 0.0202\n",
      "Epoch 25/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0226 - mse: 0.0226 - val_loss: 0.0200 - val_mse: 0.0200\n",
      "Epoch 26/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0205 - mse: 0.0205 - val_loss: 0.0237 - val_mse: 0.0237\n",
      "Epoch 27/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0212 - mse: 0.0212 - val_loss: 0.0182 - val_mse: 0.0182\n",
      "Epoch 28/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0212 - mse: 0.0212 - val_loss: 0.0197 - val_mse: 0.0197\n",
      "Epoch 29/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0223 - mse: 0.0223 - val_loss: 0.0182 - val_mse: 0.0182\n",
      "Epoch 30/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.0296 - val_mse: 0.0296\n",
      "Epoch 31/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.0121 - val_mse: 0.0121\n",
      "Epoch 32/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 33/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 34/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0102 - val_mse: 0.0102\n",
      "Epoch 35/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 36/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 37/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 38/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0061 - mse: 0.0061 - val_loss: 0.0208 - val_mse: 0.0208\n",
      "Epoch 39/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.0119 - val_mse: 0.0119\n",
      "Epoch 40/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0078 - val_mse: 0.0078\n",
      "Epoch 41/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0065 - val_mse: 0.0065\n",
      "Epoch 42/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0068 - val_mse: 0.0068\n",
      "Epoch 43/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0068 - mse: 0.0068 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 44/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 45/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0156 - val_mse: 0.0156\n",
      "Epoch 46/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 47/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 48/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0053 - val_mse: 0.0053\n",
      "Epoch 49/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 50/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0160 - val_mse: 0.0160\n",
      "Epoch 51/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 52/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 53/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 54/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0285 - val_mse: 0.0285\n",
      "Epoch 55/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0079 - mse: 0.0079 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 56/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 57/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0057 - val_mse: 0.0057\n",
      "Epoch 58/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0054 - val_mse: 0.0054\n",
      "Epoch 59/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0126 - val_mse: 0.0126\n",
      "Epoch 60/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 62/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0053 - val_mse: 0.0053\n",
      "Epoch 63/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 64/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 65/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0269 - val_mse: 0.0269\n",
      "Epoch 66/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 67/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0056 - val_mse: 0.0056\n",
      "Epoch 68/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 69/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 70/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0064 - val_mse: 0.0064\n",
      "Epoch 71/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 72/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 73/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 74/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0068 - mse: 0.0068 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 75/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 76/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0050 - val_mse: 0.0050\n",
      "Epoch 77/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 78/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 79/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0068 - val_mse: 0.0068\n",
      "Epoch 80/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 81/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 82/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 83/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 84/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0057 - val_mse: 0.0057\n",
      "Epoch 85/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 86/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0047 - val_mse: 0.0047\n",
      "Epoch 87/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 88/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 89/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 90/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 91/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 92/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 93/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 94/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 95/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 96/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 97/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0053 - val_mse: 0.0053\n",
      "Epoch 98/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0133 - val_mse: 0.0133\n",
      "Epoch 99/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 100/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 101/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0050 - val_mse: 0.0050\n",
      "Epoch 102/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 103/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0050 - val_mse: 0.0050\n",
      "Epoch 104/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0078 - val_mse: 0.0078\n",
      "Epoch 105/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0150 - val_mse: 0.0150\n",
      "Epoch 106/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 107/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0070 - val_mse: 0.0070\n",
      "Epoch 108/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 109/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 110/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0146 - val_mse: 0.0146\n",
      "Epoch 111/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 112/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 113/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 114/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 115/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 116/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0071 - val_mse: 0.0071\n",
      "Epoch 117/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 118/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 119/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 120/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 121/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 122/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0056 - val_mse: 0.0056\n",
      "Epoch 123/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 124/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 125/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 126/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0069 - val_mse: 0.0069\n",
      "Epoch 127/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0101 - val_mse: 0.0101\n",
      "Epoch 128/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 129/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0183 - val_mse: 0.0183\n",
      "Epoch 130/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 131/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 132/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 133/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 134/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 135/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0071 - val_mse: 0.0071\n",
      "Epoch 136/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 137/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 138/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 139/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 140/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 141/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0068 - val_mse: 0.0068\n",
      "Epoch 142/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 143/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 144/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 145/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 146/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 147/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 148/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 149/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 150/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 151/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 152/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 153/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 154/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 155/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 156/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0069 - val_mse: 0.0069\n",
      "Epoch 157/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0054 - val_mse: 0.0054\n",
      "Epoch 158/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0071 - val_mse: 0.0071\n",
      "Epoch 159/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 160/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 161/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 162/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 163/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0058 - val_mse: 0.0058\n",
      "Epoch 164/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 165/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0054 - val_mse: 0.0054\n",
      "Epoch 166/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 167/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 168/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 169/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 170/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 171/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 172/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 173/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 174/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 175/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 176/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 177/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 178/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 180/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0120 - val_mse: 0.0120\n",
      "Epoch 181/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 182/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 183/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 184/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 185/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 186/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 187/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 188/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 189/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 190/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 191/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 192/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 193/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 194/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 195/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 196/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 197/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 198/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 199/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 200/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "330/330 [==============================] - 3s 9ms/step\n",
      "(330,)\n",
      "(330,)\n",
      "[112.041435  49.580357 255.42807  243.45135  517.7561   125.56891\n",
      " 230.18211   64.58905  174.4159   151.14806  264.68207   45.578175\n",
      " 150.86186  179.96973  104.23091   39.54685  148.25731  133.99687\n",
      " 248.83864  180.44917  522.3972   255.149    269.24017   75.27224\n",
      "  64.14429  164.471    130.03847   70.8804   123.06777  144.16457\n",
      " 170.85814  124.77151  113.755196  42.193016 107.350716 154.39581\n",
      " 303.56674   23.467169  57.893253 472.78174  292.96893  277.7066\n",
      " 159.35686  222.22318  298.0894   309.92807   28.25506   17.48404\n",
      " 105.54603  138.94466   71.70494   85.37208  191.25758   97.190704\n",
      " 172.5377   186.19478  109.55556  242.52226  225.27153    9.988785\n",
      " 227.65334  303.26468   47.997593 449.01797  243.1946   278.39084\n",
      " 233.66309  193.13972  245.90558   94.77223  227.56213   17.909311\n",
      " 213.72594  148.29857   56.310364 228.99184  160.71304  140.71175\n",
      " 138.45158   45.023293 130.80452  374.93234   99.35233   42.59438\n",
      "  40.915123  79.26698   80.16546   73.170975  91.11464   66.92512\n",
      " 231.97458   19.510292 184.66765  160.57874  261.24625  210.82384\n",
      " 106.797066 228.99184   61.322914  65.43128  189.16391  104.43414\n",
      "  81.64619   11.226877  41.705376  48.429092 154.082    191.28877\n",
      "  64.17411   10.156885 106.091446 172.9599   145.84787   94.41967\n",
      " 129.86844  398.3673    50.10944  288.29837   56.135757 270.86115\n",
      " 249.89452  145.63788  160.06311  235.68794  221.99643  165.08784\n",
      " 269.30673   85.04342   48.394814 125.074524 252.09183  286.00806\n",
      " 465.97885   44.317707  56.9439   197.48276   73.412796 111.38081\n",
      "  45.108498 502.70267  129.64081   48.403942  35.416367 486.58878\n",
      "  60.561337 117.902916  97.14282   80.65377  158.29752  192.37473\n",
      " 282.3211    81.152176  99.31402   37.695633 498.30066   37.914566\n",
      " 149.07248  168.67097   39.45349  141.59497   80.44993  222.09816\n",
      "  81.24772  110.70671   55.938168  56.88586  246.14345  124.615\n",
      " 223.5932    35.56189   87.609695 130.69218   23.467466 130.59645\n",
      "  99.56413  223.91211  301.3212   140.6236   253.54025   85.006226\n",
      " 447.02643  139.48845  122.73332  142.80296   75.59362  121.29108\n",
      " 163.81847  108.977905 135.95831   48.40079  160.48877   97.76018\n",
      "  44.457474 125.058014 419.12885    9.822533 114.46605  118.60071\n",
      " 112.639206  50.466137  89.65953  185.44876  186.72452  110.87974\n",
      "  55.543633  95.58841  128.04207  186.56316   81.77741  190.83502\n",
      " 211.94673  181.05649  148.27707  122.71729  111.0969    95.409744\n",
      " 221.37242  276.20584   49.423695 291.09012  108.87287  128.02267\n",
      "  42.331226 110.656044 268.93823   33.26316   19.385881  99.4364\n",
      " 124.94574   71.644905 222.37103   49.864918 216.5355   218.6678\n",
      "  45.696518 112.77891  172.02612   57.012306 100.023125  41.96225\n",
      " 206.59337   70.84995  120.40393   74.42179  287.5792    51.213818\n",
      "  65.7956   137.62065  113.88472  157.37418  153.32112  190.02351\n",
      " 147.82623   76.834946 337.2128   274.62054  110.915306  35.005985\n",
      "  84.35182   64.97609   77.41697  132.71967  159.16689  502.06638\n",
      " 204.58508   49.594418  85.56038  287.31882  107.64668  107.78953\n",
      " 113.12716  111.86953  265.44403  265.31833   45.292103 173.71646\n",
      " 182.90506  235.68794  178.31696  150.3463   129.39339  229.08385\n",
      " 290.81018  233.89232   77.372345  32.43527  131.24304   12.892842\n",
      " 290.1897   247.17944  170.63838  144.46024   65.34313   24.946651\n",
      " 298.78735  247.08098  111.310326 115.44455  152.28143  251.68675\n",
      " 200.48854  492.4674    95.37603   75.64126  116.12067   82.05597\n",
      " 168.02147  118.774155 469.53928   61.52845   73.24081   20.693817\n",
      " 165.6445    30.967646 223.42613  506.39038  354.32004  117.127014\n",
      "  65.96149  287.14642  251.06406  102.2633   290.71664  266.6918\n",
      "  12.258865 176.02638   94.38451  177.32156  187.5214    54.40547 ]\n",
      "[  6.  54. 275. 240. 450. 108. 225.  58. 150. 162. 250.  80. 162.  64.\n",
      " 100.  46. 174. 128. 250. 184. 500. 300. 285.  68.  88.  30. 120.   8.\n",
      " 128. 154. 144. 126.  80.  52. 112. 158. 300.  18.  66. 450. 265. 290.\n",
      " 168. 255. 295. 300.  32.  24.  96. 132.   6.  92. 186.   8. 180. 196.\n",
      " 108. 230. 225.   8. 215. 290.  54. 450. 250. 270. 230. 200. 225.  80.\n",
      " 245.   4. 190. 168.  44. 225. 182. 122. 120.   4. 124. 350.  56.   2.\n",
      "  12.  84.  84.  78.  84.  82. 225.  18. 176. 174. 198. 225. 100. 225.\n",
      "  46.  60. 188.  88.  70.  10.  24.  48. 172. 186.  52.  10.   2. 188.\n",
      " 156.  48. 116. 400.  30. 275.  40. 275. 215. 156. 190. 225. 245. 172.\n",
      " 235. 102.   6. 122. 265. 295. 450.  46.  86. 196.  84. 112.  10. 500.\n",
      "  20.  58.  34. 500.  14.  82. 104.  92. 154. 188. 285. 100. 120.  34.\n",
      " 500.  16. 132.  46.  12. 136.  22.   1. 100.  90.  58.  38. 270. 112.\n",
      " 215.  32.  26. 124.  16. 144.  90. 182. 300. 134. 215. 102. 450. 128.\n",
      "  94. 138.  72. 146. 158.   0. 112.  40. 178.  98.  32. 136. 400.   4.\n",
      " 112.  28. 106.  28.  74. 200. 192.  96.  58.  32. 140. 172.  72.  40.\n",
      " 200. 156. 162. 142. 134.  76. 255. 240.  58. 280. 124. 128.  28. 136.\n",
      " 275.  26.  16. 128. 134.  64. 225.  32. 186. 220.  52. 154. 184.  54.\n",
      " 114.  18. 220.  94. 130.  60. 295.  74.  60. 164.  10. 198. 138. 200.\n",
      " 128.  74. 350. 285. 102.   2.   4.  92.  86. 154. 190. 500. 186.  42.\n",
      "  62. 275. 110. 124. 116.  98. 275. 265.  28. 182. 198. 225. 170. 154.\n",
      "  86. 235. 300. 240.  84.  36. 150.  14. 300. 250. 164. 160.  78.  22.\n",
      " 300. 230. 112. 118. 118. 250. 178. 500.   0.  90. 120.  80. 186.  90.\n",
      " 450.  74.  64.  24. 140.  36. 215. 450. 350. 136.  68. 290. 220. 116.\n",
      " 300. 290.   8. 120.  48. 188. 164.  86.]\n",
      "mse =  996.724552744477\n"
     ]
    }
   ],
   "source": [
    "time_str = str(time.time())\n",
    "reinitLayers(model)\n",
    "early_stop = EarlyStopping(monitor='val_mean_squared_error', patience=30)\n",
    "# model.load_weights('./modelWights/regression_model'+model_fn.__name__+'.h5')\n",
    "model_checkpoint = ModelCheckpoint('./modelWights/weights_'+'mobileNet_without_pre'+time_str+'_gray_scale.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train_g, y_train, batch_size=BATCH_SIZE, epochs=200, validation_data=(X_val_g, y_val), callbacks=[model_checkpoint])\n",
    "import json\n",
    "with open('./modelWights/history_mobileNet_without_pre_gray_scale'+time_str+'.json', 'w') as f:\n",
    "    json.dump(str(history.history), f)\n",
    "\n",
    "y_pred_ = model.predict(X_test_g, batch_size=1, verbose=1)\n",
    "\n",
    "\n",
    "y_pred_, y_test_ = post_training_data(y_test, y_pred_)\n",
    "predicted_file = 'regress_'+\"mobileNet_without_pre\"+time_str+'_gray_scale.csv'\n",
    "save_result(\"./result/\"+predicted_file,y_pred_,y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER:: input_3\n",
      "LAYER:: conv1_pad\n",
      "LAYER:: conv1\n",
      "reinitializing layer conv1.kernel\n",
      "LAYER:: conv1_bn\n",
      "reinitializing layer conv1_bn.gamma\n",
      "reinitializing layer conv1_bn.beta\n",
      "reinitializing layer conv1_bn.moving_mean\n",
      "reinitializing layer conv1_bn.moving_variance\n",
      "LAYER:: conv1_relu\n",
      "LAYER:: conv_dw_1\n",
      "reinitializing layer conv_dw_1.depthwise_kernel\n",
      "LAYER:: conv_dw_1_bn\n",
      "reinitializing layer conv_dw_1_bn.gamma\n",
      "reinitializing layer conv_dw_1_bn.beta\n",
      "reinitializing layer conv_dw_1_bn.moving_mean\n",
      "reinitializing layer conv_dw_1_bn.moving_variance\n",
      "LAYER:: conv_dw_1_relu\n",
      "LAYER:: conv_pw_1\n",
      "reinitializing layer conv_pw_1.kernel\n",
      "LAYER:: conv_pw_1_bn\n",
      "reinitializing layer conv_pw_1_bn.gamma\n",
      "reinitializing layer conv_pw_1_bn.beta\n",
      "reinitializing layer conv_pw_1_bn.moving_mean\n",
      "reinitializing layer conv_pw_1_bn.moving_variance\n",
      "LAYER:: conv_pw_1_relu\n",
      "LAYER:: conv_pad_2\n",
      "LAYER:: conv_dw_2\n",
      "reinitializing layer conv_dw_2.depthwise_kernel\n",
      "LAYER:: conv_dw_2_bn\n",
      "reinitializing layer conv_dw_2_bn.gamma\n",
      "reinitializing layer conv_dw_2_bn.beta\n",
      "reinitializing layer conv_dw_2_bn.moving_mean\n",
      "reinitializing layer conv_dw_2_bn.moving_variance\n",
      "LAYER:: conv_dw_2_relu\n",
      "LAYER:: conv_pw_2\n",
      "reinitializing layer conv_pw_2.kernel\n",
      "LAYER:: conv_pw_2_bn\n",
      "reinitializing layer conv_pw_2_bn.gamma\n",
      "reinitializing layer conv_pw_2_bn.beta\n",
      "reinitializing layer conv_pw_2_bn.moving_mean\n",
      "reinitializing layer conv_pw_2_bn.moving_variance\n",
      "LAYER:: conv_pw_2_relu\n",
      "LAYER:: conv_dw_3\n",
      "reinitializing layer conv_dw_3.depthwise_kernel\n",
      "LAYER:: conv_dw_3_bn\n",
      "reinitializing layer conv_dw_3_bn.gamma\n",
      "reinitializing layer conv_dw_3_bn.beta\n",
      "reinitializing layer conv_dw_3_bn.moving_mean\n",
      "reinitializing layer conv_dw_3_bn.moving_variance\n",
      "LAYER:: conv_dw_3_relu\n",
      "LAYER:: conv_pw_3\n",
      "reinitializing layer conv_pw_3.kernel\n",
      "LAYER:: conv_pw_3_bn\n",
      "reinitializing layer conv_pw_3_bn.gamma\n",
      "reinitializing layer conv_pw_3_bn.beta\n",
      "reinitializing layer conv_pw_3_bn.moving_mean\n",
      "reinitializing layer conv_pw_3_bn.moving_variance\n",
      "LAYER:: conv_pw_3_relu\n",
      "LAYER:: conv_pad_4\n",
      "LAYER:: conv_dw_4\n",
      "reinitializing layer conv_dw_4.depthwise_kernel\n",
      "LAYER:: conv_dw_4_bn\n",
      "reinitializing layer conv_dw_4_bn.gamma\n",
      "reinitializing layer conv_dw_4_bn.beta\n",
      "reinitializing layer conv_dw_4_bn.moving_mean\n",
      "reinitializing layer conv_dw_4_bn.moving_variance\n",
      "LAYER:: conv_dw_4_relu\n",
      "LAYER:: conv_pw_4\n",
      "reinitializing layer conv_pw_4.kernel\n",
      "LAYER:: conv_pw_4_bn\n",
      "reinitializing layer conv_pw_4_bn.gamma\n",
      "reinitializing layer conv_pw_4_bn.beta\n",
      "reinitializing layer conv_pw_4_bn.moving_mean\n",
      "reinitializing layer conv_pw_4_bn.moving_variance\n",
      "LAYER:: conv_pw_4_relu\n",
      "LAYER:: conv_dw_5\n",
      "reinitializing layer conv_dw_5.depthwise_kernel\n",
      "LAYER:: conv_dw_5_bn\n",
      "reinitializing layer conv_dw_5_bn.gamma\n",
      "reinitializing layer conv_dw_5_bn.beta\n",
      "reinitializing layer conv_dw_5_bn.moving_mean\n",
      "reinitializing layer conv_dw_5_bn.moving_variance\n",
      "LAYER:: conv_dw_5_relu\n",
      "LAYER:: conv_pw_5\n",
      "reinitializing layer conv_pw_5.kernel\n",
      "LAYER:: conv_pw_5_bn\n",
      "reinitializing layer conv_pw_5_bn.gamma\n",
      "reinitializing layer conv_pw_5_bn.beta\n",
      "reinitializing layer conv_pw_5_bn.moving_mean\n",
      "reinitializing layer conv_pw_5_bn.moving_variance\n",
      "LAYER:: conv_pw_5_relu\n",
      "LAYER:: conv_pad_6\n",
      "LAYER:: conv_dw_6\n",
      "reinitializing layer conv_dw_6.depthwise_kernel\n",
      "LAYER:: conv_dw_6_bn\n",
      "reinitializing layer conv_dw_6_bn.gamma\n",
      "reinitializing layer conv_dw_6_bn.beta\n",
      "reinitializing layer conv_dw_6_bn.moving_mean\n",
      "reinitializing layer conv_dw_6_bn.moving_variance\n",
      "LAYER:: conv_dw_6_relu\n",
      "LAYER:: conv_pw_6\n",
      "reinitializing layer conv_pw_6.kernel\n",
      "LAYER:: conv_pw_6_bn\n",
      "reinitializing layer conv_pw_6_bn.gamma\n",
      "reinitializing layer conv_pw_6_bn.beta\n",
      "reinitializing layer conv_pw_6_bn.moving_mean\n",
      "reinitializing layer conv_pw_6_bn.moving_variance\n",
      "LAYER:: conv_pw_6_relu\n",
      "LAYER:: conv_dw_7\n",
      "reinitializing layer conv_dw_7.depthwise_kernel\n",
      "LAYER:: conv_dw_7_bn\n",
      "reinitializing layer conv_dw_7_bn.gamma\n",
      "reinitializing layer conv_dw_7_bn.beta\n",
      "reinitializing layer conv_dw_7_bn.moving_mean\n",
      "reinitializing layer conv_dw_7_bn.moving_variance\n",
      "LAYER:: conv_dw_7_relu\n",
      "LAYER:: conv_pw_7\n",
      "reinitializing layer conv_pw_7.kernel\n",
      "LAYER:: conv_pw_7_bn\n",
      "reinitializing layer conv_pw_7_bn.gamma\n",
      "reinitializing layer conv_pw_7_bn.beta\n",
      "reinitializing layer conv_pw_7_bn.moving_mean\n",
      "reinitializing layer conv_pw_7_bn.moving_variance\n",
      "LAYER:: conv_pw_7_relu\n",
      "LAYER:: conv_dw_8\n",
      "reinitializing layer conv_dw_8.depthwise_kernel\n",
      "LAYER:: conv_dw_8_bn\n",
      "reinitializing layer conv_dw_8_bn.gamma\n",
      "reinitializing layer conv_dw_8_bn.beta\n",
      "reinitializing layer conv_dw_8_bn.moving_mean\n",
      "reinitializing layer conv_dw_8_bn.moving_variance\n",
      "LAYER:: conv_dw_8_relu\n",
      "LAYER:: conv_pw_8\n",
      "reinitializing layer conv_pw_8.kernel\n",
      "LAYER:: conv_pw_8_bn\n",
      "reinitializing layer conv_pw_8_bn.gamma\n",
      "reinitializing layer conv_pw_8_bn.beta\n",
      "reinitializing layer conv_pw_8_bn.moving_mean\n",
      "reinitializing layer conv_pw_8_bn.moving_variance\n",
      "LAYER:: conv_pw_8_relu\n",
      "LAYER:: conv_dw_9\n",
      "reinitializing layer conv_dw_9.depthwise_kernel\n",
      "LAYER:: conv_dw_9_bn\n",
      "reinitializing layer conv_dw_9_bn.gamma\n",
      "reinitializing layer conv_dw_9_bn.beta\n",
      "reinitializing layer conv_dw_9_bn.moving_mean\n",
      "reinitializing layer conv_dw_9_bn.moving_variance\n",
      "LAYER:: conv_dw_9_relu\n",
      "LAYER:: conv_pw_9\n",
      "reinitializing layer conv_pw_9.kernel\n",
      "LAYER:: conv_pw_9_bn\n",
      "reinitializing layer conv_pw_9_bn.gamma\n",
      "reinitializing layer conv_pw_9_bn.beta\n",
      "reinitializing layer conv_pw_9_bn.moving_mean\n",
      "reinitializing layer conv_pw_9_bn.moving_variance\n",
      "LAYER:: conv_pw_9_relu\n",
      "LAYER:: conv_dw_10\n",
      "reinitializing layer conv_dw_10.depthwise_kernel\n",
      "LAYER:: conv_dw_10_bn\n",
      "reinitializing layer conv_dw_10_bn.gamma\n",
      "reinitializing layer conv_dw_10_bn.beta\n",
      "reinitializing layer conv_dw_10_bn.moving_mean\n",
      "reinitializing layer conv_dw_10_bn.moving_variance\n",
      "LAYER:: conv_dw_10_relu\n",
      "LAYER:: conv_pw_10\n",
      "reinitializing layer conv_pw_10.kernel\n",
      "LAYER:: conv_pw_10_bn\n",
      "reinitializing layer conv_pw_10_bn.gamma\n",
      "reinitializing layer conv_pw_10_bn.beta\n",
      "reinitializing layer conv_pw_10_bn.moving_mean\n",
      "reinitializing layer conv_pw_10_bn.moving_variance\n",
      "LAYER:: conv_pw_10_relu\n",
      "LAYER:: conv_dw_11\n",
      "reinitializing layer conv_dw_11.depthwise_kernel\n",
      "LAYER:: conv_dw_11_bn\n",
      "reinitializing layer conv_dw_11_bn.gamma\n",
      "reinitializing layer conv_dw_11_bn.beta\n",
      "reinitializing layer conv_dw_11_bn.moving_mean\n",
      "reinitializing layer conv_dw_11_bn.moving_variance\n",
      "LAYER:: conv_dw_11_relu\n",
      "LAYER:: conv_pw_11\n",
      "reinitializing layer conv_pw_11.kernel\n",
      "LAYER:: conv_pw_11_bn\n",
      "reinitializing layer conv_pw_11_bn.gamma\n",
      "reinitializing layer conv_pw_11_bn.beta\n",
      "reinitializing layer conv_pw_11_bn.moving_mean\n",
      "reinitializing layer conv_pw_11_bn.moving_variance\n",
      "LAYER:: conv_pw_11_relu\n",
      "LAYER:: conv_pad_12\n",
      "LAYER:: conv_dw_12\n",
      "reinitializing layer conv_dw_12.depthwise_kernel\n",
      "LAYER:: conv_dw_12_bn\n",
      "reinitializing layer conv_dw_12_bn.gamma\n",
      "reinitializing layer conv_dw_12_bn.beta\n",
      "reinitializing layer conv_dw_12_bn.moving_mean\n",
      "reinitializing layer conv_dw_12_bn.moving_variance\n",
      "LAYER:: conv_dw_12_relu\n",
      "LAYER:: conv_pw_12\n",
      "reinitializing layer conv_pw_12.kernel\n",
      "LAYER:: conv_pw_12_bn\n",
      "reinitializing layer conv_pw_12_bn.gamma\n",
      "reinitializing layer conv_pw_12_bn.beta\n",
      "reinitializing layer conv_pw_12_bn.moving_mean\n",
      "reinitializing layer conv_pw_12_bn.moving_variance\n",
      "LAYER:: conv_pw_12_relu\n",
      "LAYER:: conv_dw_13\n",
      "reinitializing layer conv_dw_13.depthwise_kernel\n",
      "LAYER:: conv_dw_13_bn\n",
      "reinitializing layer conv_dw_13_bn.gamma\n",
      "reinitializing layer conv_dw_13_bn.beta\n",
      "reinitializing layer conv_dw_13_bn.moving_mean\n",
      "reinitializing layer conv_dw_13_bn.moving_variance\n",
      "LAYER:: conv_dw_13_relu\n",
      "LAYER:: conv_pw_13\n",
      "reinitializing layer conv_pw_13.kernel\n",
      "LAYER:: conv_pw_13_bn\n",
      "reinitializing layer conv_pw_13_bn.gamma\n",
      "reinitializing layer conv_pw_13_bn.beta\n",
      "reinitializing layer conv_pw_13_bn.moving_mean\n",
      "reinitializing layer conv_pw_13_bn.moving_variance\n",
      "LAYER:: conv_pw_13_relu\n",
      "LAYER:: global_average_pooling2d_2\n",
      "LAYER:: reshape_1\n",
      "LAYER:: dropout\n",
      "LAYER:: conv_preds\n",
      "reinitializing layer conv_preds.kernel\n",
      "reinitializing layer conv_preds.bias\n",
      "LAYER:: reshape_2\n",
      "LAYER:: dense_3\n",
      "reinitializing layer dense_3.kernel\n",
      "reinitializing layer dense_3.bias\n",
      "LAYER:: activation_52\n",
      "Train on 1053 samples, validate on 264 samples\n",
      "Epoch 1/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 277.2229 - mse: 277.2228 - val_loss: 0.0774 - val_mse: 0.0774\n",
      "Epoch 2/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0586 - mse: 0.0586 - val_loss: 0.0529 - val_mse: 0.0529\n",
      "Epoch 3/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0449 - mse: 0.0449 - val_loss: 0.0438 - val_mse: 0.0438\n",
      "Epoch 4/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 5/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0360 - mse: 0.0360 - val_loss: 0.0362 - val_mse: 0.0362\n",
      "Epoch 6/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0342 - mse: 0.0342 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 7/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0329 - mse: 0.0329 - val_loss: 0.0345 - val_mse: 0.0345\n",
      "Epoch 8/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0308 - mse: 0.0308 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 9/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0294 - mse: 0.0294 - val_loss: 0.0317 - val_mse: 0.0317\n",
      "Epoch 10/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0295 - mse: 0.0295 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 11/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0281 - mse: 0.0281 - val_loss: 0.0232 - val_mse: 0.0232\n",
      "Epoch 12/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0234 - mse: 0.0234 - val_loss: 0.0206 - val_mse: 0.0206\n",
      "Epoch 13/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0210 - mse: 0.0210 - val_loss: 0.0222 - val_mse: 0.0222\n",
      "Epoch 14/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0202 - mse: 0.0202 - val_loss: 0.0223 - val_mse: 0.0223\n",
      "Epoch 15/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0154 - mse: 0.0154 - val_loss: 0.0225 - val_mse: 0.0225\n",
      "Epoch 16/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 17/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0153 - val_mse: 0.0153\n",
      "Epoch 18/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0141 - mse: 0.0141 - val_loss: 0.0126 - val_mse: 0.0126\n",
      "Epoch 19/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0131 - val_mse: 0.0131\n",
      "Epoch 20/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0176 - val_mse: 0.0176\n",
      "Epoch 21/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 22/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0127 - val_mse: 0.0127\n",
      "Epoch 23/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 24/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 25/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 26/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 27/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0050 - val_mse: 0.0050\n",
      "Epoch 28/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0124 - val_mse: 0.0124\n",
      "Epoch 29/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 30/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 31/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0056 - val_mse: 0.0056\n",
      "Epoch 32/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 33/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0053 - val_mse: 0.0053\n",
      "Epoch 34/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0080 - mse: 0.0080 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 35/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0054 - val_mse: 0.0054\n",
      "Epoch 36/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 37/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 38/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0101 - val_mse: 0.0101\n",
      "Epoch 39/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0048 - val_mse: 0.0048\n",
      "Epoch 40/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0243 - val_mse: 0.0243\n",
      "Epoch 41/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0049 - val_mse: 0.0049\n",
      "Epoch 42/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0054 - val_mse: 0.0054\n",
      "Epoch 43/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0119 - val_mse: 0.0119\n",
      "Epoch 44/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 45/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 46/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 47/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 48/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0151 - val_mse: 0.0151\n",
      "Epoch 49/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 50/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0048 - val_mse: 0.0048\n",
      "Epoch 51/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0065 - val_mse: 0.0065\n",
      "Epoch 52/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 53/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 54/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 55/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0065 - val_mse: 0.0065\n",
      "Epoch 56/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 57/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 58/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 59/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 60/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0069 - val_mse: 0.0069\n",
      "Epoch 62/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 63/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 64/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 65/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 66/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 67/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 68/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 69/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0185 - val_mse: 0.0185\n",
      "Epoch 70/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 71/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 72/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0107 - val_mse: 0.0107\n",
      "Epoch 73/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 74/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 75/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0064 - val_mse: 0.0064\n",
      "Epoch 76/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 77/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0069 - val_mse: 0.0069\n",
      "Epoch 78/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0057 - val_mse: 0.0057\n",
      "Epoch 79/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0102 - val_mse: 0.0102\n",
      "Epoch 80/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0057 - val_mse: 0.0057\n",
      "Epoch 81/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 82/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 83/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 84/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0238 - val_mse: 0.0238\n",
      "Epoch 85/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0048 - val_mse: 0.0048\n",
      "Epoch 86/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 87/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 88/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0064 - val_mse: 0.0064\n",
      "Epoch 89/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 90/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 91/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0132 - val_mse: 0.0132\n",
      "Epoch 92/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 93/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 94/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 95/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0134 - val_mse: 0.0134\n",
      "Epoch 96/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 97/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 98/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 99/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0132 - val_mse: 0.0132\n",
      "Epoch 100/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 101/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0069 - val_mse: 0.0069\n",
      "Epoch 102/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 103/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 104/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 105/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 106/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 107/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 108/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 109/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 110/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 111/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 112/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 113/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 114/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 115/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 116/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 117/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 118/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 119/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 120/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 121/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 122/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 123/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 124/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0056 - val_mse: 0.0056\n",
      "Epoch 125/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0231 - val_mse: 0.0231\n",
      "Epoch 126/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 127/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 128/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 129/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 130/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 131/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 132/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 133/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 134/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0065 - val_mse: 0.0065\n",
      "Epoch 135/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 136/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 137/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 138/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 139/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0048 - val_mse: 0.0048\n",
      "Epoch 140/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 141/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 142/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0050 - val_mse: 0.0050\n",
      "Epoch 143/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 144/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 145/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 146/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0068 - val_mse: 0.0068\n",
      "Epoch 147/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0101 - val_mse: 0.0101\n",
      "Epoch 148/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 149/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 150/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 151/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 152/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 153/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 154/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 155/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 156/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 157/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 158/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 159/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 160/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 161/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 162/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 163/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 164/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 165/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 166/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 167/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 168/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 169/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 170/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 171/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 172/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 173/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 174/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 175/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 176/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 177/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 178/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 180/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 181/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 182/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 183/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 184/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 185/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 186/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 187/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 188/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 189/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 9.3287e-04 - mse: 9.3287e-04 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 190/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 191/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 192/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 193/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 194/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 195/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 196/200\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 197/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 198/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 199/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 200/200\n",
      "1053/1053 [==============================] - 31s 29ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "330/330 [==============================] - 4s 14ms/step\n",
      "(330,)\n",
      "(330,)\n",
      "[ 44.41373    47.80025   252.19968   231.11385   471.7837    134.26877\n",
      " 237.1248     84.53976   145.61798   177.60645   244.07098    65.09729\n",
      " 161.36333   121.47924   107.47419    58.346817  171.24826   150.44872\n",
      " 256.44177   192.40297   515.73975   225.64552   259.7724     94.999\n",
      "  89.32479    95.93662   134.88808    14.206589  125.82341   134.75195\n",
      " 154.45282   127.057106  104.07734    54.494686  120.84841   173.5876\n",
      " 275.45236    31.033457   69.5014    425.19604   264.4986    278.70038\n",
      " 171.30177   234.50142   282.18488   289.84317    41.657166   36.227165\n",
      " 118.072914  136.06775    28.47594   107.15977   174.56757    38.99257\n",
      " 196.96986   183.6865    107.1563    239.17294   214.28116    10.493294\n",
      " 235.07854   284.09607    49.983093  456.37256   259.92047   261.391\n",
      " 227.78244   210.40134   241.83566    95.30807   235.57445     5.6251435\n",
      " 175.51485   158.04388    67.62453   212.93205   149.8458    150.47098\n",
      " 138.43704    13.992325  136.66832   379.82266    56.96657    17.617167\n",
      "  25.551573   81.315704   77.51192    87.415474  111.15405    87.72716\n",
      " 220.63344    30.813292  201.05315   174.5115    240.093     228.22365\n",
      " 112.545     212.93205    50.20118    77.28158   162.99638   123.30516\n",
      "  80.693825   23.605854   46.72905    52.184025  165.76521   200.96463\n",
      "  77.67229     8.504733   42.47212   181.49217   154.04337    56.996563\n",
      " 131.79799   383.71533    36.125034  257.1639     76.14406   286.09833\n",
      " 244.6436    163.91359   151.91963   225.9472    230.89528   151.50777\n",
      " 259.54987    95.71969    30.266344  141.2631    252.33347   267.72568\n",
      " 445.27286    73.95475    83.28217   225.50154    81.24234   123.0713\n",
      "  56.766273  505.06116    60.8425     81.50019    41.0382    470.3555\n",
      "  15.353232  104.286354  119.93403    94.97931   134.96684   180.69597\n",
      " 261.05078    90.94807   106.47027    38.185673  491.3826      7.813826\n",
      " 141.39232    68.68127    19.157722  150.91504    59.905983  153.58134\n",
      " 103.54882    99.20172    51.19853    49.304726  219.04596   122.79209\n",
      " 238.82503    58.217518   47.193893  128.80722    32.81769   139.13934\n",
      "  88.19017   201.7302    287.52045   143.99382   260.4176    103.35654\n",
      " 448.74548   156.64047    90.19403   128.10114    92.532135  106.348114\n",
      " 167.55202    64.92642   147.49908    55.448196  168.13399   123.06618\n",
      "  28.382809  137.84483   388.33676     2.9590428 104.55992    82.7353\n",
      " 115.688324   40.53137    98.12523   211.37239   172.19824   116.29446\n",
      "  69.89435    77.663574  133.01611   184.68355   104.022575  114.025085\n",
      " 196.77661   166.41661   177.37314   127.894325  109.993454  103.17732\n",
      " 199.07037   261.28387    71.44084   262.56448   118.09978   159.2755\n",
      "  59.52216   122.37665   267.9562     47.386265    9.463817  122.662125\n",
      " 143.56163    79.21835   223.69925    46.38349   194.09349   185.93248\n",
      "  65.883     114.91263   184.88927    56.38576   104.801834   21.53723\n",
      " 197.32507    85.59632   142.712      78.31202   291.7968     70.11613\n",
      "  82.73507   154.31848    47.55283   133.6303    164.40114   195.68127\n",
      " 135.84763    91.08061   348.49615   261.93048   131.88815    12.910217\n",
      "  30.093864   92.11031    93.07786   144.55087   149.64725   501.73105\n",
      " 188.26382    54.476746   93.9572    288.24374   145.26855   104.59811\n",
      " 125.82907   127.63675   251.68875   268.67398    62.231937  160.0147\n",
      " 167.84448   225.9472    185.64188   137.243     120.91053   230.08478\n",
      " 266.94995   227.84099    95.22257    40.004597  146.59409    31.55169\n",
      " 295.61758   264.41016   183.93454   149.97456    76.99198    23.492544\n",
      " 287.57483   234.23163   125.95643   118.55924   159.7157    246.26735\n",
      " 175.85835   491.68958    56.271     104.2936    123.870674   78.12015\n",
      " 161.5079    113.94548   442.2463     71.84265    66.187614   27.269796\n",
      " 147.65813    39.288597  225.67381   449.3223    340.23547   125.80374\n",
      "  82.00488   260.81485   234.89809   119.33119   281.39233   268.88733\n",
      "   9.986356  157.44951    51.150368  174.20625   168.5189     89.911896 ]\n",
      "[  6.  54. 275. 240. 450. 108. 225.  58. 150. 162. 250.  80. 162.  64.\n",
      " 100.  46. 174. 128. 250. 184. 500. 300. 285.  68.  88.  30. 120.   8.\n",
      " 128. 154. 144. 126.  80.  52. 112. 158. 300.  18.  66. 450. 265. 290.\n",
      " 168. 255. 295. 300.  32.  24.  96. 132.   6.  92. 186.   8. 180. 196.\n",
      " 108. 230. 225.   8. 215. 290.  54. 450. 250. 270. 230. 200. 225.  80.\n",
      " 245.   4. 190. 168.  44. 225. 182. 122. 120.   4. 124. 350.  56.   2.\n",
      "  12.  84.  84.  78.  84.  82. 225.  18. 176. 174. 198. 225. 100. 225.\n",
      "  46.  60. 188.  88.  70.  10.  24.  48. 172. 186.  52.  10.   2. 188.\n",
      " 156.  48. 116. 400.  30. 275.  40. 275. 215. 156. 190. 225. 245. 172.\n",
      " 235. 102.   6. 122. 265. 295. 450.  46.  86. 196.  84. 112.  10. 500.\n",
      "  20.  58.  34. 500.  14.  82. 104.  92. 154. 188. 285. 100. 120.  34.\n",
      " 500.  16. 132.  46.  12. 136.  22.   1. 100.  90.  58.  38. 270. 112.\n",
      " 215.  32.  26. 124.  16. 144.  90. 182. 300. 134. 215. 102. 450. 128.\n",
      "  94. 138.  72. 146. 158.   0. 112.  40. 178.  98.  32. 136. 400.   4.\n",
      " 112.  28. 106.  28.  74. 200. 192.  96.  58.  32. 140. 172.  72.  40.\n",
      " 200. 156. 162. 142. 134.  76. 255. 240.  58. 280. 124. 128.  28. 136.\n",
      " 275.  26.  16. 128. 134.  64. 225.  32. 186. 220.  52. 154. 184.  54.\n",
      " 114.  18. 220.  94. 130.  60. 295.  74.  60. 164.  10. 198. 138. 200.\n",
      " 128.  74. 350. 285. 102.   2.   4.  92.  86. 154. 190. 500. 186.  42.\n",
      "  62. 275. 110. 124. 116.  98. 275. 265.  28. 182. 198. 225. 170. 154.\n",
      "  86. 235. 300. 240.  84.  36. 150.  14. 300. 250. 164. 160.  78.  22.\n",
      " 300. 230. 112. 118. 118. 250. 178. 500.   0.  90. 120.  80. 186.  90.\n",
      " 450.  74.  64.  24. 140.  36. 215. 450. 350. 136.  68. 290. 220. 116.\n",
      " 300. 290.   8. 120.  48. 188. 164.  86.]\n",
      "mse =  495.3785881749567\n"
     ]
    }
   ],
   "source": [
    "time_str = str(time.time())\n",
    "reinitLayers(model)\n",
    "early_stop = EarlyStopping(monitor='val_mean_squared_error', patience=30)\n",
    "# model.load_weights('./modelWights/regression_model'+model_fn.__name__+'.h5')\n",
    "model_checkpoint = ModelCheckpoint('./modelWights/weights_'+'mobileNet_without_pre'+time_str+'_gray_scale.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train_g, y_train, batch_size=BATCH_SIZE, epochs=200, validation_data=(X_val_g, y_val), callbacks=[model_checkpoint])\n",
    "import json\n",
    "with open('./modelWights/history_mobileNet_without_pre_gray_scale'+time_str+'.json', 'w') as f:\n",
    "    json.dump(str(history.history), f)\n",
    "\n",
    "y_pred_ = model.predict(X_test_g, batch_size=1, verbose=1)\n",
    "\n",
    "\n",
    "y_pred_, y_test_ = post_training_data(y_test, y_pred_)\n",
    "predicted_file = 'regress_'+\"mobileNet_without_pre\"+time_str+'_gray_scale.csv'\n",
    "save_result(\"./result/\"+predicted_file,y_pred_,y_test_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mobilenet color without pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER:: input_3\n",
      "LAYER:: conv1_pad\n",
      "LAYER:: conv1\n",
      "reinitializing layer conv1.kernel\n",
      "LAYER:: conv1_bn\n",
      "reinitializing layer conv1_bn.gamma\n",
      "reinitializing layer conv1_bn.beta\n",
      "reinitializing layer conv1_bn.moving_mean\n",
      "reinitializing layer conv1_bn.moving_variance\n",
      "LAYER:: conv1_relu\n",
      "LAYER:: conv_dw_1\n",
      "reinitializing layer conv_dw_1.depthwise_kernel\n",
      "LAYER:: conv_dw_1_bn\n",
      "reinitializing layer conv_dw_1_bn.gamma\n",
      "reinitializing layer conv_dw_1_bn.beta\n",
      "reinitializing layer conv_dw_1_bn.moving_mean\n",
      "reinitializing layer conv_dw_1_bn.moving_variance\n",
      "LAYER:: conv_dw_1_relu\n",
      "LAYER:: conv_pw_1\n",
      "reinitializing layer conv_pw_1.kernel\n",
      "LAYER:: conv_pw_1_bn\n",
      "reinitializing layer conv_pw_1_bn.gamma\n",
      "reinitializing layer conv_pw_1_bn.beta\n",
      "reinitializing layer conv_pw_1_bn.moving_mean\n",
      "reinitializing layer conv_pw_1_bn.moving_variance\n",
      "LAYER:: conv_pw_1_relu\n",
      "LAYER:: conv_pad_2\n",
      "LAYER:: conv_dw_2\n",
      "reinitializing layer conv_dw_2.depthwise_kernel\n",
      "LAYER:: conv_dw_2_bn\n",
      "reinitializing layer conv_dw_2_bn.gamma\n",
      "reinitializing layer conv_dw_2_bn.beta\n",
      "reinitializing layer conv_dw_2_bn.moving_mean\n",
      "reinitializing layer conv_dw_2_bn.moving_variance\n",
      "LAYER:: conv_dw_2_relu\n",
      "LAYER:: conv_pw_2\n",
      "reinitializing layer conv_pw_2.kernel\n",
      "LAYER:: conv_pw_2_bn\n",
      "reinitializing layer conv_pw_2_bn.gamma\n",
      "reinitializing layer conv_pw_2_bn.beta\n",
      "reinitializing layer conv_pw_2_bn.moving_mean\n",
      "reinitializing layer conv_pw_2_bn.moving_variance\n",
      "LAYER:: conv_pw_2_relu\n",
      "LAYER:: conv_dw_3\n",
      "reinitializing layer conv_dw_3.depthwise_kernel\n",
      "LAYER:: conv_dw_3_bn\n",
      "reinitializing layer conv_dw_3_bn.gamma\n",
      "reinitializing layer conv_dw_3_bn.beta\n",
      "reinitializing layer conv_dw_3_bn.moving_mean\n",
      "reinitializing layer conv_dw_3_bn.moving_variance\n",
      "LAYER:: conv_dw_3_relu\n",
      "LAYER:: conv_pw_3\n",
      "reinitializing layer conv_pw_3.kernel\n",
      "LAYER:: conv_pw_3_bn\n",
      "reinitializing layer conv_pw_3_bn.gamma\n",
      "reinitializing layer conv_pw_3_bn.beta\n",
      "reinitializing layer conv_pw_3_bn.moving_mean\n",
      "reinitializing layer conv_pw_3_bn.moving_variance\n",
      "LAYER:: conv_pw_3_relu\n",
      "LAYER:: conv_pad_4\n",
      "LAYER:: conv_dw_4\n",
      "reinitializing layer conv_dw_4.depthwise_kernel\n",
      "LAYER:: conv_dw_4_bn\n",
      "reinitializing layer conv_dw_4_bn.gamma\n",
      "reinitializing layer conv_dw_4_bn.beta\n",
      "reinitializing layer conv_dw_4_bn.moving_mean\n",
      "reinitializing layer conv_dw_4_bn.moving_variance\n",
      "LAYER:: conv_dw_4_relu\n",
      "LAYER:: conv_pw_4\n",
      "reinitializing layer conv_pw_4.kernel\n",
      "LAYER:: conv_pw_4_bn\n",
      "reinitializing layer conv_pw_4_bn.gamma\n",
      "reinitializing layer conv_pw_4_bn.beta\n",
      "reinitializing layer conv_pw_4_bn.moving_mean\n",
      "reinitializing layer conv_pw_4_bn.moving_variance\n",
      "LAYER:: conv_pw_4_relu\n",
      "LAYER:: conv_dw_5\n",
      "reinitializing layer conv_dw_5.depthwise_kernel\n",
      "LAYER:: conv_dw_5_bn\n",
      "reinitializing layer conv_dw_5_bn.gamma\n",
      "reinitializing layer conv_dw_5_bn.beta\n",
      "reinitializing layer conv_dw_5_bn.moving_mean\n",
      "reinitializing layer conv_dw_5_bn.moving_variance\n",
      "LAYER:: conv_dw_5_relu\n",
      "LAYER:: conv_pw_5\n",
      "reinitializing layer conv_pw_5.kernel\n",
      "LAYER:: conv_pw_5_bn\n",
      "reinitializing layer conv_pw_5_bn.gamma\n",
      "reinitializing layer conv_pw_5_bn.beta\n",
      "reinitializing layer conv_pw_5_bn.moving_mean\n",
      "reinitializing layer conv_pw_5_bn.moving_variance\n",
      "LAYER:: conv_pw_5_relu\n",
      "LAYER:: conv_pad_6\n",
      "LAYER:: conv_dw_6\n",
      "reinitializing layer conv_dw_6.depthwise_kernel\n",
      "LAYER:: conv_dw_6_bn\n",
      "reinitializing layer conv_dw_6_bn.gamma\n",
      "reinitializing layer conv_dw_6_bn.beta\n",
      "reinitializing layer conv_dw_6_bn.moving_mean\n",
      "reinitializing layer conv_dw_6_bn.moving_variance\n",
      "LAYER:: conv_dw_6_relu\n",
      "LAYER:: conv_pw_6\n",
      "reinitializing layer conv_pw_6.kernel\n",
      "LAYER:: conv_pw_6_bn\n",
      "reinitializing layer conv_pw_6_bn.gamma\n",
      "reinitializing layer conv_pw_6_bn.beta\n",
      "reinitializing layer conv_pw_6_bn.moving_mean\n",
      "reinitializing layer conv_pw_6_bn.moving_variance\n",
      "LAYER:: conv_pw_6_relu\n",
      "LAYER:: conv_dw_7\n",
      "reinitializing layer conv_dw_7.depthwise_kernel\n",
      "LAYER:: conv_dw_7_bn\n",
      "reinitializing layer conv_dw_7_bn.gamma\n",
      "reinitializing layer conv_dw_7_bn.beta\n",
      "reinitializing layer conv_dw_7_bn.moving_mean\n",
      "reinitializing layer conv_dw_7_bn.moving_variance\n",
      "LAYER:: conv_dw_7_relu\n",
      "LAYER:: conv_pw_7\n",
      "reinitializing layer conv_pw_7.kernel\n",
      "LAYER:: conv_pw_7_bn\n",
      "reinitializing layer conv_pw_7_bn.gamma\n",
      "reinitializing layer conv_pw_7_bn.beta\n",
      "reinitializing layer conv_pw_7_bn.moving_mean\n",
      "reinitializing layer conv_pw_7_bn.moving_variance\n",
      "LAYER:: conv_pw_7_relu\n",
      "LAYER:: conv_dw_8\n",
      "reinitializing layer conv_dw_8.depthwise_kernel\n",
      "LAYER:: conv_dw_8_bn\n",
      "reinitializing layer conv_dw_8_bn.gamma\n",
      "reinitializing layer conv_dw_8_bn.beta\n",
      "reinitializing layer conv_dw_8_bn.moving_mean\n",
      "reinitializing layer conv_dw_8_bn.moving_variance\n",
      "LAYER:: conv_dw_8_relu\n",
      "LAYER:: conv_pw_8\n",
      "reinitializing layer conv_pw_8.kernel\n",
      "LAYER:: conv_pw_8_bn\n",
      "reinitializing layer conv_pw_8_bn.gamma\n",
      "reinitializing layer conv_pw_8_bn.beta\n",
      "reinitializing layer conv_pw_8_bn.moving_mean\n",
      "reinitializing layer conv_pw_8_bn.moving_variance\n",
      "LAYER:: conv_pw_8_relu\n",
      "LAYER:: conv_dw_9\n",
      "reinitializing layer conv_dw_9.depthwise_kernel\n",
      "LAYER:: conv_dw_9_bn\n",
      "reinitializing layer conv_dw_9_bn.gamma\n",
      "reinitializing layer conv_dw_9_bn.beta\n",
      "reinitializing layer conv_dw_9_bn.moving_mean\n",
      "reinitializing layer conv_dw_9_bn.moving_variance\n",
      "LAYER:: conv_dw_9_relu\n",
      "LAYER:: conv_pw_9\n",
      "reinitializing layer conv_pw_9.kernel\n",
      "LAYER:: conv_pw_9_bn\n",
      "reinitializing layer conv_pw_9_bn.gamma\n",
      "reinitializing layer conv_pw_9_bn.beta\n",
      "reinitializing layer conv_pw_9_bn.moving_mean\n",
      "reinitializing layer conv_pw_9_bn.moving_variance\n",
      "LAYER:: conv_pw_9_relu\n",
      "LAYER:: conv_dw_10\n",
      "reinitializing layer conv_dw_10.depthwise_kernel\n",
      "LAYER:: conv_dw_10_bn\n",
      "reinitializing layer conv_dw_10_bn.gamma\n",
      "reinitializing layer conv_dw_10_bn.beta\n",
      "reinitializing layer conv_dw_10_bn.moving_mean\n",
      "reinitializing layer conv_dw_10_bn.moving_variance\n",
      "LAYER:: conv_dw_10_relu\n",
      "LAYER:: conv_pw_10\n",
      "reinitializing layer conv_pw_10.kernel\n",
      "LAYER:: conv_pw_10_bn\n",
      "reinitializing layer conv_pw_10_bn.gamma\n",
      "reinitializing layer conv_pw_10_bn.beta\n",
      "reinitializing layer conv_pw_10_bn.moving_mean\n",
      "reinitializing layer conv_pw_10_bn.moving_variance\n",
      "LAYER:: conv_pw_10_relu\n",
      "LAYER:: conv_dw_11\n",
      "reinitializing layer conv_dw_11.depthwise_kernel\n",
      "LAYER:: conv_dw_11_bn\n",
      "reinitializing layer conv_dw_11_bn.gamma\n",
      "reinitializing layer conv_dw_11_bn.beta\n",
      "reinitializing layer conv_dw_11_bn.moving_mean\n",
      "reinitializing layer conv_dw_11_bn.moving_variance\n",
      "LAYER:: conv_dw_11_relu\n",
      "LAYER:: conv_pw_11\n",
      "reinitializing layer conv_pw_11.kernel\n",
      "LAYER:: conv_pw_11_bn\n",
      "reinitializing layer conv_pw_11_bn.gamma\n",
      "reinitializing layer conv_pw_11_bn.beta\n",
      "reinitializing layer conv_pw_11_bn.moving_mean\n",
      "reinitializing layer conv_pw_11_bn.moving_variance\n",
      "LAYER:: conv_pw_11_relu\n",
      "LAYER:: conv_pad_12\n",
      "LAYER:: conv_dw_12\n",
      "reinitializing layer conv_dw_12.depthwise_kernel\n",
      "LAYER:: conv_dw_12_bn\n",
      "reinitializing layer conv_dw_12_bn.gamma\n",
      "reinitializing layer conv_dw_12_bn.beta\n",
      "reinitializing layer conv_dw_12_bn.moving_mean\n",
      "reinitializing layer conv_dw_12_bn.moving_variance\n",
      "LAYER:: conv_dw_12_relu\n",
      "LAYER:: conv_pw_12\n",
      "reinitializing layer conv_pw_12.kernel\n",
      "LAYER:: conv_pw_12_bn\n",
      "reinitializing layer conv_pw_12_bn.gamma\n",
      "reinitializing layer conv_pw_12_bn.beta\n",
      "reinitializing layer conv_pw_12_bn.moving_mean\n",
      "reinitializing layer conv_pw_12_bn.moving_variance\n",
      "LAYER:: conv_pw_12_relu\n",
      "LAYER:: conv_dw_13\n",
      "reinitializing layer conv_dw_13.depthwise_kernel\n",
      "LAYER:: conv_dw_13_bn\n",
      "reinitializing layer conv_dw_13_bn.gamma\n",
      "reinitializing layer conv_dw_13_bn.beta\n",
      "reinitializing layer conv_dw_13_bn.moving_mean\n",
      "reinitializing layer conv_dw_13_bn.moving_variance\n",
      "LAYER:: conv_dw_13_relu\n",
      "LAYER:: conv_pw_13\n",
      "reinitializing layer conv_pw_13.kernel\n",
      "LAYER:: conv_pw_13_bn\n",
      "reinitializing layer conv_pw_13_bn.gamma\n",
      "reinitializing layer conv_pw_13_bn.beta\n",
      "reinitializing layer conv_pw_13_bn.moving_mean\n",
      "reinitializing layer conv_pw_13_bn.moving_variance\n",
      "LAYER:: conv_pw_13_relu\n",
      "LAYER:: global_average_pooling2d_2\n",
      "LAYER:: reshape_1\n",
      "LAYER:: dropout\n",
      "LAYER:: conv_preds\n",
      "reinitializing layer conv_preds.kernel\n",
      "reinitializing layer conv_preds.bias\n",
      "LAYER:: reshape_2\n",
      "LAYER:: dense_3\n",
      "reinitializing layer dense_3.kernel\n",
      "reinitializing layer dense_3.bias\n",
      "LAYER:: activation_52\n",
      "Train on 1053 samples, validate on 264 samples\n",
      "Epoch 1/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 286.5277 - mse: 286.5276 - val_loss: 0.0466 - val_mse: 0.0466\n",
      "Epoch 2/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.2285 - val_mse: 0.2285\n",
      "Epoch 3/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0368 - mse: 0.0368 - val_loss: 0.4779 - val_mse: 0.4779\n",
      "Epoch 4/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0336 - mse: 0.0336 - val_loss: 0.7367 - val_mse: 0.7367\n",
      "Epoch 5/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0322 - mse: 0.0322 - val_loss: 1.3346 - val_mse: 1.3346\n",
      "Epoch 6/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0310 - mse: 0.0310 - val_loss: 1.2173 - val_mse: 1.2173\n",
      "Epoch 7/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0287 - mse: 0.0287 - val_loss: 1.1429 - val_mse: 1.1429\n",
      "Epoch 8/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0283 - mse: 0.0283 - val_loss: 1.3167 - val_mse: 1.3167\n",
      "Epoch 9/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0273 - mse: 0.0273 - val_loss: 1.1477 - val_mse: 1.1477\n",
      "Epoch 10/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0288 - mse: 0.0288 - val_loss: 1.0113 - val_mse: 1.0113\n",
      "Epoch 11/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0283 - mse: 0.0283 - val_loss: 1.1858 - val_mse: 1.1858\n",
      "Epoch 12/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 1.0509 - val_mse: 1.0509\n",
      "Epoch 13/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0260 - mse: 0.0260 - val_loss: 0.8974 - val_mse: 0.8974\n",
      "Epoch 14/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0278 - mse: 0.0278 - val_loss: 0.8751 - val_mse: 0.8751\n",
      "Epoch 15/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 1.1470 - val_mse: 1.1470\n",
      "Epoch 16/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0213 - mse: 0.0213 - val_loss: 0.8478 - val_mse: 0.8478\n",
      "Epoch 17/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0193 - mse: 0.0193 - val_loss: 0.6096 - val_mse: 0.6096\n",
      "Epoch 18/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0202 - mse: 0.0202 - val_loss: 0.4241 - val_mse: 0.4241\n",
      "Epoch 19/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.2002 - val_mse: 0.2002\n",
      "Epoch 20/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0157 - mse: 0.0157 - val_loss: 0.2663 - val_mse: 0.2663\n",
      "Epoch 21/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.1355 - val_mse: 0.1355\n",
      "Epoch 22/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.4158 - val_mse: 0.4158\n",
      "Epoch 23/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0189 - mse: 0.0189 - val_loss: 0.2692 - val_mse: 0.2692\n",
      "Epoch 24/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.1719 - val_mse: 0.1719\n",
      "Epoch 25/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.2414 - val_mse: 0.2414\n",
      "Epoch 26/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.1461 - val_mse: 0.1461\n",
      "Epoch 27/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.1404 - val_mse: 0.1404\n",
      "Epoch 28/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0141 - mse: 0.0141 - val_loss: 0.2551 - val_mse: 0.2551\n",
      "Epoch 29/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.1962 - val_mse: 0.1962\n",
      "Epoch 30/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.1172 - val_mse: 0.1172\n",
      "Epoch 31/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0079 - mse: 0.0079 - val_loss: 0.0727 - val_mse: 0.0727\n",
      "Epoch 32/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.2156 - val_mse: 0.2156\n",
      "Epoch 33/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0079 - mse: 0.0079 - val_loss: 0.1859 - val_mse: 0.1859\n",
      "Epoch 34/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0636 - val_mse: 0.0636\n",
      "Epoch 35/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0061 - mse: 0.0061 - val_loss: 0.1539 - val_mse: 0.1539\n",
      "Epoch 36/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.1045 - val_mse: 0.1045\n",
      "Epoch 37/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.1556 - val_mse: 0.1556\n",
      "Epoch 38/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.1946 - val_mse: 0.1946\n",
      "Epoch 39/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.1062 - val_mse: 0.1062\n",
      "Epoch 40/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.1353 - val_mse: 0.1353\n",
      "Epoch 41/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0819 - val_mse: 0.0819\n",
      "Epoch 42/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.1255 - val_mse: 0.1255\n",
      "Epoch 43/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0552 - val_mse: 0.0552\n",
      "Epoch 44/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0670 - val_mse: 0.0670\n",
      "Epoch 45/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 46/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 47/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0192 - val_mse: 0.0192\n",
      "Epoch 48/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 49/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 50/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 51/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0645 - val_mse: 0.0645\n",
      "Epoch 52/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0382 - val_mse: 0.0382\n",
      "Epoch 53/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 54/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0228 - val_mse: 0.0228\n",
      "Epoch 55/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0279 - val_mse: 0.0279\n",
      "Epoch 56/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 57/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0156 - val_mse: 0.0156\n",
      "Epoch 58/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 59/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0102 - val_mse: 0.0102\n",
      "Epoch 60/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0070 - val_mse: 0.0070\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 62/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 63/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0058 - val_mse: 0.0058\n",
      "Epoch 64/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0148 - val_mse: 0.0148\n",
      "Epoch 65/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 66/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 67/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 68/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 69/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0234 - val_mse: 0.0234\n",
      "Epoch 70/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0194 - val_mse: 0.0194\n",
      "Epoch 71/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 72/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 73/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 74/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 75/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0069 - val_mse: 0.0069\n",
      "Epoch 76/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0058 - val_mse: 0.0058\n",
      "Epoch 77/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 78/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0162 - val_mse: 0.0162\n",
      "Epoch 79/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0229 - val_mse: 0.0229\n",
      "Epoch 80/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0050 - val_mse: 0.0050\n",
      "Epoch 81/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0177 - val_mse: 0.0177\n",
      "Epoch 82/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0158 - val_mse: 0.0158\n",
      "Epoch 83/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0303 - val_mse: 0.0303\n",
      "Epoch 84/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 85/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0065 - val_mse: 0.0065\n",
      "Epoch 86/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0048 - val_mse: 0.0048\n",
      "Epoch 87/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 88/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 89/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 90/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 91/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 92/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 93/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 94/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0056 - val_mse: 0.0056\n",
      "Epoch 95/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 96/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 97/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 98/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0071 - val_mse: 0.0071\n",
      "Epoch 99/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0058 - val_mse: 0.0058\n",
      "Epoch 100/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 101/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 102/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 103/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 104/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 105/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 106/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 107/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0048 - val_mse: 0.0048\n",
      "Epoch 108/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0122 - val_mse: 0.0122\n",
      "Epoch 109/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0064 - val_mse: 0.0064\n",
      "Epoch 110/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 111/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 112/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 113/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0064 - val_mse: 0.0064\n",
      "Epoch 114/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 115/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 116/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 117/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0069 - val_mse: 0.0069\n",
      "Epoch 118/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 119/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 120/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 121/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 122/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0131 - val_mse: 0.0131\n",
      "Epoch 123/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 124/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 125/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 126/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 127/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0164 - val_mse: 0.0164\n",
      "Epoch 128/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 129/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 130/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 131/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 132/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 133/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 134/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 135/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 136/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 137/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 138/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 139/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 140/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 141/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 142/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 143/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 144/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 145/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 146/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 147/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 148/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0057 - val_mse: 0.0057\n",
      "Epoch 149/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0048 - val_mse: 0.0048\n",
      "Epoch 150/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 151/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 152/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 153/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 154/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 155/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 156/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 157/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 158/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 159/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 160/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 161/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 162/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 163/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 164/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 165/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 166/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 167/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 168/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 169/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 170/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 171/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 172/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 173/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 174/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 175/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 176/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 177/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 178/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 9.5629e-04 - mse: 9.5629e-04 - val_loss: 0.0024 - val_mse: 0.0024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 180/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 181/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 182/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 183/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 184/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 185/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 186/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 187/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 188/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 189/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 190/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 191/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 192/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 9.6483e-04 - mse: 9.6483e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 193/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 194/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 195/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 196/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 197/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 198/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 199/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 200/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "330/330 [==============================] - 4s 12ms/step\n",
      "(330,)\n",
      "(330,)\n",
      "[ -4.124135   42.257683  267.97882   252.17178   455.6897    115.35395\n",
      " 244.27452    28.098509  129.85828   149.38594   263.9053     39.21057\n",
      " 148.07379    65.15691    91.51022    32.223476  144.11351   124.49049\n",
      " 281.75693   193.83061   479.705     256.82748   286.9043     60.9447\n",
      "  65.572624  117.86553   117.18613    41.771812  129.72763   123.18176\n",
      " 138.61398   111.04525    93.32851    24.246574  111.0994    188.6865\n",
      " 290.4113     21.794468   15.771344  415.68973   283.0266    285.26608\n",
      " 154.91843   228.85802   285.7431    318.56693    21.994427    5.535379\n",
      " 106.461754  133.47903    14.195442   87.79714   158.76086    37.82284\n",
      " 189.6106    189.96367   106.711136  258.3416    243.59196     9.909675\n",
      " 216.28455   292.9706     46.35711   453.15338   260.42548   265.13034\n",
      " 243.62915   220.2806    260.32864    84.87657   230.90814   113.6314\n",
      " 159.33833   153.74356    65.058464  212.4151    136.21474   141.94704\n",
      " 102.38166    40.344284  101.32075   352.44763    83.842186  101.977745\n",
      " 115.41952    43.730392   40.99132    73.70914   134.62648    90.86729\n",
      " 244.11064     3.207773  201.7064    153.51706   236.68962   215.34657\n",
      "  98.23403   212.4151     55.107826   77.58995   184.57527    74.847496\n",
      "  48.13832    -9.21753     5.993262   49.292385  187.04842   184.93245\n",
      "  28.018251   12.387753   65.887955  145.02621   149.55489    37.408695\n",
      " 114.40604   425.55826     8.155569  278.83475    28.836786  289.62253\n",
      " 262.29813   152.13292   166.46098   241.28523   248.18916   129.80888\n",
      " 268.8854     77.66794    85.01303   126.0007    260.83582   268.94452\n",
      " 456.18805    32.23662    45.398174  231.0834     53.454056  117.49288\n",
      "  11.933356  471.44333    60.678646   53.554935    7.004082  472.26645\n",
      "  71.5269     86.77614   102.05117    73.75568   122.0468    169.1373\n",
      " 287.33975    76.90692    98.60549     9.170741  482.30762    -3.1905026\n",
      " 137.89792    83.75361     8.862466  143.19849   133.47122   301.88345\n",
      "  96.63914   103.478065   46.353565   41.757122  269.83105   107.09741\n",
      " 226.337      23.535505   24.75524   127.75779     1.0460615 140.68268\n",
      "  64.26541   201.9458    308.4644    112.008575  282.99112    86.50839\n",
      " 456.73987   144.87276    84.5586    107.028496   68.29907   134.93698\n",
      " 163.73071   103.92758   136.04323    21.531984  133.40866   102.8407\n",
      "  74.90756   142.21437   403.68765    88.488686   93.398865  119.064064\n",
      "  98.63137    21.583378   59.025616  196.84584   176.21411   113.94731\n",
      "  36.745846   62.822357  116.533035  176.22968    85.62535   130.8263\n",
      " 202.95805   141.57593   147.25558   154.58476   116.42044    51.03345\n",
      " 242.32686   252.55743    54.98894   282.03278   126.545876  146.66667\n",
      "   2.5031714 122.51772   284.5649     15.723482    0.7622987 109.11501\n",
      " 144.36453    88.47895   230.75494    29.465378  201.88428   220.76326\n",
      "  63.556725  112.290886  182.42511    27.025148  100.99511   -20.47792\n",
      " 225.18556    91.96999   128.73804    52.672684  286.22812    58.94862\n",
      "  83.9875    147.69368    10.869101  164.9115    156.27042   183.81903\n",
      " 129.78845    61.430557  358.5544    286.72525    96.7167    100.551384\n",
      "  39.265453   79.875786   76.56591   139.86076   165.15488   458.73422\n",
      " 204.5007     47.056004   81.67461   266.70062   124.98707   128.06126\n",
      " 106.88702    86.322014  266.1628    276.83313     7.1493983 148.59334\n",
      " 195.64104   241.28523   167.60414   118.73126   100.60622   227.3976\n",
      " 261.27426   228.70105    83.54809    34.89284   134.81255   -12.6775055\n",
      " 308.96967   261.54974   142.4778    145.10939    43.197586    4.120931\n",
      " 304.36597   245.70073    97.098076   95.48174   151.8       266.09277\n",
      " 155.71198   460.83627    92.58275    80.683426  116.07934    84.071686\n",
      " 192.50124   106.42059   461.70636    66.694275   25.635242   -1.6181767\n",
      " 164.19598    32.158897  242.72438   470.5876    344.49237   132.90564\n",
      "  41.77722   262.41718   253.65714   125.697556  280.22165   278.6061\n",
      "  28.708607  119.61296    22.931396  172.60434   166.77121    61.660244 ]\n",
      "[  6.  54. 275. 240. 450. 108. 225.  58. 150. 162. 250.  80. 162.  64.\n",
      " 100.  46. 174. 128. 250. 184. 500. 300. 285.  68.  88.  30. 120.   8.\n",
      " 128. 154. 144. 126.  80.  52. 112. 158. 300.  18.  66. 450. 265. 290.\n",
      " 168. 255. 295. 300.  32.  24.  96. 132.   6.  92. 186.   8. 180. 196.\n",
      " 108. 230. 225.   8. 215. 290.  54. 450. 250. 270. 230. 200. 225.  80.\n",
      " 245.   4. 190. 168.  44. 225. 182. 122. 120.   4. 124. 350.  56.   2.\n",
      "  12.  84.  84.  78.  84.  82. 225.  18. 176. 174. 198. 225. 100. 225.\n",
      "  46.  60. 188.  88.  70.  10.  24.  48. 172. 186.  52.  10.   2. 188.\n",
      " 156.  48. 116. 400.  30. 275.  40. 275. 215. 156. 190. 225. 245. 172.\n",
      " 235. 102.   6. 122. 265. 295. 450.  46.  86. 196.  84. 112.  10. 500.\n",
      "  20.  58.  34. 500.  14.  82. 104.  92. 154. 188. 285. 100. 120.  34.\n",
      " 500.  16. 132.  46.  12. 136.  22.   1. 100.  90.  58.  38. 270. 112.\n",
      " 215.  32.  26. 124.  16. 144.  90. 182. 300. 134. 215. 102. 450. 128.\n",
      "  94. 138.  72. 146. 158.   0. 112.  40. 178.  98.  32. 136. 400.   4.\n",
      " 112.  28. 106.  28.  74. 200. 192.  96.  58.  32. 140. 172.  72.  40.\n",
      " 200. 156. 162. 142. 134.  76. 255. 240.  58. 280. 124. 128.  28. 136.\n",
      " 275.  26.  16. 128. 134.  64. 225.  32. 186. 220.  52. 154. 184.  54.\n",
      " 114.  18. 220.  94. 130.  60. 295.  74.  60. 164.  10. 198. 138. 200.\n",
      " 128.  74. 350. 285. 102.   2.   4.  92.  86. 154. 190. 500. 186.  42.\n",
      "  62. 275. 110. 124. 116.  98. 275. 265.  28. 182. 198. 225. 170. 154.\n",
      "  86. 235. 300. 240.  84.  36. 150.  14. 300. 250. 164. 160.  78.  22.\n",
      " 300. 230. 112. 118. 118. 250. 178. 500.   0.  90. 120.  80. 186.  90.\n",
      " 450.  74.  64.  24. 140.  36. 215. 450. 350. 136.  68. 290. 220. 116.\n",
      " 300. 290.   8. 120.  48. 188. 164.  86.]\n",
      "mse =  1010.8550193809441\n"
     ]
    }
   ],
   "source": [
    "time_str = str(time.time())\n",
    "reinitLayers(model)\n",
    "early_stop = EarlyStopping(monitor='val_mean_squared_error', patience=30)\n",
    "# model.load_weights('./modelWights/regression_model'+model_fn.__name__+'.h5')\n",
    "model_checkpoint = ModelCheckpoint('./modelWights/weights_'+'mobileNet_without_pre'+time_str+'_color.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=200, validation_data=(X_val, y_val), callbacks=[model_checkpoint])\n",
    "# print(history.history)\n",
    "\n",
    "y_pred_ = model.predict(X_test, batch_size=1, verbose=1)\n",
    "\n",
    "\n",
    "y_pred_, y_test_ = post_training_data(y_test, y_pred_)\n",
    "predicted_file = 'regress_'+\"mobileNet_without_pre\"+time_str+'_color.csv'\n",
    "save_result(\"./result/\"+predicted_file,y_pred_,y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER:: input_3\n",
      "LAYER:: conv1_pad\n",
      "LAYER:: conv1\n",
      "reinitializing layer conv1.kernel\n",
      "LAYER:: conv1_bn\n",
      "reinitializing layer conv1_bn.gamma\n",
      "reinitializing layer conv1_bn.beta\n",
      "reinitializing layer conv1_bn.moving_mean\n",
      "reinitializing layer conv1_bn.moving_variance\n",
      "LAYER:: conv1_relu\n",
      "LAYER:: conv_dw_1\n",
      "reinitializing layer conv_dw_1.depthwise_kernel\n",
      "LAYER:: conv_dw_1_bn\n",
      "reinitializing layer conv_dw_1_bn.gamma\n",
      "reinitializing layer conv_dw_1_bn.beta\n",
      "reinitializing layer conv_dw_1_bn.moving_mean\n",
      "reinitializing layer conv_dw_1_bn.moving_variance\n",
      "LAYER:: conv_dw_1_relu\n",
      "LAYER:: conv_pw_1\n",
      "reinitializing layer conv_pw_1.kernel\n",
      "LAYER:: conv_pw_1_bn\n",
      "reinitializing layer conv_pw_1_bn.gamma\n",
      "reinitializing layer conv_pw_1_bn.beta\n",
      "reinitializing layer conv_pw_1_bn.moving_mean\n",
      "reinitializing layer conv_pw_1_bn.moving_variance\n",
      "LAYER:: conv_pw_1_relu\n",
      "LAYER:: conv_pad_2\n",
      "LAYER:: conv_dw_2\n",
      "reinitializing layer conv_dw_2.depthwise_kernel\n",
      "LAYER:: conv_dw_2_bn\n",
      "reinitializing layer conv_dw_2_bn.gamma\n",
      "reinitializing layer conv_dw_2_bn.beta\n",
      "reinitializing layer conv_dw_2_bn.moving_mean\n",
      "reinitializing layer conv_dw_2_bn.moving_variance\n",
      "LAYER:: conv_dw_2_relu\n",
      "LAYER:: conv_pw_2\n",
      "reinitializing layer conv_pw_2.kernel\n",
      "LAYER:: conv_pw_2_bn\n",
      "reinitializing layer conv_pw_2_bn.gamma\n",
      "reinitializing layer conv_pw_2_bn.beta\n",
      "reinitializing layer conv_pw_2_bn.moving_mean\n",
      "reinitializing layer conv_pw_2_bn.moving_variance\n",
      "LAYER:: conv_pw_2_relu\n",
      "LAYER:: conv_dw_3\n",
      "reinitializing layer conv_dw_3.depthwise_kernel\n",
      "LAYER:: conv_dw_3_bn\n",
      "reinitializing layer conv_dw_3_bn.gamma\n",
      "reinitializing layer conv_dw_3_bn.beta\n",
      "reinitializing layer conv_dw_3_bn.moving_mean\n",
      "reinitializing layer conv_dw_3_bn.moving_variance\n",
      "LAYER:: conv_dw_3_relu\n",
      "LAYER:: conv_pw_3\n",
      "reinitializing layer conv_pw_3.kernel\n",
      "LAYER:: conv_pw_3_bn\n",
      "reinitializing layer conv_pw_3_bn.gamma\n",
      "reinitializing layer conv_pw_3_bn.beta\n",
      "reinitializing layer conv_pw_3_bn.moving_mean\n",
      "reinitializing layer conv_pw_3_bn.moving_variance\n",
      "LAYER:: conv_pw_3_relu\n",
      "LAYER:: conv_pad_4\n",
      "LAYER:: conv_dw_4\n",
      "reinitializing layer conv_dw_4.depthwise_kernel\n",
      "LAYER:: conv_dw_4_bn\n",
      "reinitializing layer conv_dw_4_bn.gamma\n",
      "reinitializing layer conv_dw_4_bn.beta\n",
      "reinitializing layer conv_dw_4_bn.moving_mean\n",
      "reinitializing layer conv_dw_4_bn.moving_variance\n",
      "LAYER:: conv_dw_4_relu\n",
      "LAYER:: conv_pw_4\n",
      "reinitializing layer conv_pw_4.kernel\n",
      "LAYER:: conv_pw_4_bn\n",
      "reinitializing layer conv_pw_4_bn.gamma\n",
      "reinitializing layer conv_pw_4_bn.beta\n",
      "reinitializing layer conv_pw_4_bn.moving_mean\n",
      "reinitializing layer conv_pw_4_bn.moving_variance\n",
      "LAYER:: conv_pw_4_relu\n",
      "LAYER:: conv_dw_5\n",
      "reinitializing layer conv_dw_5.depthwise_kernel\n",
      "LAYER:: conv_dw_5_bn\n",
      "reinitializing layer conv_dw_5_bn.gamma\n",
      "reinitializing layer conv_dw_5_bn.beta\n",
      "reinitializing layer conv_dw_5_bn.moving_mean\n",
      "reinitializing layer conv_dw_5_bn.moving_variance\n",
      "LAYER:: conv_dw_5_relu\n",
      "LAYER:: conv_pw_5\n",
      "reinitializing layer conv_pw_5.kernel\n",
      "LAYER:: conv_pw_5_bn\n",
      "reinitializing layer conv_pw_5_bn.gamma\n",
      "reinitializing layer conv_pw_5_bn.beta\n",
      "reinitializing layer conv_pw_5_bn.moving_mean\n",
      "reinitializing layer conv_pw_5_bn.moving_variance\n",
      "LAYER:: conv_pw_5_relu\n",
      "LAYER:: conv_pad_6\n",
      "LAYER:: conv_dw_6\n",
      "reinitializing layer conv_dw_6.depthwise_kernel\n",
      "LAYER:: conv_dw_6_bn\n",
      "reinitializing layer conv_dw_6_bn.gamma\n",
      "reinitializing layer conv_dw_6_bn.beta\n",
      "reinitializing layer conv_dw_6_bn.moving_mean\n",
      "reinitializing layer conv_dw_6_bn.moving_variance\n",
      "LAYER:: conv_dw_6_relu\n",
      "LAYER:: conv_pw_6\n",
      "reinitializing layer conv_pw_6.kernel\n",
      "LAYER:: conv_pw_6_bn\n",
      "reinitializing layer conv_pw_6_bn.gamma\n",
      "reinitializing layer conv_pw_6_bn.beta\n",
      "reinitializing layer conv_pw_6_bn.moving_mean\n",
      "reinitializing layer conv_pw_6_bn.moving_variance\n",
      "LAYER:: conv_pw_6_relu\n",
      "LAYER:: conv_dw_7\n",
      "reinitializing layer conv_dw_7.depthwise_kernel\n",
      "LAYER:: conv_dw_7_bn\n",
      "reinitializing layer conv_dw_7_bn.gamma\n",
      "reinitializing layer conv_dw_7_bn.beta\n",
      "reinitializing layer conv_dw_7_bn.moving_mean\n",
      "reinitializing layer conv_dw_7_bn.moving_variance\n",
      "LAYER:: conv_dw_7_relu\n",
      "LAYER:: conv_pw_7\n",
      "reinitializing layer conv_pw_7.kernel\n",
      "LAYER:: conv_pw_7_bn\n",
      "reinitializing layer conv_pw_7_bn.gamma\n",
      "reinitializing layer conv_pw_7_bn.beta\n",
      "reinitializing layer conv_pw_7_bn.moving_mean\n",
      "reinitializing layer conv_pw_7_bn.moving_variance\n",
      "LAYER:: conv_pw_7_relu\n",
      "LAYER:: conv_dw_8\n",
      "reinitializing layer conv_dw_8.depthwise_kernel\n",
      "LAYER:: conv_dw_8_bn\n",
      "reinitializing layer conv_dw_8_bn.gamma\n",
      "reinitializing layer conv_dw_8_bn.beta\n",
      "reinitializing layer conv_dw_8_bn.moving_mean\n",
      "reinitializing layer conv_dw_8_bn.moving_variance\n",
      "LAYER:: conv_dw_8_relu\n",
      "LAYER:: conv_pw_8\n",
      "reinitializing layer conv_pw_8.kernel\n",
      "LAYER:: conv_pw_8_bn\n",
      "reinitializing layer conv_pw_8_bn.gamma\n",
      "reinitializing layer conv_pw_8_bn.beta\n",
      "reinitializing layer conv_pw_8_bn.moving_mean\n",
      "reinitializing layer conv_pw_8_bn.moving_variance\n",
      "LAYER:: conv_pw_8_relu\n",
      "LAYER:: conv_dw_9\n",
      "reinitializing layer conv_dw_9.depthwise_kernel\n",
      "LAYER:: conv_dw_9_bn\n",
      "reinitializing layer conv_dw_9_bn.gamma\n",
      "reinitializing layer conv_dw_9_bn.beta\n",
      "reinitializing layer conv_dw_9_bn.moving_mean\n",
      "reinitializing layer conv_dw_9_bn.moving_variance\n",
      "LAYER:: conv_dw_9_relu\n",
      "LAYER:: conv_pw_9\n",
      "reinitializing layer conv_pw_9.kernel\n",
      "LAYER:: conv_pw_9_bn\n",
      "reinitializing layer conv_pw_9_bn.gamma\n",
      "reinitializing layer conv_pw_9_bn.beta\n",
      "reinitializing layer conv_pw_9_bn.moving_mean\n",
      "reinitializing layer conv_pw_9_bn.moving_variance\n",
      "LAYER:: conv_pw_9_relu\n",
      "LAYER:: conv_dw_10\n",
      "reinitializing layer conv_dw_10.depthwise_kernel\n",
      "LAYER:: conv_dw_10_bn\n",
      "reinitializing layer conv_dw_10_bn.gamma\n",
      "reinitializing layer conv_dw_10_bn.beta\n",
      "reinitializing layer conv_dw_10_bn.moving_mean\n",
      "reinitializing layer conv_dw_10_bn.moving_variance\n",
      "LAYER:: conv_dw_10_relu\n",
      "LAYER:: conv_pw_10\n",
      "reinitializing layer conv_pw_10.kernel\n",
      "LAYER:: conv_pw_10_bn\n",
      "reinitializing layer conv_pw_10_bn.gamma\n",
      "reinitializing layer conv_pw_10_bn.beta\n",
      "reinitializing layer conv_pw_10_bn.moving_mean\n",
      "reinitializing layer conv_pw_10_bn.moving_variance\n",
      "LAYER:: conv_pw_10_relu\n",
      "LAYER:: conv_dw_11\n",
      "reinitializing layer conv_dw_11.depthwise_kernel\n",
      "LAYER:: conv_dw_11_bn\n",
      "reinitializing layer conv_dw_11_bn.gamma\n",
      "reinitializing layer conv_dw_11_bn.beta\n",
      "reinitializing layer conv_dw_11_bn.moving_mean\n",
      "reinitializing layer conv_dw_11_bn.moving_variance\n",
      "LAYER:: conv_dw_11_relu\n",
      "LAYER:: conv_pw_11\n",
      "reinitializing layer conv_pw_11.kernel\n",
      "LAYER:: conv_pw_11_bn\n",
      "reinitializing layer conv_pw_11_bn.gamma\n",
      "reinitializing layer conv_pw_11_bn.beta\n",
      "reinitializing layer conv_pw_11_bn.moving_mean\n",
      "reinitializing layer conv_pw_11_bn.moving_variance\n",
      "LAYER:: conv_pw_11_relu\n",
      "LAYER:: conv_pad_12\n",
      "LAYER:: conv_dw_12\n",
      "reinitializing layer conv_dw_12.depthwise_kernel\n",
      "LAYER:: conv_dw_12_bn\n",
      "reinitializing layer conv_dw_12_bn.gamma\n",
      "reinitializing layer conv_dw_12_bn.beta\n",
      "reinitializing layer conv_dw_12_bn.moving_mean\n",
      "reinitializing layer conv_dw_12_bn.moving_variance\n",
      "LAYER:: conv_dw_12_relu\n",
      "LAYER:: conv_pw_12\n",
      "reinitializing layer conv_pw_12.kernel\n",
      "LAYER:: conv_pw_12_bn\n",
      "reinitializing layer conv_pw_12_bn.gamma\n",
      "reinitializing layer conv_pw_12_bn.beta\n",
      "reinitializing layer conv_pw_12_bn.moving_mean\n",
      "reinitializing layer conv_pw_12_bn.moving_variance\n",
      "LAYER:: conv_pw_12_relu\n",
      "LAYER:: conv_dw_13\n",
      "reinitializing layer conv_dw_13.depthwise_kernel\n",
      "LAYER:: conv_dw_13_bn\n",
      "reinitializing layer conv_dw_13_bn.gamma\n",
      "reinitializing layer conv_dw_13_bn.beta\n",
      "reinitializing layer conv_dw_13_bn.moving_mean\n",
      "reinitializing layer conv_dw_13_bn.moving_variance\n",
      "LAYER:: conv_dw_13_relu\n",
      "LAYER:: conv_pw_13\n",
      "reinitializing layer conv_pw_13.kernel\n",
      "LAYER:: conv_pw_13_bn\n",
      "reinitializing layer conv_pw_13_bn.gamma\n",
      "reinitializing layer conv_pw_13_bn.beta\n",
      "reinitializing layer conv_pw_13_bn.moving_mean\n",
      "reinitializing layer conv_pw_13_bn.moving_variance\n",
      "LAYER:: conv_pw_13_relu\n",
      "LAYER:: global_average_pooling2d_2\n",
      "LAYER:: reshape_1\n",
      "LAYER:: dropout\n",
      "LAYER:: conv_preds\n",
      "reinitializing layer conv_preds.kernel\n",
      "reinitializing layer conv_preds.bias\n",
      "LAYER:: reshape_2\n",
      "LAYER:: dense_3\n",
      "reinitializing layer dense_3.kernel\n",
      "reinitializing layer dense_3.bias\n",
      "LAYER:: activation_52\n",
      "Train on 1053 samples, validate on 264 samples\n",
      "Epoch 1/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 345.1895 - mse: 345.1894 - val_loss: 0.0576 - val_mse: 0.0576\n",
      "Epoch 2/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0485 - mse: 0.0485 - val_loss: 0.0515 - val_mse: 0.0515\n",
      "Epoch 3/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0457 - mse: 0.0457 - val_loss: 0.0488 - val_mse: 0.0488\n",
      "Epoch 4/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0447 - mse: 0.0447 - val_loss: 0.0479 - val_mse: 0.0479\n",
      "Epoch 5/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0442 - mse: 0.0442 - val_loss: 0.0475 - val_mse: 0.0475\n",
      "Epoch 6/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0442 - mse: 0.0442 - val_loss: 0.0476 - val_mse: 0.0476\n",
      "Epoch 7/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0441 - mse: 0.0441 - val_loss: 0.0480 - val_mse: 0.0480\n",
      "Epoch 8/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0441 - mse: 0.0441 - val_loss: 0.0473 - val_mse: 0.0473\n",
      "Epoch 9/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0438 - mse: 0.0438 - val_loss: 0.0475 - val_mse: 0.0475\n",
      "Epoch 10/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0437 - mse: 0.0437 - val_loss: 0.0469 - val_mse: 0.0469\n",
      "Epoch 11/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.0485 - val_mse: 0.0485\n",
      "Epoch 12/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0419 - mse: 0.0419 - val_loss: 0.0490 - val_mse: 0.0490\n",
      "Epoch 13/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0411 - mse: 0.0411 - val_loss: 0.0466 - val_mse: 0.0466\n",
      "Epoch 14/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0424 - val_mse: 0.0424\n",
      "Epoch 15/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0347 - mse: 0.0347 - val_loss: 0.0388 - val_mse: 0.0388\n",
      "Epoch 16/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0284 - mse: 0.0284 - val_loss: 0.0265 - val_mse: 0.0265\n",
      "Epoch 17/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0225 - mse: 0.0225 - val_loss: 0.0231 - val_mse: 0.0231\n",
      "Epoch 18/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0218 - mse: 0.0218 - val_loss: 0.0230 - val_mse: 0.0230\n",
      "Epoch 19/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0191 - mse: 0.0191 - val_loss: 0.0185 - val_mse: 0.0185\n",
      "Epoch 20/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.0189 - val_mse: 0.0189\n",
      "Epoch 21/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.0167 - val_mse: 0.0167\n",
      "Epoch 22/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.0238 - val_mse: 0.0238\n",
      "Epoch 23/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0159 - mse: 0.0159 - val_loss: 0.0167 - val_mse: 0.0167\n",
      "Epoch 24/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0179 - val_mse: 0.0179\n",
      "Epoch 25/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 26/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0180 - val_mse: 0.0180\n",
      "Epoch 27/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0119 - val_mse: 0.0119\n",
      "Epoch 28/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 29/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0201 - val_mse: 0.0201\n",
      "Epoch 30/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0221 - val_mse: 0.0221\n",
      "Epoch 31/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 32/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0146 - val_mse: 0.0146\n",
      "Epoch 33/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 34/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0121 - val_mse: 0.0121\n",
      "Epoch 35/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0120 - val_mse: 0.0120\n",
      "Epoch 36/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 37/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 38/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 39/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0067 - mse: 0.0067 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 40/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0142 - val_mse: 0.0142\n",
      "Epoch 41/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 42/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 43/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0164 - val_mse: 0.0164\n",
      "Epoch 44/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 45/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0058 - val_mse: 0.0058\n",
      "Epoch 46/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 47/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0124 - val_mse: 0.0124\n",
      "Epoch 48/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 49/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0061 - mse: 0.0061 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 50/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0201 - val_mse: 0.0201\n",
      "Epoch 51/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.0118 - val_mse: 0.0118\n",
      "Epoch 52/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0070 - val_mse: 0.0070\n",
      "Epoch 53/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 54/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0157 - val_mse: 0.0157\n",
      "Epoch 55/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 56/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0157 - val_mse: 0.0157\n",
      "Epoch 57/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0069 - val_mse: 0.0069\n",
      "Epoch 58/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 59/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0065 - val_mse: 0.0065\n",
      "Epoch 60/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0070 - val_mse: 0.0070\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 62/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 63/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 64/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0101 - val_mse: 0.0101\n",
      "Epoch 65/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0065 - val_mse: 0.0065\n",
      "Epoch 66/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 67/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0131 - val_mse: 0.0131\n",
      "Epoch 68/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 69/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 70/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0124 - val_mse: 0.0124\n",
      "Epoch 71/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 72/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 73/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0064 - val_mse: 0.0064\n",
      "Epoch 74/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0054 - val_mse: 0.0054\n",
      "Epoch 75/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0053 - val_mse: 0.0053\n",
      "Epoch 76/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 77/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0116 - val_mse: 0.0116\n",
      "Epoch 78/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 79/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0166 - val_mse: 0.0166\n",
      "Epoch 80/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 81/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 82/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 83/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0126 - val_mse: 0.0126\n",
      "Epoch 84/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 85/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0049 - val_mse: 0.0049\n",
      "Epoch 86/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 87/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 88/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 89/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 90/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 91/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 92/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 93/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 94/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 95/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 96/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0197 - val_mse: 0.0197\n",
      "Epoch 97/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0070 - val_mse: 0.0070\n",
      "Epoch 98/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 99/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 100/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 101/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 102/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 103/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 104/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 105/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.1717 - val_mse: 0.1717\n",
      "Epoch 106/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 107/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0244 - val_mse: 0.0244\n",
      "Epoch 108/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 109/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0053 - val_mse: 0.0053\n",
      "Epoch 110/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 111/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 112/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 113/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0120 - val_mse: 0.0120\n",
      "Epoch 114/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 115/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 116/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 117/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 118/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 119/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0048 - val_mse: 0.0048\n",
      "Epoch 120/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 121/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 122/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0133 - val_mse: 0.0133\n",
      "Epoch 123/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0058 - val_mse: 0.0058\n",
      "Epoch 124/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 125/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0058 - val_mse: 0.0058\n",
      "Epoch 126/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 127/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0064 - val_mse: 0.0064\n",
      "Epoch 128/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 129/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0050 - val_mse: 0.0050\n",
      "Epoch 130/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 131/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 132/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 133/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 134/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 135/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 136/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 137/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 138/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 139/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 140/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 141/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 142/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 143/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 144/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 145/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0047 - val_mse: 0.0047\n",
      "Epoch 146/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 147/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 148/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 149/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 150/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 151/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 152/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 153/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 154/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 155/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0048 - val_mse: 0.0048\n",
      "Epoch 156/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 157/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 158/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 159/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 160/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 161/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 162/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 163/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 164/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 165/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 166/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 167/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 168/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 169/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 170/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 171/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 172/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 173/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 174/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 175/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 176/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 177/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 178/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 180/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 181/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0050 - val_mse: 0.0050\n",
      "Epoch 182/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 183/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 184/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 185/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 186/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 187/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 188/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 189/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0054 - val_mse: 0.0054\n",
      "Epoch 190/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 191/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 192/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 193/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 194/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 195/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 196/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 197/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 198/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0058 - val_mse: 0.0058\n",
      "Epoch 199/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 200/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "330/330 [==============================] - 4s 12ms/step\n",
      "(330,)\n",
      "(330,)\n",
      "[ 13.846919   43.7256    240.45183   221.90582   471.19363   118.19201\n",
      " 229.75084    55.452564  137.5049    146.5433    237.39862    47.06353\n",
      " 141.47493    93.322205   93.291046   34.097538  165.65445   113.98354\n",
      " 243.10718   185.96727   479.3901    253.07243   240.61868    66.74515\n",
      "  53.191303  102.13326   115.04889    32.999985  131.02754   132.27179\n",
      " 148.74539   109.184326  110.34772    32.765755  124.307785  159.81255\n",
      " 269.43335    19.776388   31.412378  406.21185   264.6287    270.99023\n",
      " 163.04239   215.40034   276.6046    294.21207    34.451416   18.113195\n",
      "  99.74007   128.8216      4.8574953  87.695526  153.23508    32.049595\n",
      " 191.92357   179.157     115.823555  234.82129   205.01901     5.5811257\n",
      " 223.8558    304.5054     47.33345   418.58054   227.00732   241.10448\n",
      " 215.10533   190.17941   224.99251    86.37604   217.4965    103.77802\n",
      " 170.87988   169.22253    43.492683  207.35046   148.7106    136.35025\n",
      " 108.24855     8.218736  122.469864  343.47122    79.37216   109.304504\n",
      "  57.3074     73.14104    69.91104    66.35959   180.4516     63.65066\n",
      " 233.52443    10.846392  181.38084   179.59778   233.28218   225.36969\n",
      "  89.172585  207.35046    41.124596   81.30718   178.25516    85.16415\n",
      "  61.00771     0.6787479  36.032215   36.811672  155.53435   200.98656\n",
      "  69.962265    2.4122448  17.986595  147.1193    128.19028    34.56867\n",
      " 107.072205  381.22476    34.61109   251.89685    67.46902   267.84705\n",
      " 214.37483   144.28743   147.23236   219.23976   220.09743   145.8471\n",
      " 273.287      61.98833    18.852844  120.431435  231.34282   267.1043\n",
      " 437.92517    34.401363   51.04594   218.7442     71.547615  109.16874\n",
      "  22.211864  466.84628    12.886345   53.341545   24.80059   463.\n",
      "  15.808762  100.858154   84.16971    57.606087  145.31401   180.7267\n",
      " 267.08603    77.842476   93.35687    27.191765  471.4719      7.745385\n",
      " 137.64095    49.63968     4.4033527 132.05737   123.210625  278.10663\n",
      "  84.35461    91.316444   38.60802    52.169666  247.53127   119.712845\n",
      " 219.30522    27.818434   19.71671   121.3809     29.860094  136.76813\n",
      "  72.21761   232.45091   290.39764    95.642555  258.7496    106.03173\n",
      " 415.8967    134.49161   115.46317   127.90826    83.12231   131.74033\n",
      " 148.95221    79.63581   150.58131    32.708332  154.74818    99.29505\n",
      "  52.728706  125.95352   376.1094    119.34763   103.75669    58.61161\n",
      " 106.96089    24.652958   90.860085  170.35622   166.46585   104.3483\n",
      "  61.920033   60.42236   123.093575  189.55702    70.302086  146.59615\n",
      " 183.32175   126.2196    157.35094   143.99431   115.47556    74.12762\n",
      " 210.65979   258.27393    44.32448   265.39798   109.91569   145.18661\n",
      "  24.803072  128.91495   293.08658    38.945927   15.123442  111.93698\n",
      " 116.80385    59.471897  218.21329    36.33141   172.55617   179.91309\n",
      "  71.01963   109.2015    195.72495    40.256477  102.358       4.9491525\n",
      " 194.81912    85.70358   109.40326    81.63906   279.89703    50.90365\n",
      "  72.75988   131.76189     9.909749  129.38489   148.72371   183.46211\n",
      " 124.382164   58.32576   317.83746   271.09827    90.22856    99.04896\n",
      "   2.4870932  68.27266    63.451416  124.64409   137.47888   462.67487\n",
      " 181.29346    24.912611   73.124176  261.21378   128.34483   111.16138\n",
      "  99.29141    79.1235    243.12712   239.56825    31.911335  158.01567\n",
      " 191.12318   219.23976   186.13159   127.74159   117.245125  219.32155\n",
      " 284.0112    217.52417    76.45244    36.513218  112.9763      4.159957\n",
      " 303.86975   235.17937   163.63994   149.48756    53.645565   15.877917\n",
      " 292.5091    221.78793   121.41634   102.76274   140.62785   258.0384\n",
      " 151.98642   468.58377    68.24597    87.25085   109.464615   88.98965\n",
      " 149.8555    102.44663   444.58905    66.67007    64.88562    13.503\n",
      " 127.79985    38.568535  222.49368   453.58084   329.3384    140.20651\n",
      "  26.92686   273.849     230.92163   139.92253   265.71225   258.7825\n",
      "  10.570556  135.30078    32.791405  162.61325   160.09554    38.63667  ]\n",
      "[  6.  54. 275. 240. 450. 108. 225.  58. 150. 162. 250.  80. 162.  64.\n",
      " 100.  46. 174. 128. 250. 184. 500. 300. 285.  68.  88.  30. 120.   8.\n",
      " 128. 154. 144. 126.  80.  52. 112. 158. 300.  18.  66. 450. 265. 290.\n",
      " 168. 255. 295. 300.  32.  24.  96. 132.   6.  92. 186.   8. 180. 196.\n",
      " 108. 230. 225.   8. 215. 290.  54. 450. 250. 270. 230. 200. 225.  80.\n",
      " 245.   4. 190. 168.  44. 225. 182. 122. 120.   4. 124. 350.  56.   2.\n",
      "  12.  84.  84.  78.  84.  82. 225.  18. 176. 174. 198. 225. 100. 225.\n",
      "  46.  60. 188.  88.  70.  10.  24.  48. 172. 186.  52.  10.   2. 188.\n",
      " 156.  48. 116. 400.  30. 275.  40. 275. 215. 156. 190. 225. 245. 172.\n",
      " 235. 102.   6. 122. 265. 295. 450.  46.  86. 196.  84. 112.  10. 500.\n",
      "  20.  58.  34. 500.  14.  82. 104.  92. 154. 188. 285. 100. 120.  34.\n",
      " 500.  16. 132.  46.  12. 136.  22.   1. 100.  90.  58.  38. 270. 112.\n",
      " 215.  32.  26. 124.  16. 144.  90. 182. 300. 134. 215. 102. 450. 128.\n",
      "  94. 138.  72. 146. 158.   0. 112.  40. 178.  98.  32. 136. 400.   4.\n",
      " 112.  28. 106.  28.  74. 200. 192.  96.  58.  32. 140. 172.  72.  40.\n",
      " 200. 156. 162. 142. 134.  76. 255. 240.  58. 280. 124. 128.  28. 136.\n",
      " 275.  26.  16. 128. 134.  64. 225.  32. 186. 220.  52. 154. 184.  54.\n",
      " 114.  18. 220.  94. 130.  60. 295.  74.  60. 164.  10. 198. 138. 200.\n",
      " 128.  74. 350. 285. 102.   2.   4.  92.  86. 154. 190. 500. 186.  42.\n",
      "  62. 275. 110. 124. 116.  98. 275. 265.  28. 182. 198. 225. 170. 154.\n",
      "  86. 235. 300. 240.  84.  36. 150.  14. 300. 250. 164. 160.  78.  22.\n",
      " 300. 230. 112. 118. 118. 250. 178. 500.   0.  90. 120.  80. 186.  90.\n",
      " 450.  74.  64.  24. 140.  36. 215. 450. 350. 136.  68. 290. 220. 116.\n",
      " 300. 290.   8. 120.  48. 188. 164.  86.]\n",
      "mse =  894.2243755873442\n"
     ]
    }
   ],
   "source": [
    "time_str = str(time.time())\n",
    "reinitLayers(model)\n",
    "early_stop = EarlyStopping(monitor='val_mean_squared_error', patience=30)\n",
    "# model.load_weights('./modelWights/regression_model'+model_fn.__name__+'.h5')\n",
    "model_checkpoint = ModelCheckpoint('./modelWights/weights_'+'mobileNet_without_pre'+time_str+'_color.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=200, validation_data=(X_val, y_val), callbacks=[model_checkpoint])\n",
    "# print(history.history)\n",
    "\n",
    "y_pred_ = model.predict(X_test, batch_size=1, verbose=1)\n",
    "\n",
    "\n",
    "y_pred_, y_test_ = post_training_data(y_test, y_pred_)\n",
    "predicted_file = 'regress_'+\"mobileNet_without_pre\"+time_str+'_color.csv'\n",
    "save_result(\"./result/\"+predicted_file,y_pred_,y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER:: input_3\n",
      "LAYER:: conv1_pad\n",
      "LAYER:: conv1\n",
      "reinitializing layer conv1.kernel\n",
      "LAYER:: conv1_bn\n",
      "reinitializing layer conv1_bn.gamma\n",
      "reinitializing layer conv1_bn.beta\n",
      "reinitializing layer conv1_bn.moving_mean\n",
      "reinitializing layer conv1_bn.moving_variance\n",
      "LAYER:: conv1_relu\n",
      "LAYER:: conv_dw_1\n",
      "reinitializing layer conv_dw_1.depthwise_kernel\n",
      "LAYER:: conv_dw_1_bn\n",
      "reinitializing layer conv_dw_1_bn.gamma\n",
      "reinitializing layer conv_dw_1_bn.beta\n",
      "reinitializing layer conv_dw_1_bn.moving_mean\n",
      "reinitializing layer conv_dw_1_bn.moving_variance\n",
      "LAYER:: conv_dw_1_relu\n",
      "LAYER:: conv_pw_1\n",
      "reinitializing layer conv_pw_1.kernel\n",
      "LAYER:: conv_pw_1_bn\n",
      "reinitializing layer conv_pw_1_bn.gamma\n",
      "reinitializing layer conv_pw_1_bn.beta\n",
      "reinitializing layer conv_pw_1_bn.moving_mean\n",
      "reinitializing layer conv_pw_1_bn.moving_variance\n",
      "LAYER:: conv_pw_1_relu\n",
      "LAYER:: conv_pad_2\n",
      "LAYER:: conv_dw_2\n",
      "reinitializing layer conv_dw_2.depthwise_kernel\n",
      "LAYER:: conv_dw_2_bn\n",
      "reinitializing layer conv_dw_2_bn.gamma\n",
      "reinitializing layer conv_dw_2_bn.beta\n",
      "reinitializing layer conv_dw_2_bn.moving_mean\n",
      "reinitializing layer conv_dw_2_bn.moving_variance\n",
      "LAYER:: conv_dw_2_relu\n",
      "LAYER:: conv_pw_2\n",
      "reinitializing layer conv_pw_2.kernel\n",
      "LAYER:: conv_pw_2_bn\n",
      "reinitializing layer conv_pw_2_bn.gamma\n",
      "reinitializing layer conv_pw_2_bn.beta\n",
      "reinitializing layer conv_pw_2_bn.moving_mean\n",
      "reinitializing layer conv_pw_2_bn.moving_variance\n",
      "LAYER:: conv_pw_2_relu\n",
      "LAYER:: conv_dw_3\n",
      "reinitializing layer conv_dw_3.depthwise_kernel\n",
      "LAYER:: conv_dw_3_bn\n",
      "reinitializing layer conv_dw_3_bn.gamma\n",
      "reinitializing layer conv_dw_3_bn.beta\n",
      "reinitializing layer conv_dw_3_bn.moving_mean\n",
      "reinitializing layer conv_dw_3_bn.moving_variance\n",
      "LAYER:: conv_dw_3_relu\n",
      "LAYER:: conv_pw_3\n",
      "reinitializing layer conv_pw_3.kernel\n",
      "LAYER:: conv_pw_3_bn\n",
      "reinitializing layer conv_pw_3_bn.gamma\n",
      "reinitializing layer conv_pw_3_bn.beta\n",
      "reinitializing layer conv_pw_3_bn.moving_mean\n",
      "reinitializing layer conv_pw_3_bn.moving_variance\n",
      "LAYER:: conv_pw_3_relu\n",
      "LAYER:: conv_pad_4\n",
      "LAYER:: conv_dw_4\n",
      "reinitializing layer conv_dw_4.depthwise_kernel\n",
      "LAYER:: conv_dw_4_bn\n",
      "reinitializing layer conv_dw_4_bn.gamma\n",
      "reinitializing layer conv_dw_4_bn.beta\n",
      "reinitializing layer conv_dw_4_bn.moving_mean\n",
      "reinitializing layer conv_dw_4_bn.moving_variance\n",
      "LAYER:: conv_dw_4_relu\n",
      "LAYER:: conv_pw_4\n",
      "reinitializing layer conv_pw_4.kernel\n",
      "LAYER:: conv_pw_4_bn\n",
      "reinitializing layer conv_pw_4_bn.gamma\n",
      "reinitializing layer conv_pw_4_bn.beta\n",
      "reinitializing layer conv_pw_4_bn.moving_mean\n",
      "reinitializing layer conv_pw_4_bn.moving_variance\n",
      "LAYER:: conv_pw_4_relu\n",
      "LAYER:: conv_dw_5\n",
      "reinitializing layer conv_dw_5.depthwise_kernel\n",
      "LAYER:: conv_dw_5_bn\n",
      "reinitializing layer conv_dw_5_bn.gamma\n",
      "reinitializing layer conv_dw_5_bn.beta\n",
      "reinitializing layer conv_dw_5_bn.moving_mean\n",
      "reinitializing layer conv_dw_5_bn.moving_variance\n",
      "LAYER:: conv_dw_5_relu\n",
      "LAYER:: conv_pw_5\n",
      "reinitializing layer conv_pw_5.kernel\n",
      "LAYER:: conv_pw_5_bn\n",
      "reinitializing layer conv_pw_5_bn.gamma\n",
      "reinitializing layer conv_pw_5_bn.beta\n",
      "reinitializing layer conv_pw_5_bn.moving_mean\n",
      "reinitializing layer conv_pw_5_bn.moving_variance\n",
      "LAYER:: conv_pw_5_relu\n",
      "LAYER:: conv_pad_6\n",
      "LAYER:: conv_dw_6\n",
      "reinitializing layer conv_dw_6.depthwise_kernel\n",
      "LAYER:: conv_dw_6_bn\n",
      "reinitializing layer conv_dw_6_bn.gamma\n",
      "reinitializing layer conv_dw_6_bn.beta\n",
      "reinitializing layer conv_dw_6_bn.moving_mean\n",
      "reinitializing layer conv_dw_6_bn.moving_variance\n",
      "LAYER:: conv_dw_6_relu\n",
      "LAYER:: conv_pw_6\n",
      "reinitializing layer conv_pw_6.kernel\n",
      "LAYER:: conv_pw_6_bn\n",
      "reinitializing layer conv_pw_6_bn.gamma\n",
      "reinitializing layer conv_pw_6_bn.beta\n",
      "reinitializing layer conv_pw_6_bn.moving_mean\n",
      "reinitializing layer conv_pw_6_bn.moving_variance\n",
      "LAYER:: conv_pw_6_relu\n",
      "LAYER:: conv_dw_7\n",
      "reinitializing layer conv_dw_7.depthwise_kernel\n",
      "LAYER:: conv_dw_7_bn\n",
      "reinitializing layer conv_dw_7_bn.gamma\n",
      "reinitializing layer conv_dw_7_bn.beta\n",
      "reinitializing layer conv_dw_7_bn.moving_mean\n",
      "reinitializing layer conv_dw_7_bn.moving_variance\n",
      "LAYER:: conv_dw_7_relu\n",
      "LAYER:: conv_pw_7\n",
      "reinitializing layer conv_pw_7.kernel\n",
      "LAYER:: conv_pw_7_bn\n",
      "reinitializing layer conv_pw_7_bn.gamma\n",
      "reinitializing layer conv_pw_7_bn.beta\n",
      "reinitializing layer conv_pw_7_bn.moving_mean\n",
      "reinitializing layer conv_pw_7_bn.moving_variance\n",
      "LAYER:: conv_pw_7_relu\n",
      "LAYER:: conv_dw_8\n",
      "reinitializing layer conv_dw_8.depthwise_kernel\n",
      "LAYER:: conv_dw_8_bn\n",
      "reinitializing layer conv_dw_8_bn.gamma\n",
      "reinitializing layer conv_dw_8_bn.beta\n",
      "reinitializing layer conv_dw_8_bn.moving_mean\n",
      "reinitializing layer conv_dw_8_bn.moving_variance\n",
      "LAYER:: conv_dw_8_relu\n",
      "LAYER:: conv_pw_8\n",
      "reinitializing layer conv_pw_8.kernel\n",
      "LAYER:: conv_pw_8_bn\n",
      "reinitializing layer conv_pw_8_bn.gamma\n",
      "reinitializing layer conv_pw_8_bn.beta\n",
      "reinitializing layer conv_pw_8_bn.moving_mean\n",
      "reinitializing layer conv_pw_8_bn.moving_variance\n",
      "LAYER:: conv_pw_8_relu\n",
      "LAYER:: conv_dw_9\n",
      "reinitializing layer conv_dw_9.depthwise_kernel\n",
      "LAYER:: conv_dw_9_bn\n",
      "reinitializing layer conv_dw_9_bn.gamma\n",
      "reinitializing layer conv_dw_9_bn.beta\n",
      "reinitializing layer conv_dw_9_bn.moving_mean\n",
      "reinitializing layer conv_dw_9_bn.moving_variance\n",
      "LAYER:: conv_dw_9_relu\n",
      "LAYER:: conv_pw_9\n",
      "reinitializing layer conv_pw_9.kernel\n",
      "LAYER:: conv_pw_9_bn\n",
      "reinitializing layer conv_pw_9_bn.gamma\n",
      "reinitializing layer conv_pw_9_bn.beta\n",
      "reinitializing layer conv_pw_9_bn.moving_mean\n",
      "reinitializing layer conv_pw_9_bn.moving_variance\n",
      "LAYER:: conv_pw_9_relu\n",
      "LAYER:: conv_dw_10\n",
      "reinitializing layer conv_dw_10.depthwise_kernel\n",
      "LAYER:: conv_dw_10_bn\n",
      "reinitializing layer conv_dw_10_bn.gamma\n",
      "reinitializing layer conv_dw_10_bn.beta\n",
      "reinitializing layer conv_dw_10_bn.moving_mean\n",
      "reinitializing layer conv_dw_10_bn.moving_variance\n",
      "LAYER:: conv_dw_10_relu\n",
      "LAYER:: conv_pw_10\n",
      "reinitializing layer conv_pw_10.kernel\n",
      "LAYER:: conv_pw_10_bn\n",
      "reinitializing layer conv_pw_10_bn.gamma\n",
      "reinitializing layer conv_pw_10_bn.beta\n",
      "reinitializing layer conv_pw_10_bn.moving_mean\n",
      "reinitializing layer conv_pw_10_bn.moving_variance\n",
      "LAYER:: conv_pw_10_relu\n",
      "LAYER:: conv_dw_11\n",
      "reinitializing layer conv_dw_11.depthwise_kernel\n",
      "LAYER:: conv_dw_11_bn\n",
      "reinitializing layer conv_dw_11_bn.gamma\n",
      "reinitializing layer conv_dw_11_bn.beta\n",
      "reinitializing layer conv_dw_11_bn.moving_mean\n",
      "reinitializing layer conv_dw_11_bn.moving_variance\n",
      "LAYER:: conv_dw_11_relu\n",
      "LAYER:: conv_pw_11\n",
      "reinitializing layer conv_pw_11.kernel\n",
      "LAYER:: conv_pw_11_bn\n",
      "reinitializing layer conv_pw_11_bn.gamma\n",
      "reinitializing layer conv_pw_11_bn.beta\n",
      "reinitializing layer conv_pw_11_bn.moving_mean\n",
      "reinitializing layer conv_pw_11_bn.moving_variance\n",
      "LAYER:: conv_pw_11_relu\n",
      "LAYER:: conv_pad_12\n",
      "LAYER:: conv_dw_12\n",
      "reinitializing layer conv_dw_12.depthwise_kernel\n",
      "LAYER:: conv_dw_12_bn\n",
      "reinitializing layer conv_dw_12_bn.gamma\n",
      "reinitializing layer conv_dw_12_bn.beta\n",
      "reinitializing layer conv_dw_12_bn.moving_mean\n",
      "reinitializing layer conv_dw_12_bn.moving_variance\n",
      "LAYER:: conv_dw_12_relu\n",
      "LAYER:: conv_pw_12\n",
      "reinitializing layer conv_pw_12.kernel\n",
      "LAYER:: conv_pw_12_bn\n",
      "reinitializing layer conv_pw_12_bn.gamma\n",
      "reinitializing layer conv_pw_12_bn.beta\n",
      "reinitializing layer conv_pw_12_bn.moving_mean\n",
      "reinitializing layer conv_pw_12_bn.moving_variance\n",
      "LAYER:: conv_pw_12_relu\n",
      "LAYER:: conv_dw_13\n",
      "reinitializing layer conv_dw_13.depthwise_kernel\n",
      "LAYER:: conv_dw_13_bn\n",
      "reinitializing layer conv_dw_13_bn.gamma\n",
      "reinitializing layer conv_dw_13_bn.beta\n",
      "reinitializing layer conv_dw_13_bn.moving_mean\n",
      "reinitializing layer conv_dw_13_bn.moving_variance\n",
      "LAYER:: conv_dw_13_relu\n",
      "LAYER:: conv_pw_13\n",
      "reinitializing layer conv_pw_13.kernel\n",
      "LAYER:: conv_pw_13_bn\n",
      "reinitializing layer conv_pw_13_bn.gamma\n",
      "reinitializing layer conv_pw_13_bn.beta\n",
      "reinitializing layer conv_pw_13_bn.moving_mean\n",
      "reinitializing layer conv_pw_13_bn.moving_variance\n",
      "LAYER:: conv_pw_13_relu\n",
      "LAYER:: global_average_pooling2d_2\n",
      "LAYER:: reshape_1\n",
      "LAYER:: dropout\n",
      "LAYER:: conv_preds\n",
      "reinitializing layer conv_preds.kernel\n",
      "reinitializing layer conv_preds.bias\n",
      "LAYER:: reshape_2\n",
      "LAYER:: dense_3\n",
      "reinitializing layer dense_3.kernel\n",
      "reinitializing layer dense_3.bias\n",
      "LAYER:: activation_52\n",
      "Train on 1053 samples, validate on 264 samples\n",
      "Epoch 1/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 284.4115 - mse: 284.4116 - val_loss: 0.0442 - val_mse: 0.0442\n",
      "Epoch 2/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0419 - mse: 0.0419 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 3/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0419 - mse: 0.0419 - val_loss: 0.0396 - val_mse: 0.0396\n",
      "Epoch 4/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0412 - mse: 0.0412 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 5/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0417 - mse: 0.0417 - val_loss: 0.0383 - val_mse: 0.0383\n",
      "Epoch 6/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0399 - mse: 0.0399 - val_loss: 0.0370 - val_mse: 0.0370\n",
      "Epoch 7/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0396 - mse: 0.0396 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 8/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0405 - mse: 0.0405 - val_loss: 0.0369 - val_mse: 0.0369\n",
      "Epoch 9/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0405 - mse: 0.0405 - val_loss: 0.0379 - val_mse: 0.0379\n",
      "Epoch 10/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0398 - mse: 0.0398 - val_loss: 0.0361 - val_mse: 0.0361\n",
      "Epoch 11/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 12/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0354 - mse: 0.0354 - val_loss: 0.0304 - val_mse: 0.0304\n",
      "Epoch 13/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0302 - mse: 0.0302 - val_loss: 0.0272 - val_mse: 0.0272\n",
      "Epoch 14/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0254 - mse: 0.0254 - val_loss: 0.0182 - val_mse: 0.0182\n",
      "Epoch 15/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0217 - mse: 0.0217 - val_loss: 0.0159 - val_mse: 0.0159\n",
      "Epoch 16/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0196 - mse: 0.0196 - val_loss: 0.0120 - val_mse: 0.0120\n",
      "Epoch 17/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.0119 - val_mse: 0.0119\n",
      "Epoch 18/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 19/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 20/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 21/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 22/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 23/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 24/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 25/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 26/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 27/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 28/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 29/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 30/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 31/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0054 - val_mse: 0.0054\n",
      "Epoch 32/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0204 - val_mse: 0.0204\n",
      "Epoch 33/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 34/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 35/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 36/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0070 - val_mse: 0.0070\n",
      "Epoch 37/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0158 - val_mse: 0.0158\n",
      "Epoch 38/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 39/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0183 - val_mse: 0.0183\n",
      "Epoch 40/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 41/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0494 - val_mse: 0.0494\n",
      "Epoch 42/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0303 - val_mse: 0.0303\n",
      "Epoch 43/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 44/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0064 - val_mse: 0.0064\n",
      "Epoch 45/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 46/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 47/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 48/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 49/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0235 - val_mse: 0.0235\n",
      "Epoch 50/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 51/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 52/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 53/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 54/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0065 - val_mse: 0.0065\n",
      "Epoch 55/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 56/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 57/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0122 - val_mse: 0.0122\n",
      "Epoch 58/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 59/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0057 - val_mse: 0.0057\n",
      "Epoch 60/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 62/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0054 - val_mse: 0.0054\n",
      "Epoch 63/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 64/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0109 - val_mse: 0.0109\n",
      "Epoch 65/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 66/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 67/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 68/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 69/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 70/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 71/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 72/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 73/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0047 - val_mse: 0.0047\n",
      "Epoch 74/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 75/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0122 - val_mse: 0.0122\n",
      "Epoch 76/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 77/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 78/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 79/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 80/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 81/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0053 - val_mse: 0.0053\n",
      "Epoch 82/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 83/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 84/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 85/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 86/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 87/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 88/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 89/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0116 - val_mse: 0.0116\n",
      "Epoch 90/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 91/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 92/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 93/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 94/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 95/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 96/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 97/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 98/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 99/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 100/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 101/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 102/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 103/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 104/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 105/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0239 - val_mse: 0.0239\n",
      "Epoch 106/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0182 - val_mse: 0.0182\n",
      "Epoch 107/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 108/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 109/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 110/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 111/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 112/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 113/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 114/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0132 - val_mse: 0.0132\n",
      "Epoch 115/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0142 - val_mse: 0.0142\n",
      "Epoch 116/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 117/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 118/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 119/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 120/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 121/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0065 - val_mse: 0.0065\n",
      "Epoch 122/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 123/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 124/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 125/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 126/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0050 - val_mse: 0.0050\n",
      "Epoch 127/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 128/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 129/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 130/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 131/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 132/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 133/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 134/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 135/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 136/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 137/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 138/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 139/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 140/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 141/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 142/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 143/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 144/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 145/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 146/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 147/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 148/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 149/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 150/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 151/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 152/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 153/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 154/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 155/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 156/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0118 - val_mse: 0.0118\n",
      "Epoch 157/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 158/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 159/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 160/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 161/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 162/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 163/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 164/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0070 - val_mse: 0.0070\n",
      "Epoch 165/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 166/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 167/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 168/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 169/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 170/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 171/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 172/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 173/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 174/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 175/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 176/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 177/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 178/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 180/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 181/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0010 - val_mse: 0.0010\n",
      "Epoch 182/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 183/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 184/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 185/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 186/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 187/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 188/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 189/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 190/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 191/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 192/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 193/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 194/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 195/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 196/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 197/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 9.7890e-04 - mse: 9.7890e-04 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 198/200\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 199/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 200/200\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "330/330 [==============================] - 4s 11ms/step\n",
      "(330,)\n",
      "(330,)\n",
      "[ 24.62329    67.78105   254.85898   238.85751   447.19617   149.42253\n",
      " 247.50453   124.80718   153.8765    174.8009    268.12122    77.70514\n",
      " 171.85071    86.61949   125.02508    52.807728  153.76605   155.68819\n",
      " 257.70355   204.9578    478.31082   255.64366   277.91577    98.65148\n",
      "  87.49201    70.927826  137.4615     26.571974  163.48438   151.86107\n",
      " 150.05385   144.23392   123.38031    72.08889   142.96838   186.04309\n",
      " 284.6319     38.27499    54.25319   444.1083    266.10608   289.67874\n",
      " 171.96268   229.62424   297.70248   325.43903    46.95725    29.218197\n",
      " 129.47362   174.05705     5.2681265  93.618195  184.8004     24.856344\n",
      " 211.44374   193.58554   129.74965   248.87875   258.8841     19.185438\n",
      " 235.11546   307.7066     68.267166  443.82767   241.20975   279.72437\n",
      " 255.225     217.55579   249.88997    94.26256   228.50687    27.648285\n",
      " 208.88695   173.01866    77.74939   243.96738   160.78314   165.99725\n",
      " 127.686066   12.030661  157.72441   394.81125    47.008446   40.466034\n",
      "  58.129192  116.535194  114.36936   106.66841    80.84894    85.816414\n",
      " 226.54712    20.661623  214.56514   161.26016   268.20984   213.47809\n",
      " 143.37743   243.96738    60.060986   84.377556  190.8068    106.94511\n",
      "  93.777275   14.538065   48.38921    78.662605  181.47252   205.9678\n",
      "  86.70171    28.618605   25.075123  186.39725   146.36853    38.737103\n",
      " 132.42313   408.63226    55.88974   267.9918     68.33124   265.31317\n",
      " 252.80243   166.96382   188.19118   237.12956   226.2324    185.60478\n",
      " 261.26224    89.61223    34.01065   143.46715   249.71843   294.25513\n",
      " 433.16513    70.03402    87.11687   214.29631    78.10195   139.58667\n",
      "  33.81504   491.6268     34.36108    78.658424   48.875004  476.72552\n",
      "  32.91574   146.2896    137.21643   113.1991    164.76924   200.04225\n",
      " 287.7685     79.138466  132.27686    46.5165    465.14923    15.210926\n",
      " 171.237      55.207947   26.47002   169.50487    83.89123    90.77525\n",
      " 109.11319   121.65434    62.71503    50.679356  271.19464   127.24179\n",
      " 231.36673    46.904705   42.500072  136.76622    21.124973  158.82748\n",
      " 100.97656   214.08609   291.4514    123.59964   267.63416   108.983246\n",
      " 426.0892    193.9267     97.955185  165.96376   112.85394   146.96898\n",
      " 161.2617     38.18136   169.53545    63.157627  189.10445   112.973495\n",
      "  42.291687  149.2098    390.3452     54.879898  144.9745     82.30212\n",
      " 140.95364    38.175175  121.9403    175.3502    161.05452   145.52692\n",
      "  60.32048    72.99812   158.8139    211.28967   100.77817    67.493835\n",
      " 230.10953   142.2973    164.13876   170.28127   123.121216  112.550316\n",
      " 237.91393   282.06534    83.96441   271.0365    142.71373   191.36\n",
      "  46.540707  141.3379    277.76926    21.686121   13.143539  122.91823\n",
      " 159.91084   100.710884  233.49947    49.03706   221.90395   209.82837\n",
      "  60.745464  136.68982   213.76959    57.36004   124.66951    30.718416\n",
      " 192.39613    89.99542   144.42656    81.971825  305.99982    95.368614\n",
      "  91.91898   174.36598    32.985195  186.3927    169.93262   206.22156\n",
      " 154.69228    95.577965  332.57135   278.9915    127.3004     41.56825\n",
      "   9.561896   99.129425   74.85336   150.79935   188.36452   473.0174\n",
      " 207.8406     56.871914  102.81406   288.93652   151.67838   152.76045\n",
      " 135.88121   119.66807   245.60497   270.63562    52.60065   180.12221\n",
      " 208.31563   237.12956   194.63092   166.05006   127.14213   238.11906\n",
      " 274.34964   228.2799    105.07143    27.411118  153.44632    21.14205\n",
      " 292.08777   242.23737   172.82117   166.10995    84.44248    23.161993\n",
      " 309.6139    235.89713   153.46526   131.93251   166.20528   252.49785\n",
      " 174.71533   451.47482    33.267708  136.38861   135.57538    76.832695\n",
      " 171.39026   118.968445  443.56085    69.279274   82.8197     17.397657\n",
      " 172.00058    43.181343  223.6124    436.3014    379.1232    145.61642\n",
      "  80.66467   293.9472    255.56389   121.596725  287.86307   280.4051\n",
      "  34.211235  141.62889    37.454082  194.44963   182.50624    70.804214 ]\n",
      "[  6.  54. 275. 240. 450. 108. 225.  58. 150. 162. 250.  80. 162.  64.\n",
      " 100.  46. 174. 128. 250. 184. 500. 300. 285.  68.  88.  30. 120.   8.\n",
      " 128. 154. 144. 126.  80.  52. 112. 158. 300.  18.  66. 450. 265. 290.\n",
      " 168. 255. 295. 300.  32.  24.  96. 132.   6.  92. 186.   8. 180. 196.\n",
      " 108. 230. 225.   8. 215. 290.  54. 450. 250. 270. 230. 200. 225.  80.\n",
      " 245.   4. 190. 168.  44. 225. 182. 122. 120.   4. 124. 350.  56.   2.\n",
      "  12.  84.  84.  78.  84.  82. 225.  18. 176. 174. 198. 225. 100. 225.\n",
      "  46.  60. 188.  88.  70.  10.  24.  48. 172. 186.  52.  10.   2. 188.\n",
      " 156.  48. 116. 400.  30. 275.  40. 275. 215. 156. 190. 225. 245. 172.\n",
      " 235. 102.   6. 122. 265. 295. 450.  46.  86. 196.  84. 112.  10. 500.\n",
      "  20.  58.  34. 500.  14.  82. 104.  92. 154. 188. 285. 100. 120.  34.\n",
      " 500.  16. 132.  46.  12. 136.  22.   1. 100.  90.  58.  38. 270. 112.\n",
      " 215.  32.  26. 124.  16. 144.  90. 182. 300. 134. 215. 102. 450. 128.\n",
      "  94. 138.  72. 146. 158.   0. 112.  40. 178.  98.  32. 136. 400.   4.\n",
      " 112.  28. 106.  28.  74. 200. 192.  96.  58.  32. 140. 172.  72.  40.\n",
      " 200. 156. 162. 142. 134.  76. 255. 240.  58. 280. 124. 128.  28. 136.\n",
      " 275.  26.  16. 128. 134.  64. 225.  32. 186. 220.  52. 154. 184.  54.\n",
      " 114.  18. 220.  94. 130.  60. 295.  74.  60. 164.  10. 198. 138. 200.\n",
      " 128.  74. 350. 285. 102.   2.   4.  92.  86. 154. 190. 500. 186.  42.\n",
      "  62. 275. 110. 124. 116.  98. 275. 265.  28. 182. 198. 225. 170. 154.\n",
      "  86. 235. 300. 240.  84.  36. 150.  14. 300. 250. 164. 160.  78.  22.\n",
      " 300. 230. 112. 118. 118. 250. 178. 500.   0.  90. 120.  80. 186.  90.\n",
      " 450.  74.  64.  24. 140.  36. 215. 450. 350. 136.  68. 290. 220. 116.\n",
      " 300. 290.   8. 120.  48. 188. 164.  86.]\n",
      "mse =  583.7742041791403\n"
     ]
    }
   ],
   "source": [
    "time_str = str(time.time())\n",
    "reinitLayers(model)\n",
    "early_stop = EarlyStopping(monitor='val_mean_squared_error', patience=30)\n",
    "# model.load_weights('./modelWights/regression_model'+model_fn.__name__+'.h5')\n",
    "model_checkpoint = ModelCheckpoint('./modelWights/weights_'+'mobileNet_without_pre'+time_str+'_color.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=200, validation_data=(X_val, y_val), callbacks=[model_checkpoint])\n",
    "# print(history.history)\n",
    "\n",
    "y_pred_ = model.predict(X_test, batch_size=1, verbose=1)\n",
    "\n",
    "\n",
    "y_pred_, y_test_ = post_training_data(y_test, y_pred_)\n",
    "predicted_file = 'regress_'+\"mobileNet_without_pre\"+time_str+'_color.csv'\n",
    "save_result(\"./result/\"+predicted_file,y_pred_,y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic = X_train[1]\n",
    "model = Sequential()\n",
    "#model.add(Input(shape=pic.shape))\n",
    "# model.add(Conv2D(filters=4, kernel_size=(7,7), strides=1, input_shape=pic.shape))\n",
    "Kerasmodel = keras.applications.resnet50.ResNet50(include_top=True\n",
    "                , weights=None#'imagenet'\n",
    "                # , input_tensor=inputs\n",
    "                , input_shape=pic.shape\n",
    "                )\n",
    "# Kerasmodel._layers.pop()\n",
    "# Kerasmodel.layers.pop()\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(1,kernel_initializer='he_normal'))\n",
    "model.add(Activation('linear')) #softmax\n",
    "model = Model( inputs=Kerasmodel.input , outputs=model(Kerasmodel.layers[-2].output))\n",
    "model.compile(loss='mean_squared_error', #mse binary_crossentropy Dice-coefficient loss function vs cross-entropy\n",
    "                optimizer=optimizers.Adam(lr=5e-3),\n",
    "                metrics=['mse'])\n",
    "\n",
    "model.summary()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_batch = np.expand_dims(pic, axis=0)\n",
    "layer_1 = K.function([model.layers[0].input], [model.layers[0].output])\n",
    "f1 = layer_1([pic_batch])[0]\n",
    "#number of your filters\n",
    "for _ in range(4):\n",
    "    show_img = f1[:, :, :, _]\n",
    "    show_img.shape = [506, 506]\n",
    "    print(show_img)\n",
    "    plt.subplot(2, 2, _+1)\n",
    "    plt.imshow(show_img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification With Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dltdc/.local/lib/python3.7/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Using Adam optimizer. Learning rate: 4e-05\n",
      "[[[[0.13946079 0.37040442 0.977248  ]\n",
      "   [0.14258578 0.37254903 0.9843137 ]\n",
      "   [0.14901961 0.37254903 0.9882353 ]\n",
      "   ...\n",
      "   [0.14117648 0.3529412  0.9843137 ]\n",
      "   [0.14509805 0.35686275 0.9882353 ]\n",
      "   [0.14117648 0.3529412  0.9843137 ]]\n",
      "\n",
      "  [[0.14184666 0.3764706  0.9816176 ]\n",
      "   [0.14117648 0.3755017  0.9843137 ]\n",
      "   [0.14509805 0.37254903 0.9843137 ]\n",
      "   ...\n",
      "   [0.14117648 0.3529412  0.9843137 ]\n",
      "   [0.14117648 0.3529412  0.9843137 ]\n",
      "   [0.14050628 0.352271   0.98364353]]\n",
      "\n",
      "  [[0.14901961 0.37606847 0.9882353 ]\n",
      "   [0.14509805 0.3764706  0.9882353 ]\n",
      "   [0.14509805 0.3764706  0.9853554 ]\n",
      "   ...\n",
      "   [0.14509805 0.35686275 0.9882353 ]\n",
      "   [0.14509805 0.35686275 0.9882353 ]\n",
      "   [0.14117648 0.3529412  0.9843137 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.20639935 0.45667893 0.9843137 ]\n",
      "   [0.20439261 0.46713772 0.9882353 ]\n",
      "   [0.19607843 0.45882353 0.9843137 ]\n",
      "   ...\n",
      "   [0.14405638 0.4028799  0.98719364]\n",
      "   [0.14650735 0.4053309  0.9882353 ]\n",
      "   [0.14117648 0.4        0.9843137 ]]\n",
      "\n",
      "  [[0.2        0.45490196 0.9843137 ]\n",
      "   [0.20251225 0.45882353 0.9882353 ]\n",
      "   [0.20071615 0.45882353 0.9882353 ]\n",
      "   ...\n",
      "   [0.14387254 0.40071616 0.9882353 ]\n",
      "   [0.14650735 0.40392157 0.9882353 ]\n",
      "   [0.14509805 0.4        0.9882353 ]]\n",
      "\n",
      "  [[0.20784314 0.45490196 0.98139167]\n",
      "   [0.1952857  0.44626608 0.9775697 ]\n",
      "   [0.19883579 0.45202205 0.9814338 ]\n",
      "   ...\n",
      "   [0.15514706 0.40784314 0.9905369 ]\n",
      "   [0.15686275 0.40392157 0.9882353 ]\n",
      "   [0.15514706 0.40392157 0.9909505 ]]]\n",
      "\n",
      "\n",
      " [[[0.2252451  0.39172795 0.88805145]\n",
      "   [0.22603783 0.4        0.9005476 ]\n",
      "   [0.23137255 0.40392157 0.9098039 ]\n",
      "   ...\n",
      "   [0.17300475 0.31025964 0.76516163]\n",
      "   [0.17254902 0.31293276 0.7647059 ]\n",
      "   [0.16862746 0.3137255  0.7647059 ]]\n",
      "\n",
      "  [[0.23473881 0.40269607 0.8980392 ]\n",
      "   [0.22841989 0.4031365  0.9019608 ]\n",
      "   [0.22745098 0.4        0.90588236]\n",
      "   ...\n",
      "   [0.17254902 0.3137255  0.7647059 ]\n",
      "   [0.17474341 0.3151348  0.7647059 ]\n",
      "   [0.17254902 0.31764707 0.7653761 ]]\n",
      "\n",
      "  [[0.23137255 0.40392157 0.9026961 ]\n",
      "   [0.23137255 0.40784314 0.90588236]\n",
      "   [0.23210785 0.40784314 0.9098039 ]\n",
      "   ...\n",
      "   [0.18039216 0.32156864 0.76862746]\n",
      "   [0.17442939 0.31764707 0.7647059 ]\n",
      "   [0.16862746 0.3137255  0.7607843 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.3230124  0.5176471  0.98039216]\n",
      "   [0.32156864 0.52156866 0.98039216]\n",
      "   [0.32156864 0.52156866 0.98039216]\n",
      "   ...\n",
      "   [0.28215763 0.4745098  0.98039216]\n",
      "   [0.2901961  0.48235294 0.9882353 ]\n",
      "   [0.28627452 0.47843137 0.98039216]]\n",
      "\n",
      "  [[0.32482    0.51617265 0.97702587]\n",
      "   [0.32721737 0.5201593  0.9821193 ]\n",
      "   [0.3254902  0.5176471  0.98039216]\n",
      "   ...\n",
      "   [0.28235295 0.4745098  0.98039216]\n",
      "   [0.28627452 0.47843137 0.9843137 ]\n",
      "   [0.28627452 0.47720972 0.9816176 ]]\n",
      "\n",
      "  [[0.33433288 0.519424   0.97432595]\n",
      "   [0.33663833 0.5254902  0.9843137 ]\n",
      "   [0.3299977  0.52156866 0.9843137 ]\n",
      "   ...\n",
      "   [0.2884804  0.4745098  0.98039216]\n",
      "   [0.29240197 0.4745098  0.9843137 ]\n",
      "   [0.295324   0.47843137 0.9843137 ]]]\n",
      "\n",
      "\n",
      " [[[0.21568628 0.4392157  0.7948721 ]\n",
      "   [0.21960784 0.43529412 0.8       ]\n",
      "   [0.22573529 0.43980163 0.8061274 ]\n",
      "   ...\n",
      "   [0.1771446  0.3781863  0.6953776 ]\n",
      "   [0.1882353  0.38039216 0.7019608 ]\n",
      "   [0.16570543 0.36570543 0.68533623]]\n",
      "\n",
      "  [[0.22108226 0.44313726 0.8026999 ]\n",
      "   [0.2139591  0.43137255 0.79607844]\n",
      "   [0.21960784 0.43529412 0.8       ]\n",
      "   ...\n",
      "   [0.1882353  0.39215687 0.714951  ]\n",
      "   [0.18431373 0.38431373 0.70980394]\n",
      "   [0.17254902 0.37769607 0.7019646 ]]\n",
      "\n",
      "  [[0.21960784 0.4392157  0.80177695]\n",
      "   [0.21960784 0.43529412 0.8       ]\n",
      "   [0.22745098 0.4392157  0.80784315]\n",
      "   ...\n",
      "   [0.17985217 0.38431373 0.7092639 ]\n",
      "   [0.1801279  0.3801279  0.7056181 ]\n",
      "   [0.18431373 0.3903799  0.7158701 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.32941177 0.5882353  0.97432595]\n",
      "   [0.3262255  0.58478475 0.9815372 ]\n",
      "   [0.3254902  0.58431375 0.98357844]\n",
      "   ...\n",
      "   [0.28319928 0.5529412  0.92241496]\n",
      "   [0.2784314  0.54901963 0.9137255 ]\n",
      "   [0.27450982 0.54295343 0.9098039 ]]\n",
      "\n",
      "  [[0.32941177 0.59607846 0.98364353]\n",
      "   [0.32408088 0.5899625  0.9882353 ]\n",
      "   [0.32743183 0.58927697 0.9885608 ]\n",
      "   ...\n",
      "   [0.2901961  0.5500613  0.92653185]\n",
      "   [0.28627452 0.54509807 0.92156863]\n",
      "   [0.2777612  0.53658473 0.91158086]]\n",
      "\n",
      "  [[0.32820544 0.587029   0.97332644]\n",
      "   [0.32941177 0.5921569  0.985723  ]\n",
      "   [0.3299977  0.5921569  0.9888212 ]\n",
      "   ...\n",
      "   [0.30300245 0.55849034 0.93216914]\n",
      "   [0.29301855 0.54509807 0.92015547]\n",
      "   [0.2841299  0.53604853 0.91158086]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.16392846 0.2884804  0.6884804 ]\n",
      "   [0.16360678 0.27984452 0.6862745 ]\n",
      "   [0.16470589 0.2790173  0.6862745 ]\n",
      "   ...\n",
      "   [0.1310317  0.2212278  0.52711016]\n",
      "   [0.13333334 0.22352941 0.5294118 ]\n",
      "   [0.12036229 0.21055837 0.5164407 ]]\n",
      "\n",
      "  [[0.15508579 0.2799058  0.6784314 ]\n",
      "   [0.16219363 0.28235295 0.6876838 ]\n",
      "   [0.16830193 0.28235295 0.6901961 ]\n",
      "   ...\n",
      "   [0.12941177 0.21960784 0.5254902 ]\n",
      "   [0.12941177 0.21960784 0.5254902 ]\n",
      "   [0.12941177 0.21960784 0.5254902 ]]\n",
      "\n",
      "  [[0.15686275 0.28235295 0.68235296]\n",
      "   [0.16266468 0.28120786 0.68653876]\n",
      "   [0.16470589 0.2784314  0.6862745 ]\n",
      "   ...\n",
      "   [0.1254902  0.21568628 0.52156866]\n",
      "   [0.12156863 0.21176471 0.5176471 ]\n",
      "   [0.1254902  0.21568628 0.52156866]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.3262255  0.52156866 0.9828699 ]\n",
      "   [0.32941177 0.52156866 0.9843137 ]\n",
      "   [0.32941177 0.52156866 0.9843137 ]\n",
      "   ...\n",
      "   [0.3019608  0.49803922 0.98039216]\n",
      "   [0.2936466  0.48972502 0.97207797]\n",
      "   [0.2901961  0.4894608  0.97181374]]\n",
      "\n",
      "  [[0.3254902  0.5254902  0.9843137 ]\n",
      "   [0.3254902  0.52156866 0.9843137 ]\n",
      "   [0.32747012 0.519627   0.98039216]\n",
      "   ...\n",
      "   [0.29803923 0.4934015  0.9764706 ]\n",
      "   [0.29411766 0.49019608 0.972549  ]\n",
      "   [0.2901961  0.49019608 0.972549  ]]\n",
      "\n",
      "  [[0.32941177 0.53035    0.98039216]\n",
      "   [0.3254902  0.5201593  0.9775697 ]\n",
      "   [0.32941177 0.5176471  0.98039216]\n",
      "   ...\n",
      "   [0.29803923 0.49019608 0.9719631 ]\n",
      "   [0.29411766 0.49019608 0.96862745]\n",
      "   [0.2931794  0.49197304 0.9704044 ]]]\n",
      "\n",
      "\n",
      " [[[0.26981083 0.5176471  0.9617838 ]\n",
      "   [0.2627451  0.5137255  0.9607843 ]\n",
      "   [0.25927925 0.5108456  0.96182597]\n",
      "   ...\n",
      "   [0.23921569 0.4627451  0.9372549 ]\n",
      "   [0.23529412 0.46997166 0.9372549 ]\n",
      "   [0.23237209 0.47279412 0.9311887 ]]\n",
      "\n",
      "  [[0.2688113  0.5210133  0.9647059 ]\n",
      "   [0.2617762  0.5176471  0.9647059 ]\n",
      "   [0.25882354 0.51566714 0.96574754]\n",
      "   ...\n",
      "   [0.24705882 0.46666667 0.9411765 ]\n",
      "   [0.23921569 0.46666667 0.9386642 ]\n",
      "   [0.23259804 0.46666667 0.9372549 ]]\n",
      "\n",
      "  [[0.25955883 0.5137255  0.96544117]\n",
      "   [0.2669309  0.5190564  0.972549  ]\n",
      "   [0.26666668 0.52156866 0.9764706 ]\n",
      "   ...\n",
      "   [0.2509804  0.47058824 0.9461397 ]\n",
      "   [0.23874463 0.4627451  0.9352137 ]\n",
      "   [0.23921569 0.47233072 0.94291896]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.30588236 0.532598   0.9882353 ]\n",
      "   [0.30588236 0.5294118  0.9843137 ]\n",
      "   [0.3019608  0.5254902  0.98039216]\n",
      "   ...\n",
      "   [0.2901961  0.50980395 0.9764706 ]\n",
      "   [0.29338235 0.51325446 0.9799211 ]\n",
      "   [0.29193857 0.5133923  0.98005897]]\n",
      "\n",
      "  [[0.3131702  0.54454273 0.99215686]\n",
      "   [0.3083946  0.5350605  0.9907476 ]\n",
      "   [0.30588236 0.5294118  0.9843137 ]\n",
      "   ...\n",
      "   [0.3019608  0.518547   0.9843137 ]\n",
      "   [0.29803923 0.5176471  0.9843137 ]\n",
      "   [0.29803923 0.5137255  0.9843137 ]]\n",
      "\n",
      "  [[0.31764707 0.54509807 0.99215686]\n",
      "   [0.3137255  0.5372549  0.99215686]\n",
      "   [0.30808824 0.5294118  0.9882353 ]\n",
      "   ...\n",
      "   [0.30542663 0.52156866 0.9843137 ]\n",
      "   [0.3019608  0.5176471  0.98039216]\n",
      "   [0.3019608  0.5137255  0.98039216]]]\n",
      "\n",
      "\n",
      " [[[0.22567402 0.3647059  0.7978554 ]\n",
      "   [0.22635187 0.37254903 0.8106656 ]\n",
      "   [0.21464461 0.36078432 0.79895836]\n",
      "   ...\n",
      "   [0.16308594 0.27288985 0.65594363]\n",
      "   [0.16140088 0.2712048  0.65159696]\n",
      "   [0.16078432 0.27058825 0.6509804 ]]\n",
      "\n",
      "  [[0.22745098 0.3647059  0.7978554 ]\n",
      "   [0.2243145  0.36941254 0.80784315]\n",
      "   [0.21464461 0.36276424 0.79895836]\n",
      "   ...\n",
      "   [0.16470589 0.27450982 0.65882355]\n",
      "   [0.16593137 0.2757353  0.65612745]\n",
      "   [0.15808824 0.26789215 0.6482843 ]]\n",
      "\n",
      "  [[0.21176471 0.35646063 0.78431374]\n",
      "   [0.21913679 0.36611518 0.79607844]\n",
      "   [0.21568628 0.3647059  0.7921569 ]\n",
      "   ...\n",
      "   [0.17254902 0.28235295 0.6666667 ]\n",
      "   [0.16658625 0.27639017 0.6567823 ]\n",
      "   [0.15471813 0.26452205 0.6449142 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.31764707 0.46666667 0.9728822 ]\n",
      "   [0.3268995  0.47058824 0.98039216]\n",
      "   [0.33491498 0.4745098  0.9843137 ]\n",
      "   ...\n",
      "   [0.2627451  0.43137255 0.9137255 ]\n",
      "   [0.2634804  0.43210784 0.90661764]\n",
      "   [0.25563726 0.42034313 0.8980392 ]]\n",
      "\n",
      "  [[0.31428078 0.46004903 0.9613396 ]\n",
      "   [0.323763   0.46886107 0.98258656]\n",
      "   [0.32941177 0.47058824 0.9879098 ]\n",
      "   ...\n",
      "   [0.25202206 0.42352942 0.9042279 ]\n",
      "   [0.2509804  0.41960785 0.89411765]\n",
      "   [0.2509804  0.4117647  0.8901961 ]]\n",
      "\n",
      "  [[0.2929113  0.43094364 0.92898285]\n",
      "   [0.30900735 0.45097655 0.96470207]\n",
      "   [0.32941177 0.4745098  0.9882353 ]\n",
      "   ...\n",
      "   [0.25422794 0.42457107 0.90300244]\n",
      "   [0.25018767 0.41568628 0.88689107]\n",
      "   [0.2509804  0.40392157 0.88014704]]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[8,256,128,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node training_1/Adam/gradients/AddN_161-1-TransposeNHWCToNCHW-LayoutOptimizer}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[metrics_1/accuracy/Identity/_6699]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[8,256,128,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node training_1/Adam/gradients/AddN_161-1-TransposeNHWCToNCHW-LayoutOptimizer}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-65e8e72c0993>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0monehot_y_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monehot_y_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0monehot_y_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;31m# score = model.score(X_test, y_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[8,256,128,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node training_1/Adam/gradients/AddN_161-1-TransposeNHWCToNCHW-LayoutOptimizer}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[metrics_1/accuracy/Identity/_6699]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[8,256,128,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node training_1/Adam/gradients/AddN_161-1-TransposeNHWCToNCHW-LayoutOptimizer}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "def get_learning_rate(cnn_type):\n",
    "    if cnn_type == 'VGG16' or cnn_type == 'VGG16_DROPOUT':\n",
    "        return 0.00004\n",
    "    elif cnn_type == 'VGG16_KERAS':\n",
    "        return 0.00005\n",
    "    elif cnn_type == 'VGG19':\n",
    "        return 0.00003\n",
    "    elif cnn_type == 'VGG19_KERAS':\n",
    "        return 0.00005\n",
    "    elif cnn_type == 'RESNET50':\n",
    "        return 0.00004\n",
    "    elif cnn_type == 'INCEPTION_V3':\n",
    "        return 0.00003\n",
    "    elif cnn_type == 'SQUEEZE_NET':\n",
    "        return 0.00003\n",
    "    elif cnn_type == 'DENSENET_161':\n",
    "        return 0.00003\n",
    "    elif cnn_type == 'DENSENET_121':\n",
    "        return 0.00001\n",
    "    else:\n",
    "        print('Error Unknown CNN type for learning rate!!')\n",
    "        exit()\n",
    "    return 0.00005\n",
    "\n",
    "\n",
    "def get_optim(cnn_type, optim_type, learning_rate=-1):\n",
    "    from keras.optimizers import SGD\n",
    "    from keras.optimizers import Adam\n",
    "\n",
    "    if learning_rate == -1:\n",
    "        lr = get_learning_rate(cnn_type)\n",
    "    else:\n",
    "        lr = learning_rate\n",
    "    if optim_type == 'Adam':\n",
    "        optim = Adam(lr=lr)\n",
    "    else:\n",
    "        optim = SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    print('Using {} optimizer. Learning rate: {}'.format(optim_type, lr))\n",
    "    return optim\n",
    "\n",
    "# X_train, y_train, X_test, y_test = cr.load_data()\n",
    "X_train = np.load('WaterQuality/X_train_ratio=0.2.npy')\n",
    "X_test = np.load('WaterQuality/X_test_ratio=0.2.npy')\n",
    "y_train = np.load('WaterQuality/y_train_ratio=0.2.npy')\n",
    "y_test = np.load('WaterQuality/y_test_ratio=0.2.npy')\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] > 300:\n",
    "        y_train[i] = 30\n",
    "    else:\n",
    "        y_train[i] = int(y_train[i]//10)\n",
    "\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] > 300:\n",
    "        y_test[i] = 30\n",
    "    else:\n",
    "        y_test[i] = int(y_test[i]//10)\n",
    "\n",
    "y_test = y_test.astype(int)\n",
    "y_train = y_train.astype(int)\n",
    "# print(y_test)\n",
    "\n",
    "onehot_y_train = []\n",
    "onehot_y_test = []\n",
    "for i in range(len(y_test)):\n",
    "    out = [0 for i in range(31)]\n",
    "    out[y_test[i]]=1\n",
    "    print(out)\n",
    "    onehot_y_test.append(out)\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    out = [0 for i in range(31)]\n",
    "    out[y_train[i]]=1\n",
    "    print(out)\n",
    "    onehot_y_train.append(out)\n",
    "\n",
    "# print(onehot_y_train)    \n",
    "\n",
    "\n",
    "\n",
    "dirs = \"classification_result\"\n",
    "if not os.path.exists(dirs):\n",
    "        os.makedirs(dirs)\n",
    "\n",
    "# print(X_train[0].shape)\n",
    "\n",
    "y_pred, y_test_ = run_model(X_train_g, onehot_y_train, X_val, y_val, X_test_g, y_test, model_name=\"resnet50\", model_fn = model_fn,is_classfication=True, nb_classes=31)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
